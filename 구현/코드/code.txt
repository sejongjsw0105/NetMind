
FILE: dkg\__init__.py




FILE: dkg\builders\__init__.py

   1: """Graph builders and transformation modules."""
   2: from .graph_build import *
   3: from .supergraph import ViewBuilder, SuperGraph, SuperNode, SuperEdge
   4: from .graph_metadata import *
   5: from .graph_updater import *
   6: from .constraint_projector import *
   7: 
   8: __all__ = [
   9:     "ViewBuilder",
  10:     "SuperGraph",
  11:     "SuperNode",
  12:     "SuperEdge",
  13: ]



FILE: dkg\cache\__init__.py

   1: """Graph caching and snapshot modules."""
   2: from .graph_version import GraphVersion
   3: from .snapshot import GraphSnapshot, load_snapshot, save_snapshot
   4: 
   5: __all__ = [
   6:     "GraphVersion",
   7:     "GraphSnapshot",
   8:     "load_snapshot",
   9:     "save_snapshot",
  10: ]



FILE: dkg\core\__init__.py

   1: """Core graph data structures and definitions."""
   2: from .graph import DKGEdge, DKGNode, EdgeFlowType, EntityClass, RelationType
   3: from .ir import *
   4: from .provenance import Provenance
   5: 
   6: __all__ = [
   7:     "DKGEdge",
   8:     "DKGNode", 
   9:     "EdgeFlowType",
  10:     "EntityClass",
  11:     "RelationType",
  12:     "Provenance",
  13: ]



FILE: dkg\parsers\__init__.py

   1: from __future__ import annotations
   2: 
   3: from abc import ABC, abstractmethod
   4: from typing import Dict
   5: 
   6: from ..core.graph import DKGEdge, DKGNode
   7: from ..builders.graph_updater import GraphUpdater
   8: from ..pipeline.stages import ParsingStage
   9: 
  10: 
  11: class ConstraintParser(ABC):
  12:     """ì œì•½ íŒŒì¼ íŒŒì„œ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
  13:     
  14:     @abstractmethod
  15:     def get_stage(self) -> ParsingStage:
  16:         """ì´ íŒŒì„œê°€ ì†í•œ stage ë°˜í™˜"""
  17:         pass
  18:     
  19:     @abstractmethod
  20:     def parse_and_update(
  21:         self,
  22:         filepath: str,
  23:         updater: GraphUpdater,
  24:         nodes: Dict[str, DKGNode],
  25:         edges: Dict[str, DKGEdge],
  26:     ) -> None:
  27:         """
  28:         íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ê·¸ë˜í”„ë¥¼ ì—…ë°ì´íŠ¸.
  29:         
  30:         Args:
  31:             filepath: íŒŒì‹±í•  íŒŒì¼ ê²½ë¡œ
  32:             updater: GraphUpdater ì¸ìŠ¤í„´ìŠ¤
  33:             nodes: ê¸°ì¡´ ë…¸ë“œ ë”•ì…”ë„ˆë¦¬
  34:             edges: ê¸°ì¡´ ì—£ì§€ ë”•ì…”ë„ˆë¦¬
  35:         """
  36:         pass



FILE: dkg\pipeline\__init__.py

   1: """Pipeline and processing stages."""
   2: from .pipeline import *
   3: from .stages import *
   4: 
   5: __all__ = []



FILE: dkg\timing\__init__.py

   1: """Timing analysis modules."""
   2: from .timing_aggregator import *
   3: from .timing_analysis_example import *
   4: from .timing_integration import *
   5: 
   6: __all__ = []



FILE: dkg\utils\__init__.py

   1: """Utility modules."""
   2: from .utils import *
   3: from .debug import *
   4: from .config import *
   5: 
   6: __all__ = []



FILE: main.py

   1: from __future__ import annotations
   2: from dkg.utils.config import YosysConfig
   3: from dkg.utils.debug import (
   4:     plot_subgraph,
   5:     print_fanout_summary,
   6:     print_graph_summary,
   7:     print_sample_node,
   8:     trace_signal,
   9: )
  10: from dkg.pipeline import DKGPipeline
  11: from dkg.builders.supergraph import GraphViewType, GraphContext
  12: 
  13: # ì„¤ì •ì€ ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìœ ì§€
  14: DEFAULT_CONFIG = YosysConfig(
  15:     src_dir_win=r"C:\Users\User\NetMind\êµ¬í˜„\ì˜ˆì‹œ",
  16:     out_json_win=r"C:\Users\User\NetMind\êµ¬í˜„\design.json",
  17:     top_module="riscvsingle",
  18: )
  19: 
  20: def main(config: YosysConfig, debug: bool = True) -> None:
  21:     # 1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
  22:     pipeline = DKGPipeline(config)
  23:     print("ğŸš€ DKG Pipeline Initialized.")
  24: 
  25:     # 2. Stage 1: RTL íŒŒì‹± (í•„ìˆ˜)
  26:     pipeline.run_rtl_stage()
  27:     print("âœ… RTL Stage Completed.")
  28: 
  29:     # 3. (ì˜µì…˜) ì œì•½ ì¡°ê±´ ë° íƒ€ì´ë° ë¦¬í¬íŠ¸ ì¶”ê°€
  30:     # ì‹¤ì œ íŒŒì¼ ê²½ë¡œê°€ ìˆë‹¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.
  31:     # pipeline.add_constraints(r"C:\Path\To\design.sdc")
  32:     # pipeline.add_constraints(r"C:\Path\To\design.xdc")
  33:     # pipeline.add_timing_report(r"C:\Path\To\timing.rpt")
  34:     # pipeline.add_floorplan(r"C:\Path\To\design.tcl")  # Design Context ê°ì§€ìš©
  35: 
  36:     # 4. Stage 4: SuperGraph êµ¬ì¶• (Task 12: ì •ì±… ë¶„ê¸° ì ìš©)
  37:     # ViewTypeê³¼ ContextëŠ” í•„ìš”ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥ (ê¸°ë³¸ê°’: Connectivity, Design)
  38:     pipeline.build_supergraph(view=GraphViewType.Connectivity)
  39:     
  40:     # ê·¸ë˜í”„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
  41:     nodes, edges = pipeline.get_graph()
  42:     supergraph = pipeline.supergraph
  43: 
  44:     # 5. ë””ë²„ê·¸ ì¶œë ¥
  45:     if debug:
  46:         print("\n" + "="*40)
  47:         print("ğŸ” Debug Summary")
  48:         print("="*40)
  49:         
  50:         # ê¸°ë³¸ ê·¸ë˜í”„ ìš”ì•½
  51:         # (Note: wires/cells ì •ë³´ëŠ” pipeline ë‚´ë¶€ì— ìº¡ìŠí™”ë˜ì–´ ìˆì–´ 
  52:         #  debug.py í•¨ìˆ˜ë“¤ì´ wires/cellsë¥¼ ìš”êµ¬í•˜ë©´ ì§ì ‘ ì ‘ê·¼ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ.
  53:         #  ì—¬ê¸°ì„œëŠ” nodes/edges ìœ„ì£¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.)
  54:         print(f"DKG Nodes: {len(nodes)}")
  55:         print(f"DKG Edges: {len(edges)}")
  56:         
  57:         if supergraph:
  58:             print("-" * 20)
  59:             print(f"Super Nodes: {len(supergraph.super_nodes)}")
  60:             print(f"Super Edges: {len(supergraph.super_edges)}")
  61:             print("-" * 20)
  62:             
  63:             # SuperGraph ìƒ˜í”Œ ì¶œë ¥ (ì²« 3ê°œ)
  64:             print("\n[Sample SuperNodes]")
  65:             for i, sn in enumerate(list(supergraph.super_nodes.values())[:3]):
  66:                 print(f"  {sn.display_name} ({sn.super_class.value}): contains {len(sn.member_nodes)} nodes")
  67: 
  68:         print_sample_node(nodes, edges)
  69:         # print_fanout_summary(wires) # wires ê°ì²´ê°€ í•„ìš”í•˜ë©´ pipeline ë‚´ë¶€ ì ‘ê·¼ í•„ìš”
  70:         
  71:         # ì‹œê°í™” (Matplotlib)
  72:         plot_subgraph(nodes, edges, limit=30)
  73: 
  74: if __name__ == "__main__":
  75:     main(DEFAULT_CONFIG, debug=True)


FILE: AICollector.py

   1: from pathlib import Path
   2: 
   3: def collect_py_files(output_txt: str = "result.txt"):
   4:     base_path = Path(__file__).parent
   5:     py_files = list(base_path.rglob("*.py"))
   6: 
   7:     def sort_key(path: Path):
   8:         name = path.name.lower()
   9:         if name == "__init__.py":
  10:             return (0, str(path))
  11:         if name == "main.py":
  12:             return (1, str(path))
  13:         return (2, str(path))
  14: 
  15:     py_files.sort(key=sort_key)
  16: 
  17:     with open(output_txt, "w", encoding="utf-8") as out:
  18:         for py_file in py_files:
  19:             relative_path = py_file.relative_to(base_path)
  20: 
  21:             out.write("\n")
  22:             out.write(f"FILE: {relative_path}\n")
  23:             out.write("\n")
  24: 
  25:             try:
  26:                 with open(py_file, "r", encoding="utf-8") as f:
  27:                     for idx, line in enumerate(f, start=1):
  28:                         out.write(f"{idx:4d}: {line}")
  29:             except Exception as e:
  30:                 out.write(f"[ERROR] íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\n")
  31: 
  32:             out.write("\n\n")
  33: 
  34: 
  35: if __name__ == "__main__":
  36:     collect_py_files()



FILE: dkg\builders\constraint_projector.py

   1: """
   2: Constraint Projector: Raw Constraint â†’ Graph Semantic Projection
   3: 
   4: ì´ ëª¨ë“ˆì€ SDC/XDC/TCL íŒŒì„œê°€ ì¶”ì¶œí•œ raw constraintë¥¼
   5: DKG ê·¸ë˜í”„ì˜ semanticìœ¼ë¡œ íˆ¬ì˜í•©ë‹ˆë‹¤.
   6: 
   7: ì²˜ë¦¬í•˜ëŠ” ì œì•½:
   8: - Clock Domain: create_clock
   9: - Timing Exceptions: set_false_path, set_multicycle_path
  10: - Delay Constraints: set_max_delay, set_min_delay
  11: - I/O Timing: set_input_delay, set_output_delay
  12: - Physical Constraints: LOC, IOSTANDARD, pblock
  13: """
  14: from __future__ import annotations
  15: 
  16: import re
  17: from dataclasses import dataclass
  18: from typing import Dict, List, Optional, Set
  19: 
  20: from ..core.graph import DKGEdge, DKGNode
  21: from .graph_updater import GraphUpdater
  22: from ..pipeline.stages import FieldSource, ParsingStage
  23: 
  24: 
  25: # ============================================================================
  26: # Raw Constraint Data Classes
  27: # ============================================================================
  28: 
  29: 
  30: @dataclass
  31: class ClockConstraint:
  32:     """create_clock ì œì•½"""
  33: 
  34:     clock_name: str
  35:     period: Optional[float] = None  # ns
  36:     waveform: Optional[List[float]] = None  # [rise_edge, fall_edge]
  37:     target_ports: Optional[List[str]] = None  # get_ports / get_pins
  38: 
  39: 
  40: @dataclass
  41: class FalsePathConstraint:
  42:     """set_false_path ì œì•½"""
  43: 
  44:     from_targets: Optional[List[str]] = None  # -from [get_pins ...]
  45:     to_targets: Optional[List[str]] = None  # -to [get_pins ...]
  46:     through_targets: Optional[List[str]] = None  # -through [get_pins ...]
  47: 
  48: 
  49: @dataclass
  50: class MulticyclePathConstraint:
  51:     """set_multicycle_path ì œì•½"""
  52: 
  53:     cycles: int
  54:     path_type: str  # "setup" or "hold"
  55:     from_targets: Optional[List[str]] = None
  56:     to_targets: Optional[List[str]] = None
  57: 
  58: 
  59: @dataclass
  60: class DelayConstraint:
  61:     """set_max_delay / set_min_delay ì œì•½"""
  62: 
  63:     constraint_type: str  # "max" or "min"
  64:     delay_value: float  # ns
  65:     from_targets: Optional[List[str]] = None
  66:     to_targets: Optional[List[str]] = None
  67: 
  68: 
  69: @dataclass
  70: class IOTimingConstraint:
  71:     """set_input_delay / set_output_delay ì œì•½"""
  72: 
  73:     constraint_type: str  # "input" or "output"
  74:     delay_value: float  # ns
  75:     clock_ref: Optional[str] = None
  76:     target_ports: Optional[List[str]] = None
  77: 
  78: 
  79: # ============================================================================
  80: # Constraint Projector
  81: # ============================================================================
  82: 
  83: 
  84: class ConstraintProjector:
  85:     """
  86:     Constraintë¥¼ DKG ê·¸ë˜í”„ì— íˆ¬ì˜í•˜ëŠ” í´ë˜ìŠ¤.
  87: 
  88:     ì£¼ìš” ê¸°ëŠ¥:
  89:     1. ì œì•½ì˜ target (get_ports, get_pins, get_cells)ë¥¼ ì‹¤ì œ ë…¸ë“œ/ì—£ì§€ì— ë§¤ì¹­
  90:     2. GraphUpdaterë¥¼ í†µí•´ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
  91:     3. Provenance ê¸°ë¡
  92:     """
  93: 
  94:     def __init__(
  95:         self,
  96:         nodes: Dict[str, DKGNode],
  97:         edges: Dict[str, DKGEdge],
  98:         updater: GraphUpdater,
  99:     ):
 100:         self.nodes = nodes
 101:         self.edges = edges
 102:         self.updater = updater
 103: 
 104:     # ========================================================================
 105:     # Target Matching Utilities
 106:     # ========================================================================
 107: 
 108:     def _match_node_by_pattern(self, pattern: str) -> List[str]:
 109:         """
 110:         íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ë…¸ë“œ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 111: 
 112:         íŒ¨í„´ ì˜ˆì‹œ:
 113:         - "clk" â†’ local_nameì´ 'clk'ì¸ ë…¸ë“œ
 114:         - "cpu/pc_reg[0]" â†’ hier_pathê°€ ë§¤ì¹­ë˜ëŠ” ë…¸ë“œ
 115:         - "*_reg*" â†’ ì™€ì¼ë“œì¹´ë“œ ë§¤ì¹­
 116: 
 117:         Returns:
 118:             ë§¤ì¹­ëœ node_id ë¦¬ìŠ¤íŠ¸
 119:         """
 120:         matched_ids: List[str] = []
 121: 
 122:         # ê°„ë‹¨í•œ ì™€ì¼ë“œì¹´ë“œ â†’ ì •ê·œì‹ ë³€í™˜
 123:         regex_pattern = pattern.replace("*", ".*").replace("?", ".")
 124:         regex = re.compile(regex_pattern, re.IGNORECASE)
 125: 
 126:         for node_id, node in self.nodes.items():
 127:             # hier_path, local_name, canonical_name ëª¨ë‘ ì²´í¬
 128:             candidates = [node.hier_path, node.local_name]
 129:             if node.canonical_name:
 130:                 candidates.append(node.canonical_name)
 131: 
 132:             if any(regex.fullmatch(c) for c in candidates):
 133:                 matched_ids.append(node_id)
 134: 
 135:         return matched_ids
 136: 
 137:     def _match_edge_by_endpoints(
 138:         self, from_pattern: Optional[str], to_pattern: Optional[str]
 139:     ) -> List[str]:
 140:         """
 141:         ì‹œì‘ì ê³¼ ëì  íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ì—£ì§€ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 142: 
 143:         Args:
 144:             from_pattern: ì‹œì‘ ë…¸ë“œ íŒ¨í„´ (Noneì´ë©´ ëª¨ë“  ë…¸ë“œ)
 145:             to_pattern: ë ë…¸ë“œ íŒ¨í„´ (Noneì´ë©´ ëª¨ë“  ë…¸ë“œ)
 146: 
 147:         Returns:
 148:             ë§¤ì¹­ëœ edge_id ë¦¬ìŠ¤íŠ¸
 149:         """
 150:         matched_ids: List[str] = []
 151: 
 152:         # íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ë…¸ë“œë“¤ ì°¾ê¸°
 153:         from_nodes = (
 154:             set(self._match_node_by_pattern(from_pattern)) if from_pattern else None
 155:         )
 156:         to_nodes = (
 157:             set(self._match_node_by_pattern(to_pattern)) if to_pattern else None
 158:         )
 159: 
 160:         for edge_id, edge in self.edges.items():
 161:             # from_nodesê°€ ì§€ì •ë˜ì—ˆìœ¼ë©´ src_nodeê°€ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
 162:             if from_nodes is not None and edge.src_node not in from_nodes:
 163:                 continue
 164: 
 165:             # to_nodesê°€ ì§€ì •ë˜ì—ˆìœ¼ë©´ dst_nodeê°€ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
 166:             if to_nodes is not None and edge.dst_node not in to_nodes:
 167:                 continue
 168: 
 169:             matched_ids.append(edge_id)
 170: 
 171:         return matched_ids
 172: 
 173:     # ========================================================================
 174:     # Clock Constraint Projection
 175:     # ========================================================================
 176: 
 177:     def project_clock_constraint(
 178:         self,
 179:         constraint: ClockConstraint,
 180:         filepath: str,
 181:         line_num: int,
 182:     ) -> None:
 183:         """
 184:         create_clock ì œì•½ì„ ê·¸ë˜í”„ì— íˆ¬ì˜í•©ë‹ˆë‹¤.
 185: 
 186:         ë§¤ì¹­ë˜ëŠ” ëª¨ë“  ë…¸ë“œì˜ clock_domain í•„ë“œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
 187:         """
 188:         if not constraint.target_ports:
 189:             return
 190: 
 191:         for port_pattern in constraint.target_ports:
 192:             matched_nodes = self._match_node_by_pattern(port_pattern)
 193: 
 194:             for node_id in matched_nodes:
 195:                 self.updater.update_node_field(
 196:                     node_id,
 197:                     "clock_domain",
 198:                     constraint.clock_name,
 199:                     FieldSource.DECLARED,
 200:                     ParsingStage.CONSTRAINTS,
 201:                     filepath,
 202:                     line_num,
 203:                 )
 204: 
 205:                 # í´ëŸ­ ì£¼ê¸°ë„ attributesì— ì €ì¥
 206:                 if constraint.period is not None:
 207:                     node = self.nodes[node_id]
 208:                     new_attrs = dict(node.attributes)
 209:                     new_attrs["clock_period"] = str(constraint.period)
 210:                     self.updater.update_node_field(
 211:                         node_id,
 212:                         "attributes",
 213:                         new_attrs,
 214:                         FieldSource.DECLARED,
 215:                         ParsingStage.CONSTRAINTS,
 216:                         filepath,
 217:                         line_num,
 218:                     )
 219: 
 220:     # ========================================================================
 221:     # False Path Constraint Projection
 222:     # ========================================================================
 223: 
 224:     def project_false_path_constraint(
 225:         self,
 226:         constraint: FalsePathConstraint,
 227:         filepath: str,
 228:         line_num: int,
 229:     ) -> None:
 230:         """
 231:         set_false_path ì œì•½ì„ ê·¸ë˜í”„ì— íˆ¬ì˜í•©ë‹ˆë‹¤.
 232: 
 233:         ë§¤ì¹­ë˜ëŠ” ì—£ì§€ì˜ timing_exception í•„ë“œë¥¼ 'false_path'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
 234:         """
 235:         # -fromê³¼ -toë¡œ ì—£ì§€ ì°¾ê¸°
 236:         from_pattern = constraint.from_targets[0] if constraint.from_targets else None
 237:         to_pattern = constraint.to_targets[0] if constraint.to_targets else None
 238: 
 239:         matched_edges = self._match_edge_by_endpoints(from_pattern, to_pattern)
 240: 
 241:         for edge_id in matched_edges:
 242:             self.updater.update_edge_field(
 243:                 edge_id,
 244:                 "timing_exception",
 245:                 "false_path",
 246:                 FieldSource.DECLARED,
 247:                 ParsingStage.CONSTRAINTS,
 248:                 filepath,
 249:                 line_num,
 250:             )
 251: 
 252:     # ========================================================================
 253:     # Multicycle Path Constraint Projection
 254:     # ========================================================================
 255: 
 256:     def project_multicycle_path_constraint(
 257:         self,
 258:         constraint: MulticyclePathConstraint,
 259:         filepath: str,
 260:         line_num: int,
 261:     ) -> None:
 262:         """
 263:         set_multicycle_path ì œì•½ì„ ê·¸ë˜í”„ì— íˆ¬ì˜í•©ë‹ˆë‹¤.
 264: 
 265:         ë§¤ì¹­ë˜ëŠ” ì—£ì§€ì˜ timing_exceptionì„ 'multicycle_{N}_{type}'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
 266:         ì˜ˆ: 'multicycle_2_setup'
 267:         """
 268:         from_pattern = constraint.from_targets[0] if constraint.from_targets else None
 269:         to_pattern = constraint.to_targets[0] if constraint.to_targets else None
 270: 
 271:         matched_edges = self._match_edge_by_endpoints(from_pattern, to_pattern)
 272: 
 273:         exception_value = f"multicycle_{constraint.cycles}_{constraint.path_type}"
 274: 
 275:         for edge_id in matched_edges:
 276:             self.updater.update_edge_field(
 277:                 edge_id,
 278:                 "timing_exception",
 279:                 exception_value,
 280:                 FieldSource.DECLARED,
 281:                 ParsingStage.CONSTRAINTS,
 282:                 filepath,
 283:                 line_num,
 284:             )
 285: 
 286:     # ========================================================================
 287:     # Delay Constraint Projection
 288:     # ========================================================================
 289: 
 290:     def project_delay_constraint(
 291:         self,
 292:         constraint: DelayConstraint,
 293:         filepath: str,
 294:         line_num: int,
 295:     ) -> None:
 296:         """
 297:         set_max_delay / set_min_delay ì œì•½ì„ ê·¸ë˜í”„ì— íˆ¬ì˜í•©ë‹ˆë‹¤.
 298: 
 299:         ë§¤ì¹­ë˜ëŠ” ì—£ì§€ì˜ parametersì— 'max_delay' ë˜ëŠ” 'min_delay'ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
 300:         """
 301:         from_pattern = constraint.from_targets[0] if constraint.from_targets else None
 302:         to_pattern = constraint.to_targets[0] if constraint.to_targets else None
 303: 
 304:         matched_edges = self._match_edge_by_endpoints(from_pattern, to_pattern)
 305: 
 306:         param_key = f"{constraint.constraint_type}_delay"
 307: 
 308:         for edge_id in matched_edges:
 309:             edge = self.edges[edge_id]
 310:             new_params = dict(edge.parameters)
 311:             new_params[param_key] = constraint.delay_value
 312: 
 313:             self.updater.update_edge_field(
 314:                 edge_id,
 315:                 "parameters",
 316:                 new_params,
 317:                 FieldSource.DECLARED,
 318:                 ParsingStage.CONSTRAINTS,
 319:                 filepath,
 320:                 line_num,
 321:             )
 322: 
 323:     # ========================================================================
 324:     # I/O Timing Constraint Projection
 325:     # ========================================================================
 326: 
 327:     def project_io_timing_constraint(
 328:         self,
 329:         constraint: IOTimingConstraint,
 330:         filepath: str,
 331:         line_num: int,
 332:     ) -> None:
 333:         """
 334:         set_input_delay / set_output_delay ì œì•½ì„ ê·¸ë˜í”„ì— íˆ¬ì˜í•©ë‹ˆë‹¤.
 335: 
 336:         ë§¤ì¹­ë˜ëŠ” I/O í¬íŠ¸ ë…¸ë“œì˜ attributesì— 'input_delay' ë˜ëŠ” 'output_delay'ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
 337:         """
 338:         if not constraint.target_ports:
 339:             return
 340: 
 341:         attr_key = f"{constraint.constraint_type}_delay"
 342: 
 343:         for port_pattern in constraint.target_ports:
 344:             matched_nodes = self._match_node_by_pattern(port_pattern)
 345: 
 346:             for node_id in matched_nodes:
 347:                 node = self.nodes[node_id]
 348:                 new_attrs = dict(node.attributes)
 349:                 new_attrs[attr_key] = str(constraint.delay_value)
 350: 
 351:                 # í´ëŸ­ ì°¸ì¡°ë„ ì €ì¥
 352:                 if constraint.clock_ref:
 353:                     new_attrs[f"{constraint.constraint_type}_delay_clock"] = (
 354:                         constraint.clock_ref
 355:                     )
 356: 
 357:                 self.updater.update_node_field(
 358:                     node_id,
 359:                     "attributes",
 360:                     new_attrs,
 361:                     FieldSource.DECLARED,
 362:                     ParsingStage.CONSTRAINTS,
 363:                     filepath,
 364:                     line_num,
 365:                 )
 366: 
 367: 
 368: # ============================================================================
 369: # High-Level Projection API
 370: # ============================================================================
 371: 
 372: 
 373: def project_constraints_to_graph(
 374:     constraints: List,  # ClockConstraint | FalsePathConstraint | ...
 375:     nodes: Dict[str, DKGNode],
 376:     edges: Dict[str, DKGEdge],
 377:     updater: GraphUpdater,
 378:     filepath: str,
 379:     line_num: int = 0,
 380: ) -> None:
 381:     """
 382:     ì—¬ëŸ¬ ì œì•½ì¡°ê±´ì„ í•œ ë²ˆì— ê·¸ë˜í”„ì— íˆ¬ì˜í•©ë‹ˆë‹¤.
 383: 
 384:     Args:
 385:         constraints: ì œì•½ì¡°ê±´ ê°ì²´ ë¦¬ìŠ¤íŠ¸
 386:         nodes: DKG ë…¸ë“œ ë”•ì…”ë„ˆë¦¬
 387:         edges: DKG ì—£ì§€ ë”•ì…”ë„ˆë¦¬
 388:         updater: GraphUpdater
 389:         filepath: ì œì•½ íŒŒì¼ ê²½ë¡œ
 390:         line_num: ì œì•½ì´ ì •ì˜ëœ ì¤„ ë²ˆí˜¸
 391:     """
 392:     projector = ConstraintProjector(nodes, edges, updater)
 393: 
 394:     for constraint in constraints:
 395:         if isinstance(constraint, ClockConstraint):
 396:             projector.project_clock_constraint(constraint, filepath, line_num)
 397:         elif isinstance(constraint, FalsePathConstraint):
 398:             projector.project_false_path_constraint(constraint, filepath, line_num)
 399:         elif isinstance(constraint, MulticyclePathConstraint):
 400:             projector.project_multicycle_path_constraint(
 401:                 constraint, filepath, line_num
 402:             )
 403:         elif isinstance(constraint, DelayConstraint):
 404:             projector.project_delay_constraint(constraint, filepath, line_num)
 405:         elif isinstance(constraint, IOTimingConstraint):
 406:             projector.project_io_timing_constraint(constraint, filepath, line_num)
 407: 
 408: 
 409: # ============================================================================
 410: # Parser Integration Helpers
 411: # ============================================================================
 412: 
 413: 
 414: def parse_sdc_create_clock(line: str) -> Optional[ClockConstraint]:
 415:     """
 416:     SDCì˜ create_clock ëª…ë ¹ì„ íŒŒì‹±í•˜ì—¬ ClockConstraintë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 417: 
 418:     ì˜ˆ: create_clock -name clk -period 10 [get_ports clk]
 419:     """
 420:     match = re.search(r"-name\s+(\w+)", line)
 421:     if not match:
 422:         return None
 423: 
 424:     clock_name = match.group(1)
 425: 
 426:     # Period ì¶”ì¶œ
 427:     period_match = re.search(r"-period\s+([\d.]+)", line)
 428:     period = float(period_match.group(1)) if period_match else None
 429: 
 430:     # Target ports ì¶”ì¶œ
 431:     port_match = re.search(r"get_ports\s+(\w+)", line)
 432:     target_ports = [port_match.group(1)] if port_match else []
 433: 
 434:     return ClockConstraint(
 435:         clock_name=clock_name, period=period, target_ports=target_ports
 436:     )
 437: 
 438: 
 439: def parse_sdc_false_path(line: str) -> Optional[FalsePathConstraint]:
 440:     """
 441:     SDCì˜ set_false_path ëª…ë ¹ì„ íŒŒì‹±í•˜ì—¬ FalsePathConstraintë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 442: 
 443:     ì˜ˆ: set_false_path -from [get_pins cpu/reset_reg/Q] -to [get_pins *]
 444:     """
 445:     from_match = re.search(r"-from\s+\[get_\w+\s+([^\]]+)\]", line)
 446:     to_match = re.search(r"-to\s+\[get_\w+\s+([^\]]+)\]", line)
 447: 
 448:     from_targets = [from_match.group(1)] if from_match else None
 449:     to_targets = [to_match.group(1)] if to_match else None
 450: 
 451:     if not from_targets and not to_targets:
 452:         return None
 453: 
 454:     return FalsePathConstraint(from_targets=from_targets, to_targets=to_targets)
 455: 
 456: 
 457: def parse_sdc_multicycle_path(line: str) -> Optional[MulticyclePathConstraint]:
 458:     """
 459:     SDCì˜ set_multicycle_path ëª…ë ¹ì„ íŒŒì‹±í•˜ì—¬ MulticyclePathConstraintë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 460: 
 461:     ì˜ˆ: set_multicycle_path 2 -setup -from [get_pins src] -to [get_pins dst]
 462:     """
 463:     cycles_match = re.search(r"set_multicycle_path\s+(\d+)", line)
 464:     if not cycles_match:
 465:         return None
 466: 
 467:     cycles = int(cycles_match.group(1))
 468:     path_type = "setup" if "-setup" in line else "hold" if "-hold" in line else "setup"
 469: 
 470:     from_match = re.search(r"-from\s+\[get_\w+\s+([^\]]+)\]", line)
 471:     to_match = re.search(r"-to\s+\[get_\w+\s+([^\]]+)\]", line)
 472: 
 473:     from_targets = [from_match.group(1)] if from_match else None
 474:     to_targets = [to_match.group(1)] if to_match else None
 475: 
 476:     return MulticyclePathConstraint(
 477:         cycles=cycles,
 478:         path_type=path_type,
 479:         from_targets=from_targets,
 480:         to_targets=to_targets,
 481:     )



FILE: dkg\builders\graph_build.py

   1: from __future__ import annotations
   2: 
   3: from collections import defaultdict
   4: from typing import Dict, Iterable, List, Optional, Tuple
   5: 
   6: from ..core.graph import (
   7:     DKGEdge,
   8:     DKGNode,
   9:     EdgeFlowType,
  10:     EntityClass,
  11:     RelationType,
  12:     make_node_canonical_name,
  13: )
  14: from ..core.ir import CellIR, Wire
  15: from ..core.provenance import Provenance, add_provenance, merge_provenances_edges
  16: from ..utils import (
  17:     is_active_low,
  18:     is_clock_name,
  19:     is_reset_name,
  20:     parse_src,
  21:     split_signal_bit,
  22:     stable_hash,
  23: )
  24: 
  25: 
  26: def get_wire(wires: Dict[int, Wire], wid) -> Optional[Wire]:
  27:     if isinstance(wid, str):
  28:         return None
  29:     if wid not in wires:
  30:         wires[wid] = Wire(wid)
  31:     return wires[wid]
  32: 
  33: 
  34: def build_wires_and_cells(yosys: dict) -> Tuple[Dict[int, Wire], List[CellIR]]:
  35:     wires: Dict[int, Wire] = {}
  36:     cells: List[CellIR] = []
  37: 
  38:     for mod in yosys.get("modules", {}).values():
  39:         for netname, netinfo in mod.get("netnames", {}).items():
  40:             src = netinfo.get("src")
  41:             for wid in netinfo.get("bits", []):
  42:                 w = get_wire(wires, wid)
  43:                 if w:
  44:                     w.name = netname
  45:                     w.src = src
  46: 
  47:     for mod_name, mod in yosys.get("modules", {}).items():
  48:         for cname, c in mod.get("cells", {}).items():
  49:             cells.append(
  50:                 CellIR(
  51:                     name=cname,
  52:                     type=c["type"],
  53:                     module=mod_name,
  54:                     port_dirs=c["port_directions"],
  55:                     connections=c["connections"],
  56:                     src=c.get("src"),
  57:                 )
  58:             )
  59: 
  60:     return wires, cells
  61: 
  62: 
  63: def map_cell_type(t: str) -> EntityClass:
  64:     if t in ["$adff", "$dff"]:
  65:         return EntityClass.FLIP_FLOP
  66:     if t in ["$mux", "$pmux"]:
  67:         return EntityClass.MUX
  68:     if t in ["$add", "$sub", "$and", "$or"]:
  69:         return EntityClass.RTL_BLOCK
  70:     return EntityClass.RTL_BLOCK
  71: 
  72: 
  73: def cell_signature(cell: CellIR) -> str:
  74:     ports = sorted(
  75:         f"{p}:{cell.port_dirs[p]}:{len(bits)}"
  76:         for p, bits in cell.connections.items()
  77:     )
  78:     return "|".join(
  79:         [
  80:             cell.type,
  81:             cell.module,
  82:             ",".join(ports),
  83:         ]
  84:     )
  85: 
  86: 
  87: def signal_signature(e: DKGEdge) -> str:
  88:     if e.bit_range:
  89:         msb, lsb = e.bit_range
  90:         return f"{e.signal_name}[{msb}:{lsb}]"
  91:     return e.signal_name
  92: 
  93: 
  94: def edge_signature(e: DKGEdge) -> str:
  95:     return "|".join(
  96:         [
  97:             e.src_node,
  98:             e.dst_node,
  99:             e.relation_type.value,
 100:             e.flow_type.value,
 101:             signal_signature(e),
 102:         ]
 103:     )
 104: 
 105: 
 106: def make_edge_id(e: DKGEdge) -> str:
 107:     sig = edge_signature(e)
 108:     h = stable_hash(sig)
 109:     return f"E_{e.relation_type.value}_{h}"
 110: 
 111: 
 112: def make_node_id(cell: CellIR) -> str:
 113:     sig = cell_signature(cell)
 114:     return f"N_{map_cell_type(cell.type).value}_{stable_hash(sig)}"
 115: 
 116: 
 117: def connect_wires_to_cells(wires: Dict[int, Wire], cells: Iterable[CellIR]) -> None:
 118:     cell_id_map: Dict[str, str] = {}
 119:     for cell in cells:
 120:         cell_key = f"{cell.module}.{cell.name}"
 121:         cell_id_map[cell_key] = make_node_id(cell)
 122: 
 123:     for cell in cells:
 124:         node_id = cell_id_map[f"{cell.module}.{cell.name}"]
 125:         for port, bits in cell.connections.items():
 126:             direction = cell.port_dirs[port]
 127:             for wid in bits:
 128:                 w = get_wire(wires, wid)
 129:                 if not w:
 130:                     continue
 131:                 if direction == "output":
 132:                     w.drivers.append(node_id)
 133:                 else:
 134:                     w.loads.append(node_id)
 135: 
 136: 
 137: def detect_clock_reset_from_ff_cells(
 138:     cells: List[CellIR],
 139:     wires: Dict[int, Wire],
 140: ) -> Tuple[set[str], set[str]]:
 141:     """
 142:     Yosys FF cell í¬íŠ¸ ì •ë³´ì—ì„œ clock/reset ì‹ í˜¸ ì§ì ‘ ì¶”ì¶œ.
 143:     
 144:     êµ¬ì¡°ì  ë¶„ì„ì„ í†µí•´ ì‹ ë¢°ë„ ë†’ì€ ì‹ë³„:
 145:     - $dff, $adff, $sdff ë“±ì˜ CLK í¬íŠ¸ â†’ clock
 146:     - ARST, SRST í¬íŠ¸ â†’ reset
 147:     """
 148:     clock_nets: set[str] = set()
 149:     reset_nets: set[str] = set()
 150:     
 151:     # FF cell íƒ€ì… ì •ì˜
 152:     ff_cell_types = {
 153:         "$dff", "$adff", "$sdff",
 154:         "$dffe", "$sdffe", "$aldff", "$aldffe",
 155:     }
 156:     
 157:     for cell in cells:
 158:         if cell.type not in ff_cell_types:
 159:             continue
 160:         
 161:         # CLK í¬íŠ¸ ì°¾ê¸°
 162:         if "CLK" in cell.connections:
 163:             clk_wids = cell.connections["CLK"]
 164:             for wid in clk_wids:
 165:                 w = get_wire(wires, wid)
 166:                 if w and w.name:
 167:                     clock_nets.add(w.name)
 168:         
 169:         # ë¹„ë™ê¸° ë¦¬ì…‹ í¬íŠ¸ (ARST, ARST_N ë“±)
 170:         async_reset_ports = {"ARST", "ARST_N", "NRST", "NRESET"}
 171:         for port in async_reset_ports:
 172:             if port in cell.connections:
 173:                 rst_wids = cell.connections[port]
 174:                 for wid in rst_wids:
 175:                     w = get_wire(wires, wid)
 176:                     if w and w.name:
 177:                         reset_nets.add(w.name)
 178:         
 179:         # ë™ê¸° ë¦¬ì…‹ í¬íŠ¸ (SRST, SRST_N ë“±)
 180:         sync_reset_ports = {"SRST", "SRST_N", "SR", "R", "RST"}
 181:         for port in sync_reset_ports:
 182:             if port in cell.connections:
 183:                 rst_wids = cell.connections[port]
 184:                 for wid in rst_wids:
 185:                     w = get_wire(wires, wid)
 186:                     if w and w.name:
 187:                         reset_nets.add(w.name)
 188:     
 189:     return clock_nets, reset_nets
 190: 
 191: 
 192: def detect_clock_reset_signals(
 193:     nodes: Dict[str, DKGNode],
 194:     edges: Dict[str, DKGEdge],
 195:     cells: List[CellIR],
 196:     wires: Dict[int, Wire],
 197: ) -> Tuple[set[str], set[str]]:
 198:     """
 199:     Clock/Reset ì‹ í˜¸ ì‹ë³„ (ë‹¤ë‹¨ê³„ ìš°ì„ ìˆœìœ„).
 200:     
 201:     1ï¸âƒ£ êµ¬ì¡°ì  ë¶„ì„: FF cell í¬íŠ¸ ì •ë³´ (ë†’ì€ ì‹ ë¢°ë„)
 202:     2ï¸âƒ£ ì‹ í˜¸ ë¶„ì„: edgeì˜ flow ì •ë³´
 203:     3ï¸âƒ£ ì´ë¦„ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±: íŒ¨í„´ ë§¤ì¹­ (ë‚®ì€ ì‹ ë¢°ë„)
 204:     """
 205:     # Stage 1: êµ¬ì¡°ì  ë¶„ì„ (FF cell í¬íŠ¸)
 206:     clock_nets, reset_nets = detect_clock_reset_from_ff_cells(cells, wires)
 207:     
 208:     # Stage 2: ì—£ì§€ ì‹ í˜¸ ì´ë¦„ ê¸°ë°˜ (ì¶”ë¡ )
 209:     for e in edges.values():
 210:         if is_clock_name(e.signal_name):
 211:             clock_nets.add(e.signal_name)
 212:         if is_reset_name(e.signal_name):
 213:             reset_nets.add(e.signal_name)
 214:     
 215:     # Stage 3: FF ì…ë ¥ ì‹ í˜¸ í™•ì¸ (êµ¬ì¡°ì  ì¬ê²€ì¦)
 216:     for n in nodes.values():
 217:         if n.entity_class != EntityClass.FLIP_FLOP:
 218:             continue
 219:         for eid in n.in_edges:
 220:             e = edges[eid]
 221:             # ì´ë¯¸ ì‹ë³„ëœ ê²ƒì€ í™•ì¸, ì•„ë‹ˆë©´ ì¶”ê°€ í™•ì¸
 222:             if is_clock_name(e.signal_name) and e.signal_name not in clock_nets:
 223:                 clock_nets.add(e.signal_name)
 224:             if is_reset_name(e.signal_name) and e.signal_name not in reset_nets:
 225:                 reset_nets.add(e.signal_name)
 226:     
 227:     return clock_nets, reset_nets
 228: 
 229: 
 230: def assign_edge_flow_types(
 231:     nodes: Dict[str, DKGNode],
 232:     edges: Dict[str, DKGEdge],
 233:     clock_nets: set[str],
 234:     reset_nets: set[str],
 235: ) -> None:
 236:     for e in edges.values():
 237:         if e.signal_name in clock_nets:
 238:             e.flow_type = EdgeFlowType.CLOCK_TREE
 239:             continue
 240:         if e.signal_name in reset_nets:
 241:             e.flow_type = EdgeFlowType.ASYNC_RESET
 242:             continue
 243: 
 244:         src = nodes[e.src_node]
 245:         dst = nodes[e.dst_node]
 246: 
 247:         if src.entity_class == EntityClass.FLIP_FLOP:
 248:             e.flow_type = EdgeFlowType.SEQ_LAUNCH
 249:         elif dst.entity_class == EntityClass.FLIP_FLOP:
 250:             e.flow_type = EdgeFlowType.SEQ_CAPTURE
 251:         else:
 252:             e.flow_type = EdgeFlowType.COMBINATIONAL
 253: 
 254: 
 255: def assign_clock_domains(
 256:     nodes: Dict[str, DKGNode],
 257:     edges: Dict[str, DKGEdge],
 258:     clock_nets: set[str],
 259: ) -> None:
 260:     for n in nodes.values():
 261:         if n.entity_class != EntityClass.FLIP_FLOP:
 262:             continue
 263:         for eid in n.in_edges:
 264:             e = edges[eid]
 265:             if e.signal_name in clock_nets:
 266:                 n.clock_domain = e.signal_name
 267:                 break
 268: 
 269: 
 270: def merge_bit_edges_to_bus(edges: Dict[str, DKGEdge]) -> Dict[str, DKGEdge]:
 271:     groups: Dict[Tuple[str, str, RelationType, EdgeFlowType, str], List[Tuple[Optional[int], DKGEdge]]] = defaultdict(list)
 272: 
 273:     for e in edges.values():
 274:         base, bit = split_signal_bit(e.signal_name)
 275:         key = (e.src_node, e.dst_node, e.relation_type, e.flow_type, base)
 276:         groups[key].append((bit, e))
 277: 
 278:     new_edges: Dict[str, DKGEdge] = {}
 279:     new_eid = 0
 280: 
 281:     for key, items in groups.items():
 282:         _, _, _, _, base = key
 283: 
 284:         if all(bit is None for bit, _ in items):
 285:             for _, e in items:
 286:                 new_edges[e.edge_id] = e
 287:             continue
 288: 
 289:         items.sort(key=lambda x: (-1 if x[0] is None else x[0]))
 290: 
 291:         current_bucket: List[Tuple[Optional[int], DKGEdge]] = []
 292:         prev_bit: Optional[int] = None
 293: 
 294:         def flush_bucket(bucket: List[Tuple[Optional[int], DKGEdge]]) -> None:
 295:             nonlocal new_eid
 296:             if not bucket:
 297:                 return
 298: 
 299:             bits = [b for b, _ in bucket if b is not None]
 300:             edges_in_bucket = [e for _, e in bucket]
 301: 
 302:             if len(bits) <= 1:
 303:                 e = edges_in_bucket[0]
 304:                 new_edges[e.edge_id] = e
 305:                 return
 306: 
 307:             msb = max(bits)
 308:             lsb = min(bits)
 309:             base_edge = edges_in_bucket[0]
 310:             merged = DKGEdge(
 311:                 edge_id=f"bus_e{new_eid}",
 312:                 src_node=base_edge.src_node,
 313:                 dst_node=base_edge.dst_node,
 314:                 relation_type=base_edge.relation_type,
 315:                 flow_type=base_edge.flow_type,
 316:                 signal_name=f"{base}[{msb}:{lsb}]",
 317:                 canonical_name=base_edge.canonical_name,
 318:                 bit_range=(msb, lsb),
 319:                 net_id=base_edge.net_id,
 320:                 driver_type=base_edge.driver_type,
 321:                 fanout_count=base_edge.fanout_count,
 322:                 clock_signal=base_edge.clock_signal,
 323:                 reset_signal=base_edge.reset_signal,
 324:                 clock_domain_id=base_edge.clock_domain_id,
 325:                 timing_exception=base_edge.timing_exception,
 326:                 delay=base_edge.delay,
 327:                 arrival_time=base_edge.arrival_time,
 328:                 required_time=base_edge.required_time,
 329:                 slack=base_edge.slack,
 330:                 attributes=dict(base_edge.attributes),
 331:                 provenances=[],
 332:                 primary_provenance=None,
 333:             )
 334: 
 335:             primary, provs = merge_provenances_edges(edges_in_bucket)
 336:             merged.provenances = provs
 337:             merged.primary_provenance = primary
 338:             merged.attributes["merged_bits"] = sorted(bits)
 339: 
 340:             new_edges[merged.edge_id] = merged
 341:             new_eid += 1
 342: 
 343:         for bit, e in items:
 344:             if bit is None:
 345:                 flush_bucket(current_bucket)
 346:                 current_bucket = []
 347:                 new_edges[e.edge_id] = e
 348:                 prev_bit = None
 349:                 continue
 350: 
 351:             if prev_bit is None or bit == prev_bit - 1:
 352:                 current_bucket.append((bit, e))
 353:             else:
 354:                 flush_bucket(current_bucket)
 355:                 current_bucket = [(bit, e)]
 356: 
 357:             prev_bit = bit
 358: 
 359:         flush_bucket(current_bucket)
 360: 
 361:     return new_edges
 362: 
 363: 
 364: def reindex_node_edges(nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge]) -> None:
 365:     for n in nodes.values():
 366:         n.in_edges = []
 367:         n.out_edges = []
 368: 
 369:     for e in edges.values():
 370:         nodes[e.src_node].out_edges.append(e.edge_id)
 371:         nodes[e.dst_node].in_edges.append(e.edge_id)
 372: 
 373: 
 374: def build_nodes_and_edges(
 375:     wires: Dict[int, Wire],
 376:     cells: List[CellIR],
 377: ) -> Tuple[Dict[str, DKGNode], Dict[str, DKGEdge]]:
 378:     connect_wires_to_cells(wires, cells)
 379: 
 380:     nodes: Dict[str, DKGNode] = {}
 381:     for cell in cells:
 382:         node_id = make_node_id(cell)
 383:         node = DKGNode(
 384:             node_id=node_id,
 385:             entity_class=map_cell_type(cell.type),
 386:             hier_path=cell.module,
 387:             local_name=cell.name,
 388:         )
 389:         node.canonical_name = make_node_canonical_name(node)
 390: 
 391:         file, line = parse_src(cell.src)
 392:         prov = Provenance(
 393:             origin_file=file,
 394:             origin_line=line,
 395:             tool_stage="rtl",
 396:             confidence="exact",
 397:         )
 398:         add_provenance(node, prov, make_primary=True)
 399:         nodes[node_id] = node
 400: 
 401:     edges: Dict[str, DKGEdge] = {}
 402:     eid = 0
 403: 
 404:     for w in wires.values():
 405:         for src in w.drivers:
 406:             for dst in w.loads:
 407:                 edge_id = f"e{eid}"
 408:                 eid += 1
 409: 
 410:                 edge = DKGEdge(
 411:                     edge_id=edge_id,
 412:                     src_node=src,
 413:                     dst_node=dst,
 414:                     relation_type=RelationType.DATA,
 415:                     flow_type=EdgeFlowType.COMBINATIONAL,
 416:                     signal_name=w.name or f"wire_{w.wire_id}",
 417:                     canonical_name=f"{src}->{dst}",
 418:                 )
 419: 
 420:                 file, line = parse_src(w.src)
 421:                 prov = Provenance(
 422:                     origin_file=file,
 423:                     origin_line=line,
 424:                     tool_stage="rtl",
 425:                     confidence="exact",
 426:                 )
 427:                 add_provenance(edge, prov, make_primary=True)
 428: 
 429:                 edges[edge_id] = edge
 430: 
 431:     edges = merge_bit_edges_to_bus(edges)
 432: 
 433:     new_edges: Dict[str, DKGEdge] = {}
 434:     for e in edges.values():
 435:         new_id = make_edge_id(e)
 436:         e.edge_id = new_id
 437:         new_edges[new_id] = e
 438: 
 439:     edges = new_edges
 440:     reindex_node_edges(nodes, edges)
 441: 
 442:     clock_nets, reset_nets = detect_clock_reset_signals(nodes, edges, cells, wires)
 443:     assign_clock_domains(nodes, edges, clock_nets)
 444:     assign_edge_flow_types(nodes, edges, clock_nets, reset_nets)
 445: 
 446:     return nodes, edges



FILE: dkg\builders\graph_metadata.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass, field
   4: from typing import Any, Dict, Optional
   5: 
   6: from ..pipeline.stages import FieldSource, ParsingStage
   7: 
   8: 
   9: @dataclass
  10: class FieldMetadata:
  11:     """ê° í•„ë“œ ê°’ì˜ ë©”íƒ€ë°ì´í„°"""
  12:     
  13:     value: Any
  14:     source: FieldSource
  15:     stage: ParsingStage
  16:     origin_file: Optional[str] = None
  17:     origin_line: Optional[int] = None
  18:     timestamp: Optional[float] = None  # ì—…ë°ì´íŠ¸ ì‹œê°
  19: 
  20: 
  21: @dataclass
  22: class NodeMetadata:
  23:     """ë…¸ë“œì˜ ëª¨ë“  í•„ë“œì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°"""
  24:     
  25:     fields: Dict[str, FieldMetadata] = field(default_factory=dict)
  26:     
  27:     def get(self, field_name: str, default: Any = None) -> Any:
  28:         """í•„ë“œ ê°’ ë°˜í™˜"""
  29:         if field_name in self.fields:
  30:             return self.fields[field_name].value
  31:         return default
  32:     
  33:     def get_source(self, field_name: str) -> Optional[FieldSource]:
  34:         """í•„ë“œì˜ ì¶œì²˜ ë°˜í™˜"""
  35:         if field_name in self.fields:
  36:             return self.fields[field_name].source
  37:         return None
  38:     
  39:     def set(
  40:         self,
  41:         field_name: str,
  42:         value: Any,
  43:         source: FieldSource,
  44:         stage: ParsingStage,
  45:         origin_file: Optional[str] = None,
  46:         origin_line: Optional[int] = None,
  47:     ) -> None:
  48:         """í•„ë“œ ê°’ ì„¤ì •"""
  49:         self.fields[field_name] = FieldMetadata(
  50:             value=value,
  51:             source=source,
  52:             stage=stage,
  53:             origin_file=origin_file,
  54:             origin_line=origin_line,
  55:         )
  56:     
  57:     def should_update(self, field_name: str, new_source: FieldSource) -> bool:
  58:         """í•„ë“œë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨"""
  59:         from .stages import should_update_field
  60:         
  61:         current_source = self.get_source(field_name)
  62:         return should_update_field(current_source, new_source)
  63: 
  64: 
  65: @dataclass
  66: class EdgeMetadata:
  67:     """ì—£ì§€ì˜ ëª¨ë“  í•„ë“œì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°"""
  68:     
  69:     fields: Dict[str, FieldMetadata] = field(default_factory=dict)
  70:     
  71:     def get(self, field_name: str, default: Any = None) -> Any:
  72:         if field_name in self.fields:
  73:             return self.fields[field_name].value
  74:         return default
  75:     
  76:     def get_source(self, field_name: str) -> Optional[FieldSource]:
  77:         if field_name in self.fields:
  78:             return self.fields[field_name].source
  79:         return None
  80:     
  81:     def set(
  82:         self,
  83:         field_name: str,
  84:         value: Any,
  85:         source: FieldSource,
  86:         stage: ParsingStage,
  87:         origin_file: Optional[str] = None,
  88:         origin_line: Optional[int] = None,
  89:     ) -> None:
  90:         self.fields[field_name] = FieldMetadata(
  91:             value=value,
  92:             source=source,
  93:             stage=stage,
  94:             origin_file=origin_file,
  95:             origin_line=origin_line,
  96:         )
  97:     
  98:     def should_update(self, field_name: str, new_source: FieldSource) -> bool:
  99:         from .stages import should_update_field
 100:         
 101:         current_source = self.get_source(field_name)
 102:         return should_update_field(current_source, new_source)



FILE: dkg\builders\graph_updater.py

   1: from __future__ import annotations
   2: 
   3: from typing import Any, Dict, Optional
   4: 
   5: from ..core.graph import DKGEdge, DKGNode
   6: from .graph_metadata import EdgeMetadata, NodeMetadata
   7: from ..pipeline.stages import FieldSource, ParsingStage
   8: 
   9: 
  10: class GraphUpdater:
  11:     """
  12:     ê·¸ë˜í”„ë¥¼ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ì—”ì§„.
  13:     ê° íŒŒì‹± stageê°€ ê¸°ì¡´ ê·¸ë˜í”„ì— ìƒˆ ì •ë³´ë¥¼ mergeí•  ë•Œ ì‚¬ìš©.
  14:     """
  15:     
  16:     def __init__(
  17:         self,
  18:         nodes: Dict[str, DKGNode],
  19:         edges: Dict[str, DKGEdge],
  20:     ):
  21:         self.nodes = nodes
  22:         self.edges = edges
  23:         
  24:         # ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ (node_id/edge_id -> metadata)
  25:         self.node_metadata: Dict[str, NodeMetadata] = {
  26:             nid: NodeMetadata() for nid in nodes
  27:         }
  28:         self.edge_metadata: Dict[str, EdgeMetadata] = {
  29:             eid: EdgeMetadata() for eid in edges
  30:         }
  31:     
  32:     def update_node_field(
  33:         self,
  34:         node_id: str,
  35:         field_name: str,
  36:         value: Any,
  37:         source: FieldSource,
  38:         stage: ParsingStage,
  39:         origin_file: Optional[str] = None,
  40:         origin_line: Optional[int] = None,
  41:     ) -> bool:
  42:         """
  43:         ë…¸ë“œ í•„ë“œë¥¼ ì—…ë°ì´íŠ¸.
  44:         
  45:         Returns:
  46:             True if updated, False if skipped (lower priority)
  47:         """
  48:         if node_id not in self.nodes:
  49:             return False
  50:         
  51:         meta = self.node_metadata[node_id]
  52:         
  53:         if not meta.should_update(field_name, source):
  54:             return False
  55:         
  56:         # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
  57:         meta.set(field_name, value, source, stage, origin_file, origin_line)
  58:         
  59:         # ì‹¤ì œ ë…¸ë“œ ê°ì²´ ì—…ë°ì´íŠ¸
  60:         if hasattr(self.nodes[node_id], field_name):
  61:             setattr(self.nodes[node_id], field_name, value)
  62:         
  63:         return True
  64:     
  65:     def update_edge_field(
  66:         self,
  67:         edge_id: str,
  68:         field_name: str,
  69:         value: Any,
  70:         source: FieldSource,
  71:         stage: ParsingStage,
  72:         origin_file: Optional[str] = None,
  73:         origin_line: Optional[int] = None,
  74:     ) -> bool:
  75:         """ì—£ì§€ í•„ë“œë¥¼ ì—…ë°ì´íŠ¸"""
  76:         if edge_id not in self.edges:
  77:             return False
  78:         
  79:         meta = self.edge_metadata[edge_id]
  80:         
  81:         if not meta.should_update(field_name, source):
  82:             return False
  83:         
  84:         meta.set(field_name, value, source, stage, origin_file, origin_line)
  85:         
  86:         if hasattr(self.edges[edge_id], field_name):
  87:             setattr(self.edges[edge_id], field_name, value)
  88:         
  89:         return True
  90:     
  91:     def batch_update_clock_domains(
  92:         self,
  93:         clock_assignments: Dict[str, str],  # node_id -> clock_domain
  94:         source: FieldSource,
  95:         stage: ParsingStage,
  96:         origin_file: Optional[str] = None,
  97:     ) -> int:
  98:         """í´ëŸ­ ë„ë©”ì¸ ì¼ê´„ ì—…ë°ì´íŠ¸"""
  99:         count = 0
 100:         for node_id, clock_domain in clock_assignments.items():
 101:             if self.update_node_field(
 102:                 node_id, "clock_domain", clock_domain, source, stage, origin_file
 103:             ):
 104:                 count += 1
 105:         return count
 106:     
 107:     def batch_update_timing_exceptions(
 108:         self,
 109:         exceptions: Dict[str, str],  # edge_id -> exception_type
 110:         source: FieldSource,
 111:         stage: ParsingStage,
 112:         origin_file: Optional[str] = None,
 113:     ) -> int:
 114:         """íƒ€ì´ë° ì˜ˆì™¸ ì¼ê´„ ì—…ë°ì´íŠ¸"""
 115:         count = 0
 116:         for edge_id, exception_type in exceptions.items():
 117:             if self.update_edge_field(
 118:                 edge_id, "timing_exception", exception_type, source, stage, origin_file
 119:             ):
 120:                 count += 1
 121:         return count
 122:     
 123:     def get_field_history(self, node_id: str, field_name: str) -> Optional[list]:
 124:         """í•„ë“œì˜ ë³€ê²½ ì´ë ¥ ë°˜í™˜ (í–¥í›„ í™•ì¥ìš©)"""
 125:         # TODO: ì´ë ¥ ì¶”ì ì´ í•„ìš”í•˜ë©´ metadataì— history ì¶”ê°€
 126:         pass
 127:     
 128:     def export_metadata_summary(self) -> dict:
 129:         """ë©”íƒ€ë°ì´í„° ìš”ì•½ ë°˜í™˜ (ë””ë²„ê¹…/ìºì‹± ìš©)"""
 130:         return {
 131:             "nodes": {
 132:                 nid: {
 133:                     field: {
 134:                         "source": meta.source.value,
 135:                         "stage": meta.stage.value,
 136:                     }
 137:                     for field, meta in nm.fields.items()
 138:                 }
 139:                 for nid, nm in self.node_metadata.items()
 140:             },
 141:             "edges": {
 142:                 eid: {
 143:                     field: {
 144:                         "source": meta.source.value,
 145:                         "stage": meta.stage.value,
 146:                     }
 147:                     for field, meta in em.fields.items()
 148:                 }
 149:                 for eid, em in self.edge_metadata.items()
 150:             },
 151:         }



FILE: dkg\builders\supergraph.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass, field
   4: from enum import Enum
   5: from typing import Any, Dict, List, Optional, Set, Tuple
   6: 
   7: from ..core.graph import DKGEdge, DKGNode, EdgeFlowType, EntityClass, RelationType
   8: from ..core.provenance import Provenance
   9: from ..utils import stable_hash
  10: 
  11: 
  12: # ============================================================================
  13: # Analysis Attachment Model
  14: # ============================================================================
  15: # AnalysisëŠ” ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ì§€ ì•ŠëŠ” immutable snapshotì´ë©°
  16: # SuperNode/SuperEdgeì— keyed bundleë¡œ ë¶€ì°©ë©ë‹ˆë‹¤.
  17: # ============================================================================
  18: class AnalysisKind(str, Enum):
  19:     TIMING = "timing"
  20:     AREA = "area"
  21:     POWER = "power"
  22: 
  23: @dataclass(frozen=True)
  24: class TimingNodeMetrics:
  25:     """
  26:     SuperNodeì— ë¶€ì°©ë˜ëŠ” Timing ë¶„ì„ ê²°ê³¼.
  27:     
  28:     ì›ì¹™:
  29:     - ì§‘ê³„ ê°€ëŠ¥í•œ í†µê³„ ì •ë³´ë§Œ í¬í•¨
  30:     - critical path ì—¬ë¶€, slack region ë“±ì˜ ë‹¨ì–¸ ê¸ˆì§€
  31:     - path membership ì •ë³´ í¬í•¨ ê¸ˆì§€
  32:     """
  33:     # í•„ìˆ˜ Metrics
  34:     min_slack: float              # ì ˆëŒ€ ìµœì•…ê°’
  35:     p5_slack: float               # tail risk ì§€í‘œ (5th percentile)
  36:     max_arrival_time: float       # ê°€ì¥ ëŠ¦ì€ ë„ì°© ì‹œê°„
  37:     min_required_time: float      # ê°€ì¥ íƒ€ì´íŠ¸í•œ ìš”êµ¬ ì‹œê°„
  38:     critical_node_ratio: float    # slack < threshold ë¹„ìœ¨
  39:     near_critical_ratio: float    # slack < Î±Â·clock ë¹„ìœ¨
  40:     
  41:     # ì„ íƒì  Metric
  42:     timing_risk_score: Optional[float] = None  # UI/Alertìš© ë‹¨ì¼ ìŠ¤ì¹¼ë¼
  43: 
  44: 
  45: @dataclass(frozen=True)
  46: class TimingEdgeMetrics:
  47:     """
  48:     SuperEdgeì— ë¶€ì°©ë˜ëŠ” Timing ë¶„ì„ ê²°ê³¼.
  49:     
  50:     ì›ì¹™:
  51:     - ì§€ì—° íŠ¹ì„±ì˜ í†µê³„ë§Œ ì œê³µ
  52:     - "ì´ edgeê°€ slackì„ ê²°ì •í•œë‹¤" ë“±ì˜ ë‹¨ì–¸ ê¸ˆì§€
  53:     - critical edge ì—¬ë¶€ í‘œí˜„ ê¸ˆì§€
  54:     """
  55:     # í•„ìˆ˜ Metrics
  56:     max_delay: float
  57:     p95_delay: float              # 95th percentile delay
  58:     flow_type_histogram: Dict[EdgeFlowType, int]  # comb / seq ë¹„ìœ¨
  59:     
  60:     # ì„ íƒì  Metrics
  61:     fanout_max: Optional[int] = None
  62:     fanout_p95: Optional[float] = None
  63: 
  64: 
  65: # ============================================================================
  66: # ê·¸ë˜í”„ ì™¸ë¶€ Timing ì •ë³´ (í•„ìˆ˜ ë¶„ë¦¬ ëŒ€ìƒ)
  67: # ============================================================================
  68: # Timing ë¶„ì„ ê²°ê³¼ ì¤‘ alert, finding, summary, path digestëŠ”
  69: # ê·¸ë˜í”„ ì™¸ë¶€ ê°ì²´ë¡œ ê´€ë¦¬í•˜ë©° ê·¸ë˜í”„ì™€ ì¬ê²°í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  70: # ============================================================================
  71: 
  72: 
  73: class TimingAlertSeverity(str, Enum):
  74:     INFO = "info"
  75:     WARN = "warn"
  76:     ERROR = "error"
  77: 
  78: 
  79: @dataclass
  80: class TimingAlert:
  81:     """
  82:     Timing ë¶„ì„ì—ì„œ ë°œê²¬ëœ Alert/Finding.
  83:     
  84:     ê·¸ë˜í”„ ì™¸ë¶€ ê°ì²´ë¡œ ê´€ë¦¬ë˜ë©°, entity_refë¥¼ í†µí•´ ì°¸ì¡°ë§Œ ìˆ˜í–‰.
  85:     """
  86:     entity_ref: str               # node_id / supernode_id / edge_id
  87:     entity_type: str              # "node" / "supernode" / "edge"
  88:     severity: TimingAlertSeverity
  89:     reason: str
  90:     metrics_snapshot: Dict[str, Any]  # ë°œê²¬ ì‹œì ì˜ metric ì‚¬ë³¸
  91: 
  92: 
  93: @dataclass
  94: class TimingSummary:
  95:     """
  96:     ì „ì²´ Timing ë¶„ì„ì˜ ìš”ì•½ ì •ë³´.
  97:     
  98:     ê·¸ë˜í”„ì™€ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬ë˜ëŠ” ë¶„ì„ ê²°ê³¼ ìš”ì•½.
  99:     """
 100:     worst_slack: float
 101:     violation_count: int
 102:     near_critical_count: int
 103:     clock_period: float
 104:     analysis_mode: str            # "setup" / "hold" / "both"
 105:     timestamp: Optional[str] = None
 106: 
 107: 
 108: @dataclass
 109: class CriticalPathDigest:
 110:     """
 111:     Critical Pathì˜ ì°¸ì¡°ìš© Digest (ì„ íƒì ).
 112:     
 113:     Path digestëŠ” ì°¸ì¡°ìš©ì´ë©° ê·¸ë˜í”„ì™€ ì¬ê²°í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
 114:     UI/Query ê³„ì¸µì—ì„œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
 115:     """
 116:     path_id: str
 117:     startpoint: str
 118:     endpoint: str
 119:     total_delay: float
 120:     slack: float
 121:     node_sequence: Optional[List[str]] = None  # ì°¸ì¡°ìš©
 122: 
 123: 
 124: class GraphViewType(str, Enum):
 125:     Structural = "Structural"
 126:     Connectivity = "Connectivity"
 127:     Physical = "Physical"
 128: 
 129: 
 130: class GraphContext(str, Enum):
 131:     """
 132:     ê·¸ë˜í”„ ìƒì„± ë° ë·° ì„ íƒì˜ ì‚¬ìš© ë§¥ë½ ì •ì˜.
 133:     
 134:     - DESIGN:      í•©ì„±, êµ¬í˜„, íƒ€ì´ë° ë¶„ì„ (ë¬¼ë¦¬ì  ì‹¤ì²´ ì¤‘ì‹¬)
 135:     - SIMULATION:  ë™ì‘ ê²€ì¦, í…ŒìŠ¤íŠ¸ë²¤ì¹˜ (ê²€ì¦ í™˜ê²½ ì¤‘ì‹¬)
 136:     """
 137:     DESIGN = "design"
 138:     SIMULATION = "simulation"
 139: 
 140: 
 141: class SuperClass(str, Enum):
 142:     ATOMIC = "Atomic"
 143:     MODULE_CLUSTER = "ModuleCluster"
 144:     SEQ_CHAIN = "SequentialChain"
 145:     COMB_CLOUD = "CombinationalCloud"
 146:     IO_CLUSTER = "IOCluster"
 147:     CONSTRAINT_GROUP = "ConstraintGroup"
 148:     ELIMINATED = "EliminatedNode" # superedgeë¡œ í™˜ì›ë˜ì§€ ì•ŠëŠ” ë…¸ë“œì˜ ë³´ì¡´ì  í‘œí˜„
 149: 
 150: 
 151: class NodeAction(Enum):
 152:     PROMOTE = "promote"
 153:     MERGE = "merge"
 154:     ELIMINATE = "eliminate"
 155: 
 156: 
 157: @dataclass
 158: class SuperNode:
 159:     node_id: str
 160:     super_class: SuperClass
 161:     member_nodes: Set[str]
 162:     member_edges: Set[str]
 163:     aggregated_attrs: Dict[str, Any] = field(default_factory=dict)
 164:     provenances: List[Provenance] = field(default_factory=list)
 165:     canonical_name: Optional[str] = None
 166:     display_name: Optional[str] = None
 167:     # Analysis Attachment: keyed bundle for extensibility
 168:     analysis: Dict[AnalysisKind, Any] = field(default_factory=dict, repr=False)
 169:     # analysis dict itself is mutable,
 170:     # but each analysis bundle MUST be immutable snapshot
 171: 
 172: # superedgeëŠ” DKGEdgeì™€ ë‹¬ë¦¬ ê³ ìœ í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ì§€ ì•Šê³  ê·¸ë˜í”„ì  ì—°ê²°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ìš©ë„. ì˜ë¯¸ëŠ” ë©¤ë²„ ì—£ì§€ë“¤ì´ ë‹´ë‹¹
 173: @dataclass
 174: class SuperEdge:
 175:     edge_id: str
 176:     src_node: str
 177:     dst_node: str
 178:     member_edges: Set[str]
 179:     member_nodes: Set[str]
 180:     relation_types: Set[RelationType]
 181:     flow_types: Set[EdgeFlowType]
 182:     provenances: List[Provenance] = field(default_factory=list)
 183:     canonical_name: Optional[str] = None
 184:     display_name: Optional[str] = None
 185:     # Analysis Attachment: keyed bundle for extensibility
 186:     analysis: Dict[AnalysisKind, Any] = field(default_factory=dict, repr=False)
 187:     # analysis dict itself is mutable,
 188:     # but each analysis bundle MUST be immutable snapshot
 189: 
 190: 
 191: 
 192: @dataclass
 193: class SuperGraph:
 194:     super_nodes: Dict[str, SuperNode]
 195:     super_edges: Dict[Tuple[str, str], SuperEdge]
 196:     node_to_super: Dict[str, str]
 197: 
 198: 
 199: def make_supernode_canonical_name(sn: SuperNode, nodes: Dict[str, DKGNode]) -> str:
 200:     any_node = nodes[next(iter(sn.member_nodes))]
 201:     base = any_node.hier_path
 202:     return f"{base} : {sn.super_class.value}"
 203: 
 204: 
 205: def make_supernode_display_name(sn: SuperNode) -> str:
 206:     if sn.super_class == SuperClass.COMB_CLOUD:
 207:         return "Combinational Logic"
 208:     if sn.super_class == SuperClass.SEQ_CHAIN:
 209:         return "Sequential Chain"
 210:     if sn.super_class == SuperClass.ATOMIC:
 211:         return "Block"
 212:     if sn.super_class == SuperClass.ELIMINATED:
 213:         return "Collapsed"
 214:     return sn.super_class.value
 215: 
 216: 
 217: def make_superedge_canonical_name(se: SuperEdge, super_nodes: Dict[str, SuperNode]) -> str:
 218:     src = super_nodes[se.src_node].canonical_name
 219:     dst = super_nodes[se.dst_node].canonical_name
 220:     return f"{src} -> {dst}"
 221: 
 222: 
 223: def make_superedge_display_name(se: SuperEdge) -> str:
 224:     if len(se.relation_types) == 1:
 225:         return next(iter(se.relation_types)).value.replace("Relation", "")
 226:     return "Multiple Signals"
 227: 
 228: 
 229: def supernode_signature(
 230:     view: GraphViewType,
 231:     super_class: SuperClass,
 232:     member_node_ids: set[str],
 233:     policy_version: str = "v1",
 234: ) -> str:
 235:     nodes_part = ",".join(sorted(member_node_ids))
 236:     return "|".join([view.value, super_class.value, policy_version, nodes_part])
 237: 
 238: 
 239: def make_supernode_id(
 240:     view: GraphViewType,
 241:     super_class: SuperClass,
 242:     member_node_ids: set[str],
 243:     policy_version: str = "v1",
 244: ) -> str:
 245:     sig = supernode_signature(view, super_class, member_node_ids, policy_version)
 246:     h = stable_hash(sig)
 247:     return f"SN_{view.value}_{super_class.value}_{h}"
 248: 
 249: 
 250: def superedge_signature(
 251:     src_sn: str,
 252:     dst_sn: str,
 253:     member_edge_ids: set[str],
 254:     policy_version: str = "v1",
 255: ) -> str:
 256:     edges_part = ",".join(sorted(member_edge_ids))
 257:     return "|".join([src_sn, dst_sn, policy_version, edges_part])
 258: 
 259: 
 260: def make_superedge_id(
 261:     src_sn: str,
 262:     dst_sn: str,
 263:     member_edge_ids: set[str],
 264:     policy_version: str = "v1",
 265: ) -> str:
 266:     sig = superedge_signature(src_sn, dst_sn, member_edge_ids, policy_version)
 267:     h = stable_hash(sig)
 268:     return f"SE_{h}"
 269: 
 270: @dataclass(frozen=True)
 271: class NodePolicy:
 272:     action: NodeAction
 273:     super_class: Optional[SuperClass]  # PROMOTEë©´ None ê°€ëŠ¥
 274: 
 275: 
 276: # ============================================================================
 277: # POLICY_MAP_DESIGN: í•©ì„±/êµ¬í˜„/íƒ€ì´ë° ë¶„ì„ìš© ì •ì±…
 278: # ============================================================================
 279: # ëª©í‘œ: ì‹¤ì œ ì¹©(FPGA/ASIC)ì— ë°°ì¹˜ë˜ëŠ” ë¬¼ë¦¬ì  ì‹¤ì²´ë§Œ ë³´ì¡´
 280: # í•µì‹¬ ì² í•™: "ì‹¤ì œ í•˜ë“œì›¨ì–´ëŠ” ë¬´ì—‡ì¸ê°€?"
 281: # ============================================================================
 282: POLICY_MAP_DESIGN: dict[GraphViewType, dict[EntityClass, NodePolicy]] = {
 283: 
 284:     GraphViewType.Structural: {
 285:         EntityClass.MODULE_INSTANCE: NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 286:         EntityClass.IO_PORT:        NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 287: 
 288:         EntityClass.RTL_BLOCK:      NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 289:         EntityClass.FSM:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 290:         EntityClass.FLIP_FLOP:      NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 291:         EntityClass.LUT:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 292:         EntityClass.MUX:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 293:         EntityClass.DSP:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 294:         EntityClass.BRAM:           NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 295: 
 296:         EntityClass.PACKAGE_PIN:    NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 297:         EntityClass.PBLOCK:         NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 298:         EntityClass.BOARD_CONNECTOR:NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 299:     },
 300: 
 301:     GraphViewType.Connectivity: {
 302:         EntityClass.FLIP_FLOP:      NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 303:         EntityClass.DSP:            NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 304:         EntityClass.BRAM:           NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 305:         EntityClass.IO_PORT:        NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 306: 
 307:         EntityClass.RTL_BLOCK:      NodePolicy(NodeAction.MERGE, SuperClass.COMB_CLOUD),
 308:         EntityClass.FSM:            NodePolicy(NodeAction.MERGE, SuperClass.COMB_CLOUD),
 309:         EntityClass.LUT:            NodePolicy(NodeAction.MERGE, SuperClass.COMB_CLOUD),
 310:         EntityClass.MUX:            NodePolicy(NodeAction.MERGE, SuperClass.COMB_CLOUD),
 311: 
 312:         EntityClass.MODULE_INSTANCE:NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 313:         EntityClass.PACKAGE_PIN:    NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 314:         EntityClass.PBLOCK:         NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 315:         EntityClass.BOARD_CONNECTOR:NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 316:     },
 317: 
 318:     GraphViewType.Physical: {
 319:         EntityClass.IO_PORT:        NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 320:         EntityClass.PACKAGE_PIN:    NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 321:         EntityClass.PBLOCK:         NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 322:         EntityClass.BOARD_CONNECTOR:NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 323: 
 324:         EntityClass.DSP:            NodePolicy(NodeAction.MERGE, SuperClass.CONSTRAINT_GROUP),
 325:         EntityClass.BRAM:           NodePolicy(NodeAction.MERGE, SuperClass.CONSTRAINT_GROUP),
 326: 
 327:         # ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ ì œê±°
 328:     },
 329: }
 330: 
 331: 
 332: # ============================================================================
 333: # POLICY_MAP_SIMULATION: ë™ì‘ ê²€ì¦/í…ŒìŠ¤íŠ¸ë²¤ì¹˜ìš© ì •ì±…
 334: # ============================================================================
 335: # ëª©í‘œ: ê²€ì¦ í™˜ê²½ê³¼ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ(DUT)ì˜ ê´€ê³„ë¥¼ ëª…í™•íˆ í‘œí˜„
 336: # í•µì‹¬ ì² í•™: "í…ŒìŠ¤íŠ¸ì—ì„œ ë¬´ì—‡ì„ ê²€ì¦í•˜ëŠ”ê°€?"
 337: # ============================================================================
 338: POLICY_MAP_SIMULATION: dict[GraphViewType, dict[EntityClass, NodePolicy]] = {
 339: 
 340:     GraphViewType.Structural: {
 341:         # ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” ëª¨ë“ˆ ê³„ì¸µì´ ì¤‘ìš” (ì¸í„°í˜ì´ìŠ¤ ì¤‘ì‹¬)
 342:         EntityClass.MODULE_INSTANCE: NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 343:         EntityClass.IO_PORT:        NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 344:         
 345:         # ì´ˆê¸°ê°’ ì„¤ì • ë¸”ë¡ì´ë‚˜ í´ëŸ­ ìƒì„±ê¸°ëŠ” ê²€ì¦ì— í•„ìˆ˜
 346:         # (íŒŒì„œê°€ ì´ë¥¼ RTL_BLOCKìœ¼ë¡œ ë¶„ë¥˜í•  ê²½ìš° ë³´ì¡´)
 347:         EntityClass.RTL_BLOCK:      NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 348:         
 349:         # ë‚´ë¶€ ë¡œì§ì€ ê³¼ê°í•˜ê²Œ í•©ì¹¨ (ë¸”ë™ë°•ìŠ¤ ë·°)
 350:         EntityClass.FSM:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 351:         EntityClass.FLIP_FLOP:      NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 352:         EntityClass.LUT:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 353:         EntityClass.MUX:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 354:         EntityClass.DSP:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 355:         EntityClass.BRAM:           NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 356: 
 357:         # ë¬¼ë¦¬ì  ì œì•½ì€ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ë¬´ì˜ë¯¸ -> ì œê±°
 358:         EntityClass.PACKAGE_PIN:    NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 359:         EntityClass.PBLOCK:         NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 360:         EntityClass.BOARD_CONNECTOR:NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 361:     },
 362: 
 363:     GraphViewType.Connectivity: {
 364:         # ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ëŠ” ì‹œë®¬ë ˆì´ì…˜ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬
 365:         EntityClass.MODULE_INSTANCE: NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 366:         EntityClass.IO_PORT:        NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 367:         
 368:         # ë ˆì§€ìŠ¤í„°ëŠ” ìƒíƒœ ë³€í™” ì¶”ì ì— ìœ ìš©í•  ìˆ˜ ìˆìŒ
 369:         EntityClass.FLIP_FLOP:      NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 370:         EntityClass.DSP:            NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 371:         EntityClass.BRAM:           NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 372:         
 373:         # ë‚´ë¶€ ë¡œì§ì€ DUT ë¸”ë™ë°•ìŠ¤ë¡œ í‘œí˜„
 374:         EntityClass.RTL_BLOCK:      NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 375:         EntityClass.FSM:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 376:         EntityClass.LUT:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 377:         EntityClass.MUX:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 378: 
 379:         # ë¬¼ë¦¬ì  ì •ë³´ëŠ” ì œê±°
 380:         EntityClass.PACKAGE_PIN:    NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 381:         EntityClass.PBLOCK:         NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 382:         EntityClass.BOARD_CONNECTOR:NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 383:     },
 384: 
 385:     GraphViewType.Physical: {
 386:         # Simulationì—ì„œëŠ” Physical Viewê°€ ì˜ë¯¸ ì—†ìŒ (ëª¨ë‘ ì œê±°)
 387:         # í•„ìš”ì‹œ í–¥í›„ í™•ì¥ ê°€ëŠ¥
 388:     },
 389: }
 390: 
 391: 
 392: def select_policy_map(context: GraphContext) -> dict[GraphViewType, dict[EntityClass, NodePolicy]]:
 393:     """
 394:     ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì ì ˆí•œ ì •ì±… ë§µ ì„ íƒ.
 395:     
 396:     Args:
 397:         context: GraphContext ê°’ (DESIGN ë˜ëŠ” SIMULATION)
 398:         
 399:     Returns:
 400:         ì„ íƒëœ ì •ì±… ë§µ
 401:     """
 402:     if context == GraphContext.SIMULATION:
 403:         return POLICY_MAP_SIMULATION
 404:     else:
 405:         # ê¸°ë³¸ê°’: DESIGN
 406:         return POLICY_MAP_DESIGN
 407: 
 408: 
 409: def get_node_policy(
 410:     node: DKGNode,
 411:     view: GraphViewType,
 412:     context: GraphContext = GraphContext.DESIGN,
 413: ) -> NodePolicy:
 414:     """
 415:     ë…¸ë“œì— ëŒ€í•œ ì •ì±… ê²°ì •.
 416:     
 417:     Args:
 418:         node: ëŒ€ìƒ ë…¸ë“œ
 419:         view: ê·¸ë˜í”„ ë·° íƒ€ì…
 420:         context: ì‚¬ìš© ë§¥ë½ (ê¸°ë³¸ê°’: DESIGN)
 421:         
 422:     Returns:
 423:         ì ìš©í•  ì •ì±… (NodePolicy)
 424:         
 425:     ì›ì¹™:
 426:     1. ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì •ì±… ë§µ ì„ íƒ
 427:     2. ë·°ì— í•´ë‹¹í•˜ëŠ” ì •ì±… ì¡°íšŒ
 428:     3. ì—”í‹°í‹° í´ë˜ìŠ¤ì— ë”°ë¥¸ ì •ì±… ë°˜í™˜
 429:     4. ì†ì„± ê¸°ë°˜ ë™ì  ì˜¤ë²„ë¼ì´ë”© (ì˜ˆ: testbench ì´ë¦„ íŒ¨í„´)
 430:     """
 431:     # 1. ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì •ì±… ë§µ ì„ íƒ
 432:     policy_map = select_policy_map(context)
 433:     
 434:     # 2. ë·°ì— í•´ë‹¹í•˜ëŠ” ì •ì±… ì¡°íšŒ
 435:     view_policies = policy_map.get(view, {})
 436:     
 437:     # 3. ì—”í‹°í‹° í´ë˜ìŠ¤ì— ë”°ë¥¸ ê¸°ë³¸ ì •ì±…
 438:     base_policy = view_policies.get(
 439:         node.entity_class,
 440:         NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED)
 441:     )
 442:     
 443:     # 4. ì†ì„± ê¸°ë°˜ ë™ì  ì˜¤ë²„ë¼ì´ë”© (Task 12ì˜ í•µì‹¬)
 444:     # Design ëª¨ë“œì—ì„œ: testbench ê´€ë ¨ ìš”ì†ŒëŠ” ê°•ì œ ì œê±°
 445:     if context == GraphContext.DESIGN:
 446:         is_testbench = (
 447:             node.local_name.lower().startswith("tb_") or
 448:             "testbench" in node.hier_path.lower() or
 449:             "sim" in node.hier_path.lower()
 450:         )
 451:         if is_testbench:
 452:             return NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED)
 453:     
 454:     # Simulation ëª¨ë“œì—ì„œ: ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í¨ëŸ¬ëŸ¬ìŠ¤ ìƒì„±ê¸°ëŠ” ë³´ì¡´
 455:     if context == GraphContext.SIMULATION:
 456:         is_important_for_sim = (
 457:             node.local_name.lower().startswith("clk_gen") or
 458:             node.local_name.lower().startswith("reset_gen") or
 459:             "initial" in node.attributes.get("verilog_construct", "").lower()
 460:         )
 461:         if is_important_for_sim and base_policy.action == NodeAction.MERGE:
 462:             # í´ëŸ­/ë¦¬ì…‹ ìƒì„±ê¸°ëŠ” Atomicìœ¼ë¡œ ìƒí–¥
 463:             return NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC)
 464:     
 465:     return base_policy
 466: 
 467: 
 468: class ViewBuilder:
 469:     def __init__(
 470:         self,
 471:         nodes: Dict[str, DKGNode],
 472:         edges: Dict[str, DKGEdge],
 473:         view: GraphViewType,
 474:         context: GraphContext = GraphContext.DESIGN,
 475:     ):
 476:         self.nodes = nodes
 477:         self.edges = edges
 478:         self.view = view
 479:         self.context = context
 480: 
 481:         self.node_to_super: Dict[str, str] = {}
 482:         self.super_nodes: Dict[str, SuperNode] = {}
 483:         self.super_edges: Dict[Tuple[str, str], SuperEdge] = {}
 484: 
 485:     def _neighbors_1hop(self, nid: str) -> Set[str]:
 486:         n = self.nodes[nid]
 487:         nbrs = set()
 488:         for eid in n.in_edges + n.out_edges:
 489:             e = self.edges[eid]
 490:             nbrs.add(e.src_node)
 491:             nbrs.add(e.dst_node)
 492:         return nbrs
 493: 
 494:     def cycle1_promote(self) -> None:
 495:         for n in self.nodes.values():
 496:             node_policy = get_node_policy(n, self.view, self.context)
 497:             if node_policy.action != NodeAction.PROMOTE:
 498:                 continue
 499:             if node_policy.super_class is None:
 500:                 continue
 501: 
 502:             sn = SuperNode(
 503:                 node_id=f"SN_{n.node_id}",
 504:                 super_class=node_policy.super_class,
 505:                 member_nodes={n.node_id},
 506:                 member_edges=set(),
 507:                 provenances=list(n.provenances),
 508:             )
 509:             sn.canonical_name = make_supernode_canonical_name(sn, self.nodes)
 510:             sn.display_name = make_supernode_display_name(sn)
 511:             self.super_nodes[sn.node_id] = sn
 512:             self.node_to_super[n.node_id] = sn.node_id
 513: 
 514:     def cycle2_merge(self) -> None:
 515:         # ê° ë…¸ë“œê°€ ë¨¸ì§€ë  super_class ê²°ì •
 516:         node_merge_class: Dict[str, SuperClass] = {}
 517:         for nid, n in self.nodes.items():
 518:             node_policy = get_node_policy(n, self.view, self.context)
 519:             if node_policy.action == NodeAction.MERGE and node_policy.super_class is not None:
 520:                 node_merge_class[nid] = node_policy.super_class
 521:         
 522:         merge_candidates = set(node_merge_class.keys())
 523:         visited: Set[str] = set()
 524: 
 525:         for nid in merge_candidates:
 526:             if nid in visited:
 527:                 continue
 528: 
 529:             target_class = node_merge_class[nid]
 530:             stack = [nid]
 531:             component: Set[str] = set()
 532: 
 533:             while stack:
 534:                 cur = stack.pop()
 535:                 if cur in visited or cur not in merge_candidates:
 536:                     continue
 537:                 # ê°™ì€ super_classë¥¼ ê°€ì§„ ë…¸ë“œë§Œ ì²˜ë¦¬
 538:                 if node_merge_class[cur] != target_class:
 539:                     continue
 540: 
 541:                 visited.add(cur)
 542:                 component.add(cur)
 543: 
 544:                 for nb in self._neighbors_1hop(cur):
 545:                     if nb in merge_candidates and node_merge_class.get(nb) == target_class:
 546:                         stack.append(nb)
 547: 
 548:             if not component:
 549:                 continue
 550: 
 551:             sn_id = make_supernode_id(
 552:                 view=self.view,
 553:                 super_class=target_class,
 554:                 member_node_ids=component,
 555:                 policy_version="v2",
 556:             )
 557: 
 558:             sn = SuperNode(
 559:                 node_id=sn_id,
 560:                 super_class=target_class,
 561:                 member_nodes=component,
 562:                 member_edges=set(),
 563:             )
 564:             sn.canonical_name = make_supernode_canonical_name(sn, self.nodes)
 565:             sn.display_name = make_supernode_display_name(sn)
 566: 
 567:             self.super_nodes[sn.node_id] = sn
 568:             for n in component:
 569:                 self.node_to_super[n] = sn.node_id
 570: 
 571:     def cycle2_5_eliminate(self) -> None:
 572:         for nid, n in self.nodes.items():
 573:             if nid in self.node_to_super:
 574:                 continue
 575: 
 576:             node_policy = get_node_policy(n, self.view, self.context)
 577:             if node_policy.action != NodeAction.ELIMINATE:
 578:                 raise RuntimeError(f"Unassigned node in view {self.view}: {nid}")
 579: 
 580:             eliminate_class = node_policy.super_class if node_policy.super_class is not None else SuperClass.ELIMINATED
 581: 
 582:             sn = SuperNode(
 583:                 node_id=make_supernode_id(
 584:                     view=self.view,
 585:                     super_class=eliminate_class,
 586:                     member_node_ids={nid},
 587:                     policy_version="v1",
 588:                 ),
 589:                 super_class=eliminate_class,
 590:                 member_nodes={nid},
 591:                 member_edges=set(),
 592:             )
 593:             sn.canonical_name = make_supernode_canonical_name(sn, self.nodes)
 594:             sn.display_name = make_supernode_display_name(sn)
 595: 
 596:             self.super_nodes[sn.node_id] = sn
 597:             self.node_to_super[nid] = sn.node_id
 598: 
 599:     def cycle3_rewrite_edges(self) -> None:
 600:         for e in self.edges.values():
 601:             src_sn = self.node_to_super[e.src_node]
 602:             dst_sn = self.node_to_super[e.dst_node]
 603: 
 604:             if src_sn == dst_sn:
 605:                 self.super_nodes[src_sn].member_edges.add(e.edge_id)
 606:                 continue
 607: 
 608:             key = (src_sn, dst_sn)
 609:             if key not in self.super_edges:
 610:                 self.super_edges[key] = SuperEdge(
 611:                     edge_id=make_superedge_id(src_sn, dst_sn, set()),
 612:                     src_node=src_sn,
 613:                     dst_node=dst_sn,
 614:                     member_edges=set(),
 615:                     member_nodes=set(),
 616:                     relation_types=set(),
 617:                     flow_types=set(),
 618:                     provenances=[],
 619:                 )
 620:                 self.super_edges[key].canonical_name = make_superedge_canonical_name(
 621:                     self.super_edges[key],
 622:                     self.super_nodes,
 623:                 )
 624:                 self.super_edges[key].display_name = make_superedge_display_name(
 625:                     self.super_edges[key],
 626:                 )
 627: 
 628:             se = self.super_edges[key]
 629:             se.member_edges.add(e.edge_id)
 630:             se.member_nodes.update({e.src_node, e.dst_node})
 631:             se.relation_types.add(e.relation_type)
 632:             se.flow_types.add(e.flow_type)
 633:             se.provenances.extend(e.provenances)
 634: 
 635:     def build(self) -> SuperGraph:
 636:         self.cycle1_promote()
 637:         self.cycle2_merge()
 638:         self.cycle2_5_eliminate()
 639:         self.cycle3_rewrite_edges()
 640: 
 641:         return SuperGraph(
 642:             super_nodes=self.super_nodes,
 643:             super_edges=self.super_edges,
 644:             node_to_super=self.node_to_super,
 645:         )
 646: 
 647: 
 648: # ============================================================================
 649: # Analysis Attachment Helper Functions
 650: # ============================================================================
 651: # AnalysisëŠ” êµ¬ì¡° ë¡œì§ì—ì„œ ì§ì ‘ ì°¸ì¡°í•˜ì§€ ì•Šìœ¼ë©°, ê²°ê³¼ì˜ ê·€ì† ëŒ€ìƒìœ¼ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
 652: # ============================================================================
 653: 
 654: def attach_timing_analysis_to_supernode(
 655:     sn: SuperNode,
 656:     metrics: TimingNodeMetrics
 657: ) -> None:
 658:     """
 659:     SuperNodeì— Timing Analysis ê²°ê³¼ë¥¼ ë¶€ì°©í•©ë‹ˆë‹¤.
 660:     
 661:     ì›ì¹™:
 662:     - êµ¬ì¡° ë³€ê²½ ì—†ìŒ
 663:     - ê¸°ì¡´ analysis["timing"]ì€ ì „ì²´ êµì²´ë¨ (immutable snapshot)
 664:     - êµ¬ì¡° ë¡œì§ì€ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
 665:     """
 666:     sn.analysis[AnalysisKind.TIMING] = metrics
 667: 
 668: 
 669: def attach_timing_analysis_to_superedge(
 670:     se: SuperEdge,
 671:     metrics: TimingEdgeMetrics
 672: ) -> None:
 673:     """
 674:     SuperEdgeì— Timing Analysis ê²°ê³¼ë¥¼ ë¶€ì°©í•©ë‹ˆë‹¤.
 675:     
 676:     ì›ì¹™:
 677:     - êµ¬ì¡° ë³€ê²½ ì—†ìŒ
 678:     - ê¸°ì¡´ analysis["timing"]ì€ ì „ì²´ êµì²´ë¨ (immutable snapshot)
 679:     - êµ¬ì¡° ë¡œì§ì€ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
 680:     """
 681:     se.analysis[AnalysisKind.TIMING] = metrics
 682: 
 683: 
 684: def get_timing_analysis_from_supernode(
 685:     sn: SuperNode
 686: ) -> Optional[TimingNodeMetrics]:
 687:     """
 688:     SuperNodeì—ì„œ Timing Analysis ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
 689:     
 690:     Returns:
 691:         TimingNodeMetrics or None if not attached
 692:     """
 693:     return sn.analysis.get(AnalysisKind.TIMING)
 694: 
 695: 
 696: def get_timing_analysis_from_superedge(
 697:     se: SuperEdge
 698: ) -> Optional[TimingEdgeMetrics]:
 699:     """
 700:     SuperEdgeì—ì„œ Timing Analysis ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
 701:     
 702:     Returns:
 703:         TimingEdgeMetrics or None if not attached
 704:     """
 705:     return se.analysis.get(AnalysisKind.TIMING)
 706: 
 707: 
 708: # ============================================================================
 709: # Task 12: Design vs Simulation ì •ì±… ì„¤ëª…ì„œ
 710: # ============================================================================
 711: #
 712: # ì´ í•¨ìˆ˜ë“¤ê³¼ ì •ì±…ë§µë“¤ì€ Task 12ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
 713: #
 714: # 1. í•µì‹¬ ì•„ì´ë””ì–´
 715: # ---
 716: # Policyì˜ ì—­í• : "ìˆ˜ì²œ ê°œì˜ RTL ìš”ì†Œë¥¼ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ë¬¶ì–´ì„œ ë³´ì—¬ì¤„ ê²ƒì¸ê°€?"
 717: #
 718: # Design Mode (êµ¬í˜„/í•©ì„± ë·°):
 719: #   - ëª©í‘œ: ì‹¤ì œ ì¹©ì— ë°°ì¹˜ë˜ëŠ” ë¬¼ë¦¬ì  ì‹¤ì²´ë§Œ ë‚¨ê¸´ë‹¤
 720: #   - ê´€ì‹¬ì‚¬: Timing Path, Pblock(ë¬¼ë¦¬ì  ìœ„ì¹˜), Resource ì‚¬ìš©ëŸ‰
 721: #   - Critical Pathë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ í•˜ë“œì›¨ì–´ ë³‘ëª© êµ¬ê°„ ì‹ë³„
 722: #
 723: # Simulation Mode (ê²€ì¦/ë™ì‘ ë·°):
 724: #   - ëª©í‘œ: ê²€ì¦ í™˜ê²½(Environment)ê³¼ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ(DUT)ì˜ ê´€ê³„ë¥¼ ë³´ì—¬ì¤€ë‹¤
 725: #   - ê´€ì‹¬ì‚¬: Stimulus(ì…ë ¥ ìƒì„±), DUT(í…ŒìŠ¤íŠ¸ ëŒ€ìƒ), Checker/Monitor
 726: #   - Transactionì˜ íë¦„ê³¼ ëª¨ë“ˆ ê°„ ì¸í„°í˜ì´ìŠ¤ í™•ì¸
 727: #
 728: # 2. ì—”í‹°í‹°ë³„ ì²˜ë¦¬ ì „ëµ
 729: # ---
 730: #
 731: # Design Mode:
 732: #   Module Instance: PROMOTE (ì£¼ìš” ê³„ì¸µ êµ¬ì¡° ìœ ì§€)
 733: #   Logic (LUT/MUX): MERGE -> CombinationalCloud (ë³µì¡ë„ ê°ì†Œ)
 734: #   Register (FF): MERGE -> SequentialChain (ë˜ëŠ” PROMOTE if Timing View)
 735: #   Testbench/Virtual Logic: ELIMINATE (í•©ì„±ë˜ì§€ ì•ŠëŠ” ì½”ë“œ ì œê±°)
 736: #
 737: # Simulation Mode:
 738: #   Module Instance: PROMOTE (ì¸í„°í˜ì´ìŠ¤ ëª…í™•í™”)
 739: #   Testbench Top: PROMOTE -> ATOMIC (ìµœìƒìœ„ ë…¸ë“œ)
 740: #   DUT (Device Under Test): PROMOTE (ë¸”ë™ë°•ìŠ¤ í‘œí˜„)
 741: #   Initial Block / Always (Clock Gen): PROMOTE (ê²€ì¦ í¬ì¸íŠ¸)
 742: #   Assert/Cover Property: PROMOTE (ê²€ì¦ í¬ì¸íŠ¸)
 743: #   Physical Constraints (Pblock/LOC): ELIMINATE (ìœ„ì¹˜ ì •ë³´ ë¬´ì˜ë¯¸)
 744: #
 745: # 3. ì‚¬ìš© ì˜ˆì‹œ
 746: # ---
 747: #
 748: # # Design Modeë¡œ Connectivity ë·° ìƒì„±
 749: # builder_design = ViewBuilder(
 750: #     nodes=dkg_nodes,
 751: #     edges=dkg_edges,
 752: #     view=GraphViewType.Connectivity,
 753: #     context=GraphContext.DESIGN
 754: # )
 755: # supergraph_design = builder_design.build()
 756: #
 757: # # Simulation Modeë¡œ Structural ë·° ìƒì„±
 758: # builder_sim = ViewBuilder(
 759: #     nodes=dkg_nodes,
 760: #     edges=dkg_edges,
 761: #     view=GraphViewType.Structural,
 762: #     context=GraphContext.SIMULATION
 763: # )
 764: # supergraph_sim = builder_sim.build()
 765: #
 766: # 4. í™•ì¥ ê°€ëŠ¥ì„±
 767: # ---
 768: #
 769: # get_node_policy í•¨ìˆ˜ì—ëŠ” ì†ì„± ê¸°ë°˜ ë™ì  ì˜¤ë²„ë¼ì´ë”© ë¡œì§ì´ ìˆìŠµë‹ˆë‹¤.
 770: # ì´ë¥¼ ì´ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê³ ê¸‰ ì •ì±…ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
 771: #
 772: # - Testbench ì´ë¦„ íŒ¨í„´ ê¸°ë°˜ ìë™ í•„í„°ë§
 773: # - Clock Generator, Reset Generatorì˜ ìë™ ì‹ë³„
 774: # - initial ë¸”ë¡ íƒì§€ ë° ë³´ì¡´
 775: # - Verilog $display ë¬¸ í¬í•¨ ë…¸ë“œ ì œê±°
 776: #
 777: # ì´ë“¤ ë¡œì§ì€ ë…¸ë“œì˜ attributes, parameters, hier_path, local_name ë“±ì„
 778: # ë¶„ì„í•˜ì—¬ êµ¬í˜„ë©ë‹ˆë‹¤.
 779: #
 780: # ============================================================================
 781: 
 782: 
 783: # ============================================================================
 784: # í–¥í›„ í™•ì¥ ì˜ˆì‹œ (Area / Power Analysis)
 785: # ============================================================================
 786: # ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥:
 787: #
 788: # @dataclass(frozen=True)
 789: # class AreaMetrics:
 790: #     area_density: float
 791: #     area_utilization: float
 792: #     area_total: float
 793: #
 794: # @dataclass(frozen=True)
 795: # class PowerMetrics:
 796: #     power_peak: float
 797: #     power_average: float
 798: #     power_leakage: float
 799: #
 800: # def attach_area_analysis_to_supernode(sn: SuperNode, metrics: AreaMetrics) -> None:
 801: #     sn.analysis[AnalysisKind.AREA] = metrics
 802: #
 803: # def attach_power_analysis_to_supernode(sn: SuperNode, metrics: PowerMetrics) -> None:
 804: #     sn.analysis[AnalysisKind.POWER] = metrics
 805: #
 806: # ì‚¬ìš© ì˜ˆì‹œ:
 807: #     supernode.analysis[AnalysisKind.TIMING]  -> TimingNodeMetrics
 808: #     supernode.analysis[AnalysisKind.AREA]    -> AreaMetrics
 809: #     supernode.analysis[AnalysisKind.POWER]   -> PowerMetrics
 810: # ============================================================================



FILE: dkg\cache\graph_version.py

   1: from dataclasses import dataclass
   2: from typing import Optional
   3: @dataclass
   4: class GraphVersion:
   5:     rtl_hash: str
   6:     constraint_hash: Optional[str]
   7:     timing_hash: Optional[str]
   8:     policy_versions: dict
   9: # ìºì‹±ì„ ìœ„í•œ ê·¸ë˜í”„ ë²„ì „ ì •ë³´
  10: # TODO: í–¥í›„ ê·¸ë˜í”„ êµ¬ì„± ìš”ì†Œë³„ í•´ì‹œ ì¶”ê°€ ê³ ë ¤


FILE: dkg\cache\snapshot.py

   1: """
   2: ê·¸ë˜í”„ ìŠ¤ëƒ…ìƒ· ì €ì¥/ë¡œë”© (ì½ê¸° ì „ìš© ìºì‹±ìš©)
   3: 
   4: ë©”íƒ€ë°ì´í„°ëŠ” ì œì™¸í•˜ê³  í•„ìˆ˜ ë°ì´í„°ë§Œ JSONìœ¼ë¡œ ì €ì¥:
   5: - DKG ê·¸ë˜í”„ (nodes + edges)
   6: - SuperGraph (supernodes + superedges + node_to_super)
   7: - GraphVersion (ë©”íƒ€ë°ì´í„°)
   8: """
   9: from __future__ import annotations
  10: 
  11: import json
  12: from dataclasses import asdict, dataclass
  13: from pathlib import Path
  14: from typing import Any, Dict, Optional, Set, Tuple
  15: 
  16: from ..core.graph import DKGEdge, DKGNode, EdgeFlowType, EntityClass, RelationType
  17: from ..core.provenance import Provenance
  18: from ..builders.supergraph import SuperClass, SuperEdge, SuperGraph, SuperNode
  19: from .graph_version import GraphVersion
  20: 
  21: 
  22: @dataclass
  23: class GraphSnapshot:
  24:     """ì „ì²´ ê·¸ë˜í”„ ìŠ¤ëƒ…ìƒ·"""
  25:     version: GraphVersion
  26:     dkg_nodes: Dict[str, DKGNode]
  27:     dkg_edges: Dict[str, DKGEdge]
  28:     supergraph: Optional[SuperGraph] = None
  29: 
  30: 
  31: def _serialize_node(node: DKGNode) -> dict:
  32:     """DKGNodeë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜ (ì–•ê²Œ)"""
  33:     return {
  34:         "node_id": node.node_id,
  35:         "entity_class": node.entity_class.value,
  36:         "hier_path": node.hier_path,
  37:         "local_name": node.local_name,
  38:         "canonical_name": node.canonical_name,
  39:         "short_alias": node.short_alias,
  40:         "parameters": node.parameters,
  41:         "attributes": node.attributes,
  42:         "clock_domain": node.clock_domain,
  43:         "arrival_time": node.arrival_time,
  44:         "required_time": node.required_time,
  45:         "slack": node.slack,
  46:         "in_edges": node.in_edges,
  47:         "out_edges": node.out_edges,
  48:         # provenanceëŠ” ì–•ê²Œ: ê¸°ë³¸ ì •ë³´ë§Œ
  49:         "provenances": [
  50:             {
  51:                 "origin_file": p.origin_file,
  52:                 "origin_line": p.origin_line,
  53:                 "tool_stage": p.tool_stage,
  54:                 "confidence": p.confidence,
  55:             }
  56:             for p in node.provenances
  57:         ] if node.provenances else [],
  58:         "primary_provenance": {
  59:             "origin_file": node.primary_provenance.origin_file,
  60:             "origin_line": node.primary_provenance.origin_line,
  61:             "tool_stage": node.primary_provenance.tool_stage,
  62:             "confidence": node.primary_provenance.confidence,
  63:         } if node.primary_provenance else None,
  64:     }
  65: 
  66: 
  67: def _deserialize_node(data: dict) -> DKGNode:
  68:     """dictì—ì„œ DKGNode ë³µì›"""
  69:     from ..stages import ParsingStage
  70:     
  71:     # provenance ë³µì›
  72:     provenances = []
  73:     if data.get("provenances"):
  74:         for p in data["provenances"]:
  75:             provenances.append(Provenance(
  76:                 origin_file=p.get("origin_file"),
  77:                 origin_line=p.get("origin_line"),
  78:                 tool_stage=p.get("tool_stage", "rtl"),
  79:                 confidence=p.get("confidence", "exact"),
  80:             ))
  81:     
  82:     primary_provenance = None
  83:     if data.get("primary_provenance"):
  84:         p = data["primary_provenance"]
  85:         primary_provenance = Provenance(
  86:             origin_file=p.get("origin_file"),
  87:             origin_line=p.get("origin_line"),
  88:             tool_stage=p.get("tool_stage", "rtl"),
  89:             confidence=p.get("confidence", "exact"),
  90:         )
  91:     
  92:     return DKGNode(
  93:         node_id=data["node_id"],
  94:         entity_class=EntityClass(data["entity_class"]),
  95:         hier_path=data["hier_path"],
  96:         local_name=data["local_name"],
  97:         canonical_name=data.get("canonical_name"),
  98:         short_alias=data.get("short_alias"),
  99:         parameters=data.get("parameters", {}),
 100:         attributes=data.get("attributes", {}),
 101:         clock_domain=data.get("clock_domain"),
 102:         arrival_time=data.get("arrival_time"),
 103:         required_time=data.get("required_time"),
 104:         slack=data.get("slack"),
 105:         in_edges=data.get("in_edges", []),
 106:         out_edges=data.get("out_edges", []),
 107:         provenances=provenances,
 108:         primary_provenance=primary_provenance,
 109:     )
 110: 
 111: 
 112: def _serialize_edge(edge: DKGEdge) -> dict:
 113:     """DKGEdgeë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜"""
 114:     return {
 115:         "edge_id": edge.edge_id,
 116:         "src_node": edge.src_node,
 117:         "dst_node": edge.dst_node,
 118:         "relation_type": edge.relation_type.value,
 119:         "flow_type": edge.flow_type.value,
 120:         "signal_name": edge.signal_name,
 121:         "canonical_name": edge.canonical_name,
 122:         "bit_range": list(edge.bit_range) if edge.bit_range else None,
 123:         "net_id": edge.net_id,
 124:         "driver_type": edge.driver_type,
 125:         "fanout_count": edge.fanout_count,
 126:         "clock_signal": edge.clock_signal,
 127:         "reset_signal": edge.reset_signal,
 128:         "clock_domain_id": edge.clock_domain_id,
 129:         "timing_exception": edge.timing_exception,
 130:         "parameters": edge.parameters,
 131:         "delay": edge.delay,
 132:         "arrival_time": edge.arrival_time,
 133:         "required_time": edge.required_time,
 134:     }
 135: 
 136: 
 137: def _deserialize_edge(data: dict) -> DKGEdge:
 138:     """dictì—ì„œ DKGEdge ë³µì›"""
 139:     return DKGEdge(
 140:         edge_id=data["edge_id"],
 141:         src_node=data["src_node"],
 142:         dst_node=data["dst_node"],
 143:         relation_type=RelationType(data["relation_type"]),
 144:         flow_type=EdgeFlowType(data["flow_type"]),
 145:         signal_name=data["signal_name"],
 146:         canonical_name=data["canonical_name"],
 147:         bit_range=tuple(data["bit_range"]) if data.get("bit_range") else None,
 148:         net_id=data.get("net_id"),
 149:         driver_type=data.get("driver_type"),
 150:         fanout_count=data.get("fanout_count"),
 151:         clock_signal=data.get("clock_signal"),
 152:         reset_signal=data.get("reset_signal"),
 153:         clock_domain_id=data.get("clock_domain_id"),
 154:         timing_exception=data.get("timing_exception"),
 155:         parameters=data.get("parameters", {}),
 156:         delay=data.get("delay"),
 157:         arrival_time=data.get("arrival_time"),
 158:         required_time=data.get("required_time"),
 159:     )
 160: 
 161: 
 162: def _serialize_supernode(sn: SuperNode) -> dict:
 163:     """SuperNodeë¥¼ JSON ì§ë ¬í™”"""
 164:     return {
 165:         "node_id": sn.node_id,
 166:         "super_class": sn.super_class.value,
 167:         "member_nodes": list(sn.member_nodes),
 168:         "member_edges": list(sn.member_edges),
 169:         "aggregated_attrs": sn.aggregated_attrs,
 170:         "canonical_name": sn.canonical_name,
 171:         "display_name": sn.display_name,
 172:         "provenances": [
 173:             {"origin_file": p.origin_file, "origin_line": p.origin_line, "tool_stage": p.tool_stage}
 174:             for p in sn.provenances
 175:         ] if sn.provenances else [],
 176:     }
 177: 
 178: 
 179: def _deserialize_supernode(data: dict) -> SuperNode:
 180:     """dictì—ì„œ SuperNode ë³µì›"""
 181:     provenances = []
 182:     if data.get("provenances"):
 183:         for p in data["provenances"]:
 184:             provenances.append(Provenance(
 185:                 origin_file=p.get("origin_file"),
 186:                 origin_line=p.get("origin_line"),
 187:                 tool_stage=p.get("tool_stage", "rtl"),
 188:             ))
 189:     
 190:     return SuperNode(
 191:         node_id=data["node_id"],
 192:         super_class=SuperClass(data["super_class"]),
 193:         member_nodes=set(data["member_nodes"]),
 194:         member_edges=set(data["member_edges"]),
 195:         aggregated_attrs=data.get("aggregated_attrs", {}),
 196:         canonical_name=data.get("canonical_name"),
 197:         display_name=data.get("display_name"),
 198:         provenances=provenances,
 199:     )
 200: 
 201: 
 202: def _serialize_superedge(se: SuperEdge) -> dict:
 203:     """SuperEdgeë¥¼ JSON ì§ë ¬í™”"""
 204:     return {
 205:         "edge_id": se.edge_id,
 206:         "src_node": se.src_node,
 207:         "dst_node": se.dst_node,
 208:         "member_edges": list(se.member_edges),
 209:         "member_nodes": list(se.member_nodes),
 210:         "relation_types": [rt.value for rt in se.relation_types],
 211:         "flow_types": [ft.value for ft in se.flow_types],
 212:         "canonical_name": se.canonical_name,
 213:         "display_name": se.display_name,
 214:         "provenances": [
 215:             {"origin_file": p.origin_file, "origin_line": p.origin_line, "tool_stage": p.tool_stage}
 216:             for p in se.provenances
 217:         ] if se.provenances else [],
 218:     }
 219: 
 220: 
 221: def _deserialize_superedge(data: dict) -> SuperEdge:
 222:     """dictì—ì„œ SuperEdge ë³µì›"""
 223:     provenances = []
 224:     if data.get("provenances"):
 225:         for p in data["provenances"]:
 226:             provenances.append(Provenance(
 227:                 origin_file=p.get("origin_file"),
 228:                 origin_line=p.get("origin_line"),
 229:                 tool_stage=p.get("tool_stage", "rtl"),
 230:             ))
 231:     
 232:     return SuperEdge(
 233:         edge_id=data["edge_id"],
 234:         src_node=data["src_node"],
 235:         dst_node=data["dst_node"],
 236:         member_edges=set(data["member_edges"]),
 237:         member_nodes=set(data["member_nodes"]),
 238:         relation_types={RelationType(rt) for rt in data["relation_types"]},
 239:         flow_types={EdgeFlowType(ft) for ft in data["flow_types"]},
 240:         canonical_name=data.get("canonical_name"),
 241:         display_name=data.get("display_name"),
 242:         provenances=provenances,
 243:     )
 244: 
 245: 
 246: def save_snapshot(
 247:     snapshot: GraphSnapshot,
 248:     filepath: Path | str,
 249:     indent: Optional[int] = None,
 250: ) -> None:
 251:     """ìŠ¤ëƒ…ìƒ·ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
 252:     filepath = Path(filepath)
 253:     filepath.parent.mkdir(parents=True, exist_ok=True)
 254:     
 255:     # ì§ë ¬í™”
 256:     data = {
 257:         "version": {
 258:             "rtl_hash": snapshot.version.rtl_hash,
 259:             "constraint_hash": snapshot.version.constraint_hash,
 260:             "timing_hash": snapshot.version.timing_hash,
 261:             "policy_versions": snapshot.version.policy_versions,
 262:         },
 263:         "dkg": {
 264:             "nodes": {
 265:                 node_id: _serialize_node(node)
 266:                 for node_id, node in snapshot.dkg_nodes.items()
 267:             },
 268:             "edges": {
 269:                 edge_id: _serialize_edge(edge)
 270:                 for edge_id, edge in snapshot.dkg_edges.items()
 271:             },
 272:         },
 273:     }
 274:     
 275:     # SuperGraphê°€ ìˆìœ¼ë©´ ì¶”ê°€
 276:     if snapshot.supergraph:
 277:         sg = snapshot.supergraph
 278:         data["supergraph"] = {
 279:             "super_nodes": {
 280:                 node_id: _serialize_supernode(sn)
 281:                 for node_id, sn in sg.super_nodes.items()
 282:             },
 283:             "super_edges": {
 284:                 f"{src}â†’{dst}": _serialize_superedge(se)
 285:                 for (src, dst), se in sg.super_edges.items()
 286:             },
 287:             "node_to_super": sg.node_to_super,
 288:         }
 289:     
 290:     # JSON ì €ì¥
 291:     with open(filepath, "w", encoding="utf-8") as f:
 292:         json.dump(data, f, indent=indent, ensure_ascii=False)
 293: 
 294: 
 295: def load_snapshot(filepath: Path | str) -> GraphSnapshot:
 296:     """JSON íŒŒì¼ì—ì„œ ìŠ¤ëƒ…ìƒ· ë¡œë”©"""
 297:     filepath = Path(filepath)
 298:     
 299:     with open(filepath, "r", encoding="utf-8") as f:
 300:         data = json.load(f)
 301:     
 302:     # GraphVersion ë³µì›
 303:     version_data = data["version"]
 304:     version = GraphVersion(
 305:         rtl_hash=version_data["rtl_hash"],
 306:         constraint_hash=version_data.get("constraint_hash"),
 307:         timing_hash=version_data.get("timing_hash"),
 308:         policy_versions=version_data.get("policy_versions", {}),
 309:     )
 310:     
 311:     # DKG ê·¸ë˜í”„ ë³µì›
 312:     dkg_data = data["dkg"]
 313:     dkg_nodes = {
 314:         node_id: _deserialize_node(node_data)
 315:         for node_id, node_data in dkg_data["nodes"].items()
 316:     }
 317:     dkg_edges = {
 318:         edge_id: _deserialize_edge(edge_data)
 319:         for edge_id, edge_data in dkg_data["edges"].items()
 320:     }
 321:     
 322:     # SuperGraph ë³µì› (ìˆìœ¼ë©´)
 323:     supergraph = None
 324:     if "supergraph" in data:
 325:         sg_data = data["supergraph"]
 326:         super_nodes = {
 327:             node_id: _deserialize_supernode(sn_data)
 328:             for node_id, sn_data in sg_data["super_nodes"].items()
 329:         }
 330:         super_edges = {
 331:             tuple(key.split("â†’")): _deserialize_superedge(se_data)
 332:             for key, se_data in sg_data["super_edges"].items()
 333:         }
 334:         node_to_super = sg_data["node_to_super"]
 335:         
 336:         supergraph = SuperGraph(
 337:             super_nodes=super_nodes,
 338:             super_edges=super_edges,
 339:             node_to_super=node_to_super,
 340:         )
 341:     
 342:     return GraphSnapshot(
 343:         version=version,
 344:         dkg_nodes=dkg_nodes,
 345:         dkg_edges=dkg_edges,
 346:         supergraph=supergraph,
 347:     )



FILE: dkg\core\graph.py

   1: # NOTE:
   2: # canonical_name is a human-readable debug label.
   3: # It is intended for debugging, tracing, and debug-like queries only.
   4: # It is NOT guaranteed to be stable across views, abstractions, or builds,
   5: # and MUST NOT be used as a persistent identifier, cache key, or external reference.
   6: from __future__ import annotations
   7: 
   8: from dataclasses import dataclass, field
   9: from enum import Enum
  10: from typing import Any, Dict, List, Optional, Tuple
  11: 
  12: from .provenance import Provenance
  13: 
  14: 
  15: class EntityClass(str, Enum):
  16:     MODULE_INSTANCE = "ModuleInstance"
  17:     RTL_BLOCK = "RTLBlock"
  18:     FSM = "FSM"
  19: 
  20:     FLIP_FLOP = "FlipFlop"
  21:     LUT = "LUT"
  22:     MUX = "MUX"
  23:     DSP = "DSP"
  24:     BRAM = "BRAM"
  25: 
  26:     IO_PORT = "IOPort"
  27:     PACKAGE_PIN = "PackagePin"
  28:     PBLOCK = "Pblock"
  29:     BOARD_CONNECTOR = "BoardConnector"
  30: 
  31: 
  32: class RelationType(str, Enum):
  33:     DATA = "DataRelation"
  34:     CLOCK = "ClockRelation"
  35:     RESET = "ResetRelation"
  36:     PARAMETER = "ParameterRelation"
  37:     CONSTRAINT = "ConstraintRelation"
  38:     PHYSICAL_MAP = "PhysicalMappingRelation"
  39: 
  40: 
  41: class EdgeFlowType(str, Enum):
  42:     COMBINATIONAL = "combinational"
  43:     SEQ_LAUNCH = "sequential_launch"
  44:     SEQ_CAPTURE = "sequential_capture"
  45:     CLOCK_TREE = "clock_tree"
  46:     ASYNC_RESET = "async_reset"
  47: 
  48: 
  49: @dataclass
  50: class DKGNode:
  51:     node_id: str
  52:     entity_class: EntityClass
  53:     hier_path: str
  54:     local_name: str
  55:     canonical_name: Optional[str] = None
  56: 
  57:     short_alias: Optional[str] = None
  58: 
  59:     parameters: Dict[str, str] = field(default_factory=dict)
  60:     attributes: Dict[str, str] = field(default_factory=dict)
  61: 
  62:     clock_domain: Optional[str] = None
  63:     arrival_time: Optional[float] = None
  64:     required_time: Optional[float] = None
  65:     slack: Optional[float] = None
  66: 
  67:     in_edges: List[str] = field(default_factory=list)
  68:     out_edges: List[str] = field(default_factory=list)
  69: 
  70:     provenances: List[Provenance] = field(default_factory=list)
  71:     primary_provenance: Optional[Provenance] = None
  72: 
  73: 
  74: @dataclass
  75: class DKGEdge:
  76:     edge_id: str
  77:     src_node: str
  78:     dst_node: str
  79: 
  80:     relation_type: RelationType
  81:     flow_type: EdgeFlowType
  82: 
  83:     signal_name: str
  84:     canonical_name: str
  85:     bit_range: Optional[Tuple[int, int]] = None
  86:     net_id: Optional[str] = None
  87: 
  88:     driver_type: Optional[str] = None
  89:     fanout_count: Optional[int] = None
  90: 
  91:     clock_signal: Optional[str] = None
  92:     reset_signal: Optional[str] = None
  93:     clock_domain_id: Optional[str] = None
  94: 
  95:     timing_exception: Optional[str] = None
  96:     parameters: Dict[str, Any] = field(default_factory=dict)
  97: 
  98:     delay: Optional[float] = None
  99:     arrival_time: Optional[float] = None
 100:     required_time: Optional[float] = None
 101:     slack: Optional[float] = None
 102: 
 103:     attributes: Dict[str, Any] = field(default_factory=dict)
 104: 
 105:     provenances: List[Provenance] = field(default_factory=list)
 106:     primary_provenance: Optional[Provenance] = None
 107: 
 108: 
 109: def make_node_canonical_name(node: DKGNode) -> str:
 110:     base = node.hier_path
 111: 
 112:     cls = node.entity_class
 113:     if cls == EntityClass.FLIP_FLOP:
 114:         suffix = f"reg_{node.local_name}"
 115:     elif cls == EntityClass.MUX:
 116:         suffix = "mux"
 117:     elif cls == EntityClass.LUT:
 118:         suffix = "comb"
 119:     elif cls == EntityClass.BRAM:
 120:         suffix = "bram"
 121:     elif cls == EntityClass.DSP:
 122:         suffix = "dsp"
 123:     elif cls == EntityClass.IO_PORT:
 124:         suffix = f"port_{node.local_name}"
 125:     else:
 126:         suffix = node.local_name or cls.value.lower()
 127: 
 128:     return f"{base}.{suffix}"
 129: 
 130: 
 131: def make_node_display_name(node: DKGNode) -> str:
 132:     if node.entity_class == EntityClass.FLIP_FLOP:
 133:         return f"Reg {node.local_name}"
 134:     if node.entity_class == EntityClass.BRAM:
 135:         return "BRAM"
 136:     if node.entity_class == EntityClass.MUX:
 137:         return "MUX"
 138:     if node.entity_class == EntityClass.LUT:
 139:         return "Logic"
 140:     if node.entity_class == EntityClass.DSP:
 141:         return "DSP"
 142:     if node.entity_class == EntityClass.IO_PORT:
 143:         return f"Port {node.local_name}"
 144: 
 145:     return node.local_name or node.entity_class.value
 146: 
 147: 
 148: def make_edge_canonical_name(e: DKGEdge, nodes: dict[str, DKGNode]) -> str:
 149:     src = nodes[e.src_node].canonical_name
 150:     dst = nodes[e.dst_node].canonical_name
 151: 
 152:     if e.bit_range:
 153:         msb, lsb = e.bit_range
 154:         sig = f"{e.signal_name}[{msb}:{lsb}]"
 155:     else:
 156:         sig = e.signal_name
 157: 
 158:     return f"{src} -> {dst} : {sig}"
 159: 
 160: 
 161: def make_edge_display_name(e: DKGEdge) -> str:
 162:     if e.bit_range:
 163:         msb, lsb = e.bit_range
 164:         return f"{e.signal_name}[{msb}:{lsb}]"
 165:     return e.signal_name



FILE: dkg\core\ir.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass, field
   4: from typing import Optional
   5: 
   6: 
   7: @dataclass
   8: class Wire:
   9:     wire_id: int
  10:     name: str | None = None
  11:     drivers: list[str] = field(default_factory=list)
  12:     loads: list[str] = field(default_factory=list)
  13:     src: Optional[str] = None
  14: 
  15: 
  16: @dataclass
  17: class CellIR:
  18:     name: str
  19:     type: str
  20:     module: str
  21:     port_dirs: dict[str, str]
  22:     connections: dict[str, list[int]]
  23:     src: Optional[str] = None



FILE: dkg\core\provenance.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass
   4: from typing import Iterable, Optional, Tuple, List
   5: 
   6: 
   7: @dataclass
   8: class Provenance:
   9:     origin_file: Optional[str] = None
  10:     origin_line: Optional[int] = None
  11:     tool_stage: str = "rtl"  # rtl / synth / timing / constraint
  12:     confidence: str = "exact"  # exact / inferred
  13: 
  14: 
  15: def add_provenance(obj, prov: Provenance, make_primary: bool = False) -> None:
  16:     obj.provenances.append(prov)
  17:     if make_primary or obj.primary_provenance is None:
  18:         obj.primary_provenance = prov
  19: 
  20: 
  21: def merge_provenances_nodes(nodes: Iterable) -> Tuple[Provenance, List[Provenance]]:
  22:     new_provs: List[Provenance] = []
  23:     for n in nodes:
  24:         new_provs.extend(n.provenances)
  25: 
  26:     files = [p.origin_file for p in new_provs if p.origin_file]
  27:     lines = [p.origin_line for p in new_provs if p.origin_line]
  28: 
  29:     primary = Provenance(
  30:         origin_file=files[0] if files else None,
  31:         origin_line=min(lines) if lines else None,
  32:         tool_stage="rtl",
  33:         confidence="inferred",
  34:     )
  35: 
  36:     return primary, new_provs
  37: 
  38: 
  39: def merge_provenances_edges(edges: Iterable) -> Tuple[Provenance, List[Provenance]]:
  40:     new_provs: List[Provenance] = []
  41:     for e in edges:
  42:         new_provs.extend(e.provenances)
  43: 
  44:     files = [p.origin_file for p in new_provs if p.origin_file]
  45:     lines = [p.origin_line for p in new_provs if p.origin_line]
  46: 
  47:     primary = Provenance(
  48:         origin_file=files[0] if files else None,
  49:         origin_line=min(lines) if lines else None,
  50:         tool_stage="rtl",
  51:         confidence="inferred",
  52:     )
  53: 
  54:     return primary, new_provs



FILE: dkg\parsers\bd_parser.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Dict
   5: 
   6: from ..core.graph import DKGEdge, DKGNode
   7: from ..builders.graph_updater import GraphUpdater
   8: from ..pipeline.stages import FieldSource, ParsingStage
   9: from . import ConstraintParser
  10: 
  11: 
  12: class BdParser(ConstraintParser):
  13:     """
  14:     BD (block design) parser for IP instance grouping seeds.
  15:     """
  16: 
  17:     def get_stage(self) -> ParsingStage:
  18:         return ParsingStage.BOARD
  19: 
  20:     def parse_and_update(
  21:         self,
  22:         filepath: str,
  23:         updater: GraphUpdater,
  24:         nodes: Dict[str, DKGNode],
  25:         edges: Dict[str, DKGEdge],
  26:     ) -> None:
  27:         with open(filepath, "r", encoding="utf-8") as f:
  28:             lines = f.readlines()
  29: 
  30:         for line_num, line in enumerate(lines, start=1):
  31:             raw = line.strip()
  32:             if not raw or raw.startswith("#"):
  33:                 continue
  34: 
  35:             if raw.startswith("create_bd_cell"):
  36:                 self._parse_create_bd_cell(raw, line_num, filepath, updater, nodes)
  37: 
  38:     def _parse_create_bd_cell(
  39:         self,
  40:         line: str,
  41:         line_num: int,
  42:         filepath: str,
  43:         updater: GraphUpdater,
  44:         nodes: Dict[str, DKGNode],
  45:     ) -> None:
  46:         match = re.search(r"create_bd_cell\s+-type\s+ip\s+-vlnv\s+(\S+)\s+(\S+)", line)
  47:         if not match:
  48:             return
  49: 
  50:         vlnv = match.group(1)
  51:         inst = match.group(2)
  52: 
  53:         for node_id, node in nodes.items():
  54:             candidates = [node.local_name, node.hier_path, node.canonical_name]
  55:             if not any(inst == cand or (cand and inst in cand) for cand in candidates):
  56:                 continue
  57: 
  58:             new_attrs = dict(node.attributes)
  59:             new_attrs["bd_ip"] = vlnv
  60:             new_attrs["bd_group"] = vlnv
  61: 
  62:             updater.update_node_field(
  63:                 node_id,
  64:                 "attributes",
  65:                 new_attrs,
  66:                 FieldSource.DECLARED,
  67:                 ParsingStage.BOARD,
  68:                 filepath,
  69:                 line_num,
  70:             )



FILE: dkg\parsers\parser_utils.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Iterable, List
   5: 
   6: 
   7: def _split_target_list(raw: str) -> List[str]:
   8:     text = raw.strip()
   9:     if text.startswith("{") and text.endswith("}"):
  10:         text = text[1:-1].strip()
  11:     parts = re.split(r"\s+", text)
  12:     return [p.strip('"') for p in parts if p.strip('"')]
  13: 
  14: 
  15: def extract_bracket_targets(line: str, object_types: Iterable[str]) -> List[str]:
  16:     targets: List[str] = []
  17:     pattern = r"\[get_(%s)\s+([^\]]+)\]" % "|".join(object_types)
  18:     for match in re.finditer(pattern, line):
  19:         targets.extend(_split_target_list(match.group(2)))
  20:     return targets
  21: 
  22: 
  23: def extract_option_targets(line: str, option: str, object_types: Iterable[str]) -> List[str]:
  24:     targets: List[str] = []
  25:     pattern = r"%s\s+\[get_(%s)\s+([^\]]+)\]" % (re.escape(option), "|".join(object_types))
  26:     for match in re.finditer(pattern, line):
  27:         targets.extend(_split_target_list(match.group(2)))
  28:     return targets
  29: 
  30: 
  31: def pattern_match(pattern: str, candidate: str) -> bool:
  32:     if not pattern:
  33:         return False
  34:     if "*" not in pattern and "?" not in pattern:
  35:         if pattern == candidate:
  36:             return True
  37:         if pattern in candidate or candidate in pattern:
  38:             return True
  39:     escaped = re.escape(pattern)
  40:     escaped = escaped.replace(r"\*", ".*").replace(r"\?", ".")
  41:     regex = re.compile(r"^%s$" % escaped)
  42:     return bool(regex.match(candidate))
  43: 
  44: 
  45: def match_any(patterns: Iterable[str], candidates: Iterable[str]) -> bool:
  46:     for pattern in patterns:
  47:         for cand in candidates:
  48:             if cand and pattern_match(pattern, cand):
  49:                 return True
  50:     return False



FILE: dkg\parsers\sdc_parser.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Dict
   5: 
   6: from ..core.graph import DKGEdge, DKGNode
   7: from ..builders.graph_updater import GraphUpdater
   8: from ..pipeline.stages import FieldSource, ParsingStage
   9: from . import ConstraintParser
  10: from .parser_utils import extract_option_targets, match_any
  11: 
  12: 
  13: class SdcParser(ConstraintParser):
  14:     """
  15:     SDC (Synopsys Design Constraints) íŒŒì„œ.
  16:     
  17:     ì²˜ë¦¬í•˜ëŠ” ëª…ë ¹:
  18:     - create_clock: í´ëŸ­ ì •ì˜
  19:     - set_input_delay / set_output_delay: I/O íƒ€ì´ë°
  20:     - set_false_path: false path ì œì•½
  21:     - set_multicycle_path: multicycle ì œì•½
  22:     - set_max_delay / set_min_delay: ì§€ì—° ì œì•½
  23:     """
  24:     
  25:     def get_stage(self) -> ParsingStage:
  26:         return ParsingStage.CONSTRAINTS
  27:     
  28:     def parse_and_update(
  29:         self,
  30:         filepath: str,
  31:         updater: GraphUpdater,
  32:         nodes: Dict[str, DKGNode],
  33:         edges: Dict[str, DKGEdge],
  34:     ) -> None:
  35:         with open(filepath, "r", encoding="utf-8") as f:
  36:             lines = f.readlines()
  37:         
  38:         for line_num, line in enumerate(lines, start=1):
  39:             line = line.strip()
  40:             
  41:             # create_clock ì²˜ë¦¬
  42:             if line.startswith("create_clock"):
  43:                 self._parse_create_clock(
  44:                     line, line_num, filepath, updater, nodes, edges
  45:                 )
  46:             
  47:             # set_false_path ì²˜ë¦¬
  48:             elif line.startswith("set_false_path"):
  49:                 self._parse_false_path(
  50:                     line, line_num, filepath, updater, nodes, edges
  51:                 )
  52:             
  53:             # set_multicycle_path ì²˜ë¦¬
  54:             elif line.startswith("set_multicycle_path"):
  55:                 self._parse_multicycle_path(
  56:                     line, line_num, filepath, updater, nodes, edges
  57:                 )
  58:     
  59:     def _parse_create_clock(
  60:         self,
  61:         line: str,
  62:         line_num: int,
  63:         filepath: str,
  64:         updater: GraphUpdater,
  65:         nodes: Dict[str, DKGNode],
  66:         edges: Dict[str, DKGEdge],
  67:     ) -> None:
  68:         """
  69:         create_clock ëª…ë ¹ íŒŒì‹±.
  70:         ì˜ˆ: create_clock -name clk -period 10 [get_ports clk]
  71:         """
  72:         # ê°„ë‹¨í•œ ì •ê·œì‹ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•´ì•¼ í•¨)
  73:         match = re.search(r"-name\s+(\w+)", line)
  74:         if not match:
  75:             return
  76:         
  77:         clock_name = match.group(1)
  78:         
  79:         # get_portsë¡œ í¬íŠ¸ ì´ë¦„ ì¶”ì¶œ
  80:         port_match = re.search(r"get_ports\s+(\w+)", line)
  81:         if not port_match:
  82:             return
  83:         
  84:         port_name = port_match.group(1)
  85:         
  86:         # í•´ë‹¹ í¬íŠ¸ë¥¼ ê°€ì§„ ë…¸ë“œë“¤ ì°¾ê¸°
  87:         for node_id, node in nodes.items():
  88:             if node.local_name == port_name:
  89:                 updater.update_node_field(
  90:                     node_id,
  91:                     "clock_domain",
  92:                     clock_name,
  93:                     FieldSource.DECLARED,
  94:                     ParsingStage.CONSTRAINTS,
  95:                     filepath,
  96:                     line_num,
  97:                 )
  98:         
  99:         # í•´ë‹¹ ì‹ í˜¸ë¥¼ ê°€ì§„ ì—£ì§€ë“¤ë„ ì—…ë°ì´íŠ¸
 100:         for edge_id, edge in edges.items():
 101:             if edge.signal_name == port_name:
 102:                 updater.update_edge_field(
 103:                     edge_id,
 104:                     "clock_signal",
 105:                     clock_name,
 106:                     FieldSource.DECLARED,
 107:                     ParsingStage.CONSTRAINTS,
 108:                     filepath,
 109:                     line_num,
 110:                 )
 111:     
 112:     def _parse_false_path(
 113:         self,
 114:         line: str,
 115:         line_num: int,
 116:         filepath: str,
 117:         updater: GraphUpdater,
 118:         nodes: Dict[str, DKGNode],
 119:         edges: Dict[str, DKGEdge],
 120:     ) -> None:
 121:         """
 122:         set_false_path ëª…ë ¹ íŒŒì‹±.
 123:         ì˜ˆ: set_false_path -from [get_pins src/*] -to [get_pins dst/*]
 124:         """
 125:         from_patterns = extract_option_targets(line, "-from", ("ports", "pins", "cells"))
 126:         to_patterns = extract_option_targets(line, "-to", ("ports", "pins", "cells"))
 127: 
 128:         if not from_patterns and not to_patterns:
 129:             return
 130: 
 131:         for edge_id, edge in edges.items():
 132:             src_node = nodes.get(edge.src_node)
 133:             dst_node = nodes.get(edge.dst_node)
 134:             if not src_node or not dst_node:
 135:                 continue
 136: 
 137:             src_candidates = [src_node.local_name, src_node.hier_path, src_node.canonical_name]
 138:             dst_candidates = [dst_node.local_name, dst_node.hier_path, dst_node.canonical_name]
 139: 
 140:             src_match = True if not from_patterns else match_any(from_patterns, src_candidates)
 141:             dst_match = True if not to_patterns else match_any(to_patterns, dst_candidates)
 142: 
 143:             if src_match and dst_match:
 144:                 updater.update_edge_field(
 145:                     edge_id,
 146:                     "timing_exception",
 147:                     "false_path",
 148:                     FieldSource.DECLARED,
 149:                     ParsingStage.CONSTRAINTS,
 150:                     filepath,
 151:                     line_num,
 152:                 )
 153:     
 154:     def _parse_multicycle_path(
 155:         self,
 156:         line: str,
 157:         line_num: int,
 158:         filepath: str,
 159:         updater: GraphUpdater,
 160:         nodes: Dict[str, DKGNode],
 161:         edges: Dict[str, DKGEdge],
 162:     ) -> None:
 163:         """
 164:         set_multicycle_path ëª…ë ¹ íŒŒì‹±.
 165:         ì˜ˆ: set_multicycle_path 2 -from [get_pins ...] -to [get_pins ...]
 166:         """
 167:         value_match = re.search(r"set_multicycle_path\s+(-?\d+)", line)
 168:         if not value_match:
 169:             return
 170: 
 171:         multicycle = int(value_match.group(1))
 172:         mc_type = None
 173:         if "-hold" in line:
 174:             mc_type = "hold"
 175:         elif "-setup" in line:
 176:             mc_type = "setup"
 177: 
 178:         from_patterns = extract_option_targets(line, "-from", ("ports", "pins", "cells"))
 179:         to_patterns = extract_option_targets(line, "-to", ("ports", "pins", "cells"))
 180: 
 181:         if not from_patterns and not to_patterns:
 182:             return
 183: 
 184:         for edge_id, edge in edges.items():
 185:             src_node = nodes.get(edge.src_node)
 186:             dst_node = nodes.get(edge.dst_node)
 187:             if not src_node or not dst_node:
 188:                 continue
 189: 
 190:             src_candidates = [src_node.local_name, src_node.hier_path, src_node.canonical_name]
 191:             dst_candidates = [dst_node.local_name, dst_node.hier_path, dst_node.canonical_name]
 192: 
 193:             src_match = True if not from_patterns else match_any(from_patterns, src_candidates)
 194:             dst_match = True if not to_patterns else match_any(to_patterns, dst_candidates)
 195: 
 196:             if not (src_match and dst_match):
 197:                 continue
 198: 
 199:             new_params = dict(edge.parameters)
 200:             existing = new_params.get("multicycle")
 201:             if existing is None or multicycle > existing:
 202:                 new_params["multicycle"] = multicycle
 203:             if mc_type:
 204:                 new_params["multicycle_type"] = mc_type
 205: 
 206:             updater.update_edge_field(
 207:                 edge_id,
 208:                 "parameters",
 209:                 new_params,
 210:                 FieldSource.DECLARED,
 211:                 ParsingStage.CONSTRAINTS,
 212:                 filepath,
 213:                 line_num,
 214:             )



FILE: dkg\parsers\tcl_parser.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Dict, Optional
   5: 
   6: from ..core.graph import DKGEdge, DKGNode
   7: from ..builders.graph_updater import GraphUpdater
   8: from ..pipeline.stages import FieldSource, ParsingStage
   9: from . import ConstraintParser
  10: 
  11: 
  12: class TclParser(ConstraintParser):
  13:     """
  14:     TCL floorplan parser.
  15: 
  16:     - design vs sim flag
  17:     - top scope
  18:     """
  19: 
  20:     def get_stage(self) -> ParsingStage:
  21:         return ParsingStage.FLOORPLAN
  22: 
  23:     def parse_and_update(
  24:         self,
  25:         filepath: str,
  26:         updater: GraphUpdater,
  27:         nodes: Dict[str, DKGNode],
  28:         edges: Dict[str, DKGEdge],
  29:     ) -> None:
  30:         with open(filepath, "r", encoding="utf-8") as f:
  31:             lines = f.readlines()
  32: 
  33:         top_scope: Optional[str] = None
  34:         design_context: Optional[str] = None
  35: 
  36:         for line in lines:
  37:             raw = line.strip()
  38:             if not raw or raw.startswith("#"):
  39:                 continue
  40: 
  41:             top_scope = top_scope or self._parse_top_scope(raw)
  42:             design_context = design_context or self._parse_design_context(raw)
  43: 
  44:         if not top_scope and not design_context:
  45:             return
  46: 
  47:         for node_id, node in nodes.items():
  48:             if top_scope and node.hier_path != top_scope:
  49:                 continue
  50: 
  51:             new_attrs = dict(node.attributes)
  52:             if top_scope:
  53:                 new_attrs["top_scope"] = top_scope
  54:             if design_context:
  55:                 new_attrs["design_context"] = design_context
  56: 
  57:             updater.update_node_field(
  58:                 node_id,
  59:                 "attributes",
  60:                 new_attrs,
  61:                 FieldSource.DECLARED,
  62:                 ParsingStage.FLOORPLAN,
  63:                 filepath,
  64:                 None,
  65:             )
  66: 
  67:     def _parse_top_scope(self, line: str) -> Optional[str]:
  68:         match = re.search(r"set_property\s+top\s+(\S+)", line)
  69:         if match:
  70:             return match.group(1)
  71: 
  72:         match = re.search(r"set\s+top_(?:module|scope)\s+(\S+)", line)
  73:         if match:
  74:             return match.group(1)
  75: 
  76:         return None
  77: 
  78:     def _parse_design_context(self, line: str) -> Optional[str]:
  79:         if "-simset" in line or "simulation" in line.lower():
  80:             return "sim"
  81:         if "-constrset" in line or "synth" in line.lower():
  82:             return "design"
  83: 
  84:         match = re.search(r"set_property\s+design_mode\s+(\S+)", line)
  85:         if match:
  86:             value = match.group(1).lower()
  87:             return "sim" if "sim" in value else "design"
  88: 
  89:         return None



FILE: dkg\parsers\timing_report_parser.py

   1: """
   2: íƒ€ì´ë° ë¦¬í¬íŠ¸ íŒŒì„œ (Vivado/PrimeTime)
   3: 
   4: íƒ€ì´ë° ë¦¬í¬íŠ¸ì—ì„œ DKG ê·¸ë˜í”„ êµ¬ì¶•ì— í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ:
   5: - Startpoint/Endpoint
   6: - ê²½ë¡œìƒì˜ ëª¨ë“  ì…€
   7: - ê° ë‹¨ê³„ë³„ delay
   8: - ìµœì¢… slack
   9: - í´ëŸ­ ë„ë©”ì¸
  10: 
  11: âš ï¸ ì¤‘ìš”: í•œ ë…¸ë“œ/ì—£ì§€ëŠ” ì—¬ëŸ¬ íƒ€ì´ë° ê²½ë¡œì— ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŒ
  12: - Setup pathì™€ Hold pathê°€ ë‹¤ë¦„
  13: - ì—¬ëŸ¬ í´ëŸ­ ë„ë©”ì¸ ì¡´ì¬
  14: - ë”°ë¼ì„œ worst-case ê°’ë§Œ ì €ì¥í•˜ê³ , ìƒì„¸ ì •ë³´ëŠ” ë©”íƒ€ë°ì´í„°ì— ëˆ„ì 
  15: """
  16: from __future__ import annotations
  17: 
  18: import re
  19: from dataclasses import dataclass, field
  20: from pathlib import Path
  21: from typing import Dict, List, Optional
  22: 
  23: from ..core.graph import DKGEdge, DKGNode
  24: from ..builders.graph_updater import GraphUpdater
  25: 
  26: 
  27: @dataclass
  28: class TimingStage:
  29:     """íƒ€ì´ë° ê²½ë¡œì˜ í•œ ë‹¨ê³„"""
  30:     point: str              # ì…€/í•€ ì´ë¦„
  31:     incr_delay: float       # ì¦ë¶„ delay (ns)
  32:     cumulative_delay: float # ëˆ„ì  delay (ns)
  33:     transition: str         # 'r' (rising) or 'f' (falling)
  34: 
  35: 
  36: @dataclass
  37: class TimingPath:
  38:     """í•˜ë‚˜ì˜ íƒ€ì´ë° ê²½ë¡œ"""
  39:     startpoint: str
  40:     endpoint: str
  41:     clock: str
  42:     path_type: str  # 'Setup' or 'Hold'
  43:     
  44:     slack: Optional[float] = None
  45:     arrival_time: Optional[float] = None
  46:     required_time: Optional[float] = None
  47:     
  48:     stages: List[TimingStage] = field(default_factory=list)
  49: 
  50: 
  51: class TimingReportParser:
  52:     """íƒ€ì´ë° ë¦¬í¬íŠ¸ íŒŒì„œ (Vivado/PrimeTime í˜•ì‹)"""
  53:     
  54:     def __init__(self):
  55:         self.paths: List[TimingPath] = []
  56:     
  57:     def parse_file(self, filepath: str | Path) -> List[TimingPath]:
  58:         """íƒ€ì´ë° ë¦¬í¬íŠ¸ íŒŒì¼ ì „ì²´ íŒŒì‹±"""
  59:         filepath = Path(filepath)
  60:         with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
  61:             content = f.read()
  62:         
  63:         # "Startpoint:"ë¡œ ì‹œì‘í•˜ëŠ” ê° ê²½ë¡œ ì„¹ì…˜ ë¶„ë¦¬
  64:         # Vivado: "Startpoint: ..."
  65:         # PrimeTime: "Point ..."ë¡œ ì‹œì‘í•˜ì§€ë§Œ í—¤ë”ê°€ ë‹¤ë¦„
  66:         
  67:         # Vivado í˜•ì‹ ê°ì§€
  68:         if 'Startpoint:' in content:
  69:             return self._parse_vivado_format(content)
  70:         else:
  71:             # TODO: PrimeTime í˜•ì‹ ì§€ì›
  72:             return []
  73:     
  74:     def _parse_vivado_format(self, content: str) -> List[TimingPath]:
  75:         """Vivado íƒ€ì´ë° ë¦¬í¬íŠ¸ íŒŒì‹±"""
  76:         paths = []
  77:         
  78:         # "Startpoint:"ë¡œ êµ¬ë¶„
  79:         sections = re.split(r'\n(?=Startpoint:)', content)
  80:         
  81:         for section in sections:
  82:             if 'Startpoint:' not in section:
  83:                 continue
  84:             
  85:             path = self._parse_single_path(section)
  86:             if path:
  87:                 paths.append(path)
  88:         
  89:         self.paths = paths
  90:         return paths
  91:     
  92:     def _parse_single_path(self, section: str) -> Optional[TimingPath]:
  93:         """ê°œë³„ íƒ€ì´ë° ê²½ë¡œ íŒŒì‹±"""
  94:         path = TimingPath(
  95:             startpoint='',
  96:             endpoint='',
  97:             clock='',
  98:             path_type='Setup',
  99:         )
 100:         
 101:         # Startpoint ì¶”ì¶œ
 102:         # ì˜ˆ: "Startpoint: cpu/pc_reg[0] (rising edge-triggered flip-flop clocked by sys_clk)"
 103:         start_match = re.search(r'Startpoint:\s+(\S+)', section)
 104:         if not start_match:
 105:             return None
 106:         path.startpoint = start_match.group(1)
 107:         
 108:         # Endpoint ì¶”ì¶œ
 109:         end_match = re.search(r'Endpoint:\s+(\S+)', section)
 110:         if end_match:
 111:             path.endpoint = end_match.group(1)
 112:         
 113:         # Clock ì¶”ì¶œ
 114:         clock_match = re.search(r'clocked by (\w+)', section)
 115:         if clock_match:
 116:             path.clock = clock_match.group(1)
 117:         
 118:         # Path Type ì¶”ì¶œ
 119:         type_match = re.search(r'Path Type:\s+(\w+)', section)
 120:         if type_match:
 121:             path.path_type = type_match.group(1)
 122:         
 123:         # Slack ì¶”ì¶œ
 124:         # ì˜ˆ: "slack (MET)                                         9.37"
 125:         slack_match = re.search(r'slack.*?([-\d.]+)', section, re.IGNORECASE)
 126:         if slack_match:
 127:             path.slack = float(slack_match.group(1))
 128:         
 129:         # Arrival/Required time ì¶”ì¶œ
 130:         arrival_match = re.search(r'data arrival time\s+([\d.]+)', section)
 131:         if arrival_match:
 132:             path.arrival_time = float(arrival_match.group(1))
 133:         
 134:         required_match = re.search(r'data required time\s+([\d.]+)', section)
 135:         if required_match:
 136:             path.required_time = float(required_match.group(1))
 137:         
 138:         # íƒ€ì´ë° í…Œì´ë¸” íŒŒì‹±
 139:         # í˜•ì‹:
 140:         #   Point                                    Incr       Path
 141:         #   --------------------------------------------------------
 142:         #   cpu/pc_reg[0]/Q (DFFQX1)                 0.15       0.65 r
 143:         #   cpu/decode_inst/U123/Y (AND2X1)          0.08       0.73 r
 144:         
 145:         table_pattern = r'Point\s+Incr\s+Path\s*\n\s*-+\s*\n(.*?)\n\s*data arrival time'
 146:         table_match = re.search(table_pattern, section, re.DOTALL)
 147:         
 148:         if table_match:
 149:             table_content = table_match.group(1)
 150:             path.stages = self._parse_timing_table(table_content)
 151:         
 152:         return path
 153:     
 154:     def _parse_timing_table(self, table_content: str) -> List[TimingStage]:
 155:         """íƒ€ì´ë° í…Œì´ë¸” íŒŒì‹±"""
 156:         stages = []
 157:         lines = table_content.strip().split('\n')
 158:         
 159:         for line in lines:
 160:             line = line.strip()
 161:             if not line or line.startswith('-'):
 162:                 continue
 163:             
 164:             stage = self._parse_timing_line(line)
 165:             if stage:
 166:                 stages.append(stage)
 167:         
 168:         return stages
 169:     
 170:     def _parse_timing_line(self, line: str) -> Optional[TimingStage]:
 171:         """íƒ€ì´ë° í…Œì´ë¸”ì˜ í•œ ì¤„ íŒŒì‹±"""
 172:         # ì—¬ëŸ¬ í˜•ì‹ ì§€ì›:
 173:         # 1. "cpu/pc_reg[0]/Q (DFFQX1)     0.15       0.65 r"
 174:         # 2. "clock network delay (ideal)  0.50       0.50"
 175:         # 3. "U123/Y (AND2X1)               0.08       0.73 r"
 176:         
 177:         # íŒ¨í„´: ì…€ ì´ë¦„, ì¦ë¶„ delay, ëˆ„ì  delay, transition (ì˜µì…˜)
 178:         pattern = r'^\s*(\S+(?:\s+\([^)]+\))?)\s+([-\d.]+)\s+([-\d.]+)\s*([rf])?'
 179:         match = re.match(pattern, line)
 180:         
 181:         if not match:
 182:             return None
 183:         
 184:         point = match.group(1)
 185:         # ê´„í˜¸ ì•ˆì˜ ì…€ íƒ€ì… ì œê±°
 186:         point = re.sub(r'\s*\([^)]+\)', '', point).strip()
 187:         
 188:         incr = float(match.group(2))
 189:         path = float(match.group(3))
 190:         transition = match.group(4) or ''
 191:         
 192:         return TimingStage(
 193:             point=point,
 194:             incr_delay=incr,
 195:             cumulative_delay=path,
 196:             transition=transition,
 197:         )
 198:     
 199:     def apply_to_graph(
 200:         self,
 201:         nodes: Dict[str, DKGNode],
 202:         edges: Dict[str, DKGEdge],
 203:         updater: GraphUpdater,
 204:     ) -> None:
 205:         """íŒŒì‹±í•œ íƒ€ì´ë° ì •ë³´ë¥¼ DKG ê·¸ë˜í”„ì— ë°˜ì˜"""
 206:         from ..stages import FieldSource, ParsingStage
 207:         
 208:         for path in self.paths:
 209:             # 1. Startpoint/Endpoint ë…¸ë“œ ì—…ë°ì´íŠ¸
 210:             self._update_node_timing(
 211:                 path.startpoint, path, nodes, updater, is_endpoint=False
 212:             )
 213:             self._update_node_timing(
 214:                 path.endpoint, path, nodes, updater, is_endpoint=True
 215:             )
 216:             
 217:             # 2. ê²½ë¡œìƒ ê° ì—£ì§€ì— delay ì„¤ì •
 218:             for i in range(len(path.stages) - 1):
 219:                 src_stage = path.stages[i]
 220:                 dst_stage = path.stages[i + 1]
 221:                 
 222:                 self._update_edge_timing(
 223:                     src_stage, dst_stage, path, edges, updater
 224:                 )
 225:     
 226:     def _update_node_timing(
 227:         self,
 228:         node_name: str,
 229:         path: TimingPath,
 230:         nodes: Dict[str, DKGNode],
 231:         updater: GraphUpdater,
 232:         is_endpoint: bool,
 233:     ) -> None:
 234:         """ë…¸ë“œì˜ íƒ€ì´ë° ì •ë³´ ì—…ë°ì´íŠ¸
 235:         
 236:         ì£¼ì˜: í•œ ë…¸ë“œëŠ” ì—¬ëŸ¬ ê²½ë¡œì— ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ:
 237:         - slackì€ ìµœì•…ê°’(worst-case)ë§Œ ì €ì¥
 238:         - ìƒì„¸ ì •ë³´ëŠ” ë©”íƒ€ë°ì´í„°ì— ëˆ„ì 
 239:         """
 240:         from ..stages import FieldSource, ParsingStage
 241:         
 242:         # ë…¸ë“œ ì´ë¦„ ì •ê·œí™” (hier_path ë˜ëŠ” canonical_name ë§¤ì¹­)
 243:         node = self._find_node_by_name(node_name, nodes)
 244:         if not node:
 245:             return
 246:         
 247:         node_id = node.node_id
 248:         
 249:         # Slack ì—…ë°ì´íŠ¸ (startpointì—ë§Œ, ìµœì•…ê°’ë§Œ)
 250:         if not is_endpoint and path.slack is not None:
 251:             # ê¸°ì¡´ slackë³´ë‹¤ ë‚˜ì˜ë©´(ì‘ìœ¼ë©´) ì—…ë°ì´íŠ¸
 252:             if node.slack is None or path.slack < node.slack:
 253:                 node.slack = path.slack
 254:             
 255:             # ë©”íƒ€ë°ì´í„°ì—ëŠ” ê²½ë¡œë³„ë¡œ ëˆ„ì  ì €ì¥ (ë¦¬ìŠ¤íŠ¸)
 256:             metadata = updater.node_metadata[node_id]
 257:             existing_slacks = metadata.get('timing_slacks', [])
 258:             existing_slacks.append({
 259:                 'slack': path.slack,
 260:                 'path_type': path.path_type,
 261:                 'clock': path.clock,
 262:                 'endpoint': path.endpoint,
 263:             })
 264:             metadata.set(
 265:                 'timing_slacks',
 266:                 existing_slacks,
 267:                 FieldSource.ANALYZED,
 268:                 ParsingStage.TIMING,
 269:             )
 270:         
 271:         # Arrival time - ì—¬ëŸ¬ ê°’ ì¤‘ ìµœì•…(ìµœëŒ€)ë§Œ ì €ì¥
 272:         if path.arrival_time is not None:
 273:             if node.arrival_time is None or path.arrival_time > node.arrival_time:
 274:                 node.arrival_time = path.arrival_time
 275:         
 276:         # Required time - ì—¬ëŸ¬ ê°’ ì¤‘ ìµœì„ (ìµœì†Œ)ë§Œ ì €ì¥
 277:         if path.required_time is not None:
 278:             if node.required_time is None or path.required_time < node.required_time:
 279:                 node.required_time = path.required_time
 280:         
 281:         # Clock domain - ì—¬ëŸ¬ í´ëŸ­ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëŒ€í‘œê°’ë§Œ ì €ì¥
 282:         # (ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ í´ëŸ­ or ê°€ì¥ ë¹ˆë²ˆí•œ í´ëŸ­)
 283:         if path.clock and not node.clock_domain:
 284:             node.clock_domain = path.clock
 285:     
 286:     def _update_edge_timing(
 287:         self,
 288:         src_stage: TimingStage,
 289:         dst_stage: TimingStage,
 290:         path: TimingPath,
 291:         edges: Dict[str, DKGEdge],
 292:         updater: GraphUpdater,
 293:     ) -> None:
 294:         """ì—£ì§€ì˜ íƒ€ì´ë° ì •ë³´ ì—…ë°ì´íŠ¸
 295:         
 296:         ì£¼ì˜: í•œ ì—£ì§€ë„ ì—¬ëŸ¬ ê²½ë¡œì— ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ:
 297:         - delayëŠ” ìµœì•…ê°’ë§Œ ì €ì¥ (ì¼ë°˜ì ìœ¼ë¡œ ë™ì¼í•´ì•¼ í•¨)
 298:         - ìƒì„¸ ì •ë³´ëŠ” ë©”íƒ€ë°ì´í„°ì— ëˆ„ì 
 299:         """
 300:         from ..stages import FieldSource, ParsingStage
 301:         
 302:         # ì—£ì§€ ì°¾ê¸° (íœ´ë¦¬ìŠ¤í‹±: src/dst ì´ë¦„ ê¸°ë°˜)
 303:         edge = self._find_edge_by_pins(src_stage.point, dst_stage.point, edges)
 304:         if not edge:
 305:             return
 306:         
 307:         edge_id = edge.edge_id
 308:         
 309:         # Delay ì—…ë°ì´íŠ¸ - ìµœëŒ€ê°’ ì €ì¥ (ë³´ìˆ˜ì )
 310:         if edge.delay is None or dst_stage.incr_delay > edge.delay:
 311:             edge.delay = dst_stage.incr_delay
 312:         
 313:         # ë©”íƒ€ë°ì´í„°ì— ê²½ë¡œë³„ delay ëˆ„ì 
 314:         metadata = updater.edge_metadata[edge_id]
 315:         existing_delays = metadata.get('timing_delays', [])
 316:         existing_delays.append({
 317:             'delay': dst_stage.incr_delay,
 318:             'path_type': path.path_type,
 319:             'clock': path.clock,
 320:         })
 321:         metadata.set(
 322:             'timing_delays',
 323:             existing_delays,
 324:             FieldSource.ANALYZED,
 325:             ParsingStage.TIMING,
 326:         )
 327:         
 328:         # Arrival time - ìµœëŒ€ê°’ë§Œ ì €ì¥
 329:         if edge.arrival_time is None or dst_stage.cumulative_delay > edge.arrival_time:
 330:             edge.arrival_time = dst_stage.cumulative_delay
 331:         
 332:         # Clock domain - ì²« ë²ˆì§¸ í´ëŸ­ ì €ì¥ (ë˜ëŠ” ê°€ì¥ ë¹ˆë²ˆí•œ í´ëŸ­)
 333:         if path.clock and not edge.clock_domain_id:
 334:             edge.clock_domain_id = path.clock
 335:     
 336:     def _find_node_by_name(
 337:         self, name: str, nodes: Dict[str, DKGNode]
 338:     ) -> Optional[DKGNode]:
 339:         """ì´ë¦„ìœ¼ë¡œ ë…¸ë“œ ì°¾ê¸° (íœ´ë¦¬ìŠ¤í‹±)"""
 340:         # 1. ì§ì ‘ ë§¤ì¹­
 341:         if name in nodes:
 342:             return nodes[name]
 343:         
 344:         # 2. hier_pathë¡œ ë§¤ì¹­
 345:         for node in nodes.values():
 346:             if node.hier_path == name:
 347:                 return node
 348:         
 349:         # 3. canonical_nameìœ¼ë¡œ ë§¤ì¹­
 350:         for node in nodes.values():
 351:             if node.canonical_name == name:
 352:                 return node
 353:         
 354:         # 4. ë¶€ë¶„ ë§¤ì¹­ (ë§ˆì§€ë§‰ ì‹œë„)
 355:         for node in nodes.values():
 356:             if name in node.hier_path or node.hier_path in name:
 357:                 return node
 358:         
 359:         return None
 360:     
 361:     def _find_edge_by_pins(
 362:         self, src_pin: str, dst_pin: str, edges: Dict[str, DKGEdge]
 363:     ) -> Optional[DKGEdge]:
 364:         """src/dst í•€ ì´ë¦„ìœ¼ë¡œ ì—£ì§€ ì°¾ê¸° (íœ´ë¦¬ìŠ¤í‹±)"""
 365:         # íƒ€ì´ë° ë¦¬í¬íŠ¸ì˜ í•€ ì´ë¦„ê³¼ DKG ì—£ì§€ì˜ ë…¸ë“œ ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
 366:         # ì˜ˆ: "cpu/pc_reg[0]/Q" vs "cpu.pc_reg[0]"
 367:         
 368:         for edge in edges.values():
 369:             src_node_name = edge.src_node
 370:             dst_node_name = edge.dst_node
 371:             
 372:             # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
 373:             if (src_pin in src_node_name or src_node_name in src_pin) and \
 374:                (dst_pin in dst_node_name or dst_node_name in dst_pin):
 375:                 return edge
 376:         
 377:         return None
 378:     
 379:     def get_summary(self) -> Dict:
 380:         """íŒŒì‹± ê²°ê³¼ ìš”ì•½"""
 381:         if not self.paths:
 382:             return {'total_paths': 0}
 383:         
 384:         slacks = [p.slack for p in self.paths if p.slack is not None]
 385:         worst_slack = min(slacks) if slacks else None
 386:         met_count = sum(1 for s in slacks if s >= 0)
 387:         
 388:         return {
 389:             'total_paths': len(self.paths),
 390:             'worst_slack': worst_slack,
 391:             'met_timing': met_count,
 392:             'failed_timing': len(slacks) - met_count,
 393:             'clocks': list(set(p.clock for p in self.paths if p.clock)),
 394:         }



FILE: dkg\parsers\xdc_parser.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Dict
   5: 
   6: from ..core.graph import DKGEdge, DKGNode
   7: from ..builders.graph_updater import GraphUpdater
   8: from ..pipeline.stages import FieldSource, ParsingStage
   9: from . import ConstraintParser
  10: from .parser_utils import extract_bracket_targets, match_any
  11: 
  12: 
  13: class XdcParser(ConstraintParser):
  14:     """
  15:     XDC (Xilinx Design Constraints) íŒŒì„œ.
  16:     
  17:     SDCì™€ ìœ ì‚¬í•˜ì§€ë§Œ Xilinx íŠ¹í™” ëª…ë ¹ í¬í•¨:
  18:     - set_property LOC / IOSTANDARD: í•€ ë°°ì¹˜
  19:     - create_pblock: ë¬¼ë¦¬ì  ë¸”ë¡ ì •ì˜
  20:     """
  21:     
  22:     def get_stage(self) -> ParsingStage:
  23:         return ParsingStage.CONSTRAINTS
  24:     
  25:     def parse_and_update(
  26:         self,
  27:         filepath: str,
  28:         updater: GraphUpdater,
  29:         nodes: Dict[str, DKGNode],
  30:         edges: Dict[str, DKGEdge],
  31:     ) -> None:
  32:         with open(filepath, "r", encoding="utf-8") as f:
  33:             lines = f.readlines()
  34: 
  35:         for line_num, line in enumerate(lines, start=1):
  36:             raw = line.strip()
  37:             if not raw or raw.startswith("#"):
  38:                 continue
  39: 
  40:             if raw.startswith("set_property"):
  41:                 self._parse_set_property(raw, line_num, filepath, updater, nodes)
  42:             elif raw.startswith("create_pblock"):
  43:                 self._parse_create_pblock(raw)
  44:             elif raw.startswith("add_cells_to_pblock"):
  45:                 self._parse_add_cells_to_pblock(raw, line_num, filepath, updater, nodes)
  46: 
  47:     def _parse_set_property(
  48:         self,
  49:         line: str,
  50:         line_num: int,
  51:         filepath: str,
  52:         updater: GraphUpdater,
  53:         nodes: Dict[str, DKGNode],
  54:     ) -> None:
  55:         match = re.search(r"set_property\s+(LOC|IOSTANDARD)\s+(\S+)", line)
  56:         if not match:
  57:             return
  58: 
  59:         prop = match.group(1)
  60:         value = match.group(2)
  61:         targets = extract_bracket_targets(line, ("ports", "pins", "cells"))
  62:         if not targets:
  63:             return
  64: 
  65:         for node_id, node in nodes.items():
  66:             candidates = [node.local_name, node.hier_path, node.canonical_name]
  67:             if not match_any(targets, candidates):
  68:                 continue
  69: 
  70:             new_attrs = dict(node.attributes)
  71:             new_attrs[prop] = value
  72:             updater.update_node_field(
  73:                 node_id,
  74:                 "attributes",
  75:                 new_attrs,
  76:                 FieldSource.DECLARED,
  77:                 ParsingStage.CONSTRAINTS,
  78:                 filepath,
  79:                 line_num,
  80:             )
  81: 
  82:     def _parse_create_pblock(self, line: str) -> None:
  83:         match = re.search(r"create_pblock\s+(\S+)", line)
  84:         if not match:
  85:             return
  86: 
  87:     def _parse_add_cells_to_pblock(
  88:         self,
  89:         line: str,
  90:         line_num: int,
  91:         filepath: str,
  92:         updater: GraphUpdater,
  93:         nodes: Dict[str, DKGNode],
  94:     ) -> None:
  95:         pblock_name = None
  96:         pblock_match = re.search(r"add_cells_to_pblock\s+\[get_pblocks\s+([^\]]+)\]", line)
  97:         if pblock_match:
  98:             pblock_name = pblock_match.group(1).strip()
  99:         else:
 100:             direct = re.search(r"add_cells_to_pblock\s+(\S+)", line)
 101:             if direct:
 102:                 pblock_name = direct.group(1)
 103: 
 104:         if not pblock_name:
 105:             return
 106: 
 107:         targets = extract_bracket_targets(line, ("cells",))
 108:         if not targets:
 109:             return
 110: 
 111:         for node_id, node in nodes.items():
 112:             candidates = [node.local_name, node.hier_path, node.canonical_name]
 113:             if not match_any(targets, candidates):
 114:                 continue
 115: 
 116:             new_attrs = dict(node.attributes)
 117:             new_attrs["pblock"] = pblock_name
 118:             new_attrs["pblock_seed"] = pblock_name
 119:             updater.update_node_field(
 120:                 node_id,
 121:                 "attributes",
 122:                 new_attrs,
 123:                 FieldSource.DECLARED,
 124:                 ParsingStage.FLOORPLAN,
 125:                 filepath,
 126:                 line_num,
 127:             )
 128: 



FILE: dkg\parsers\yosys_parser.py

   1: from __future__ import annotations
   2: 
   3: import glob
   4: import json
   5: import os
   6: import subprocess
   7: from pathlib import Path
   8: from typing import List
   9: 
  10: from ..utils.config import YosysConfig
  11: from ..utils import win_to_wsl_path
  12: 
  13: 
  14: def collect_hdl_files(src_dir_win: str) -> List[str]:
  15:     verilog_files = glob.glob(os.path.join(src_dir_win, "*.v"))
  16:     sv_files = glob.glob(os.path.join(src_dir_win, "*.sv"))
  17:     return verilog_files + sv_files
  18: 
  19: 
  20: def build_yosys_script(files_wsl: List[str], top_module: str, out_json_wsl: str) -> str:
  21:     return "\n".join(
  22:         [
  23:             f"read_verilog -sv {' '.join(files_wsl)};",
  24:             f"hierarchy -check -top {top_module};",
  25:             "proc;",
  26:             "opt;",
  27:             f"write_json {out_json_wsl}",
  28:         ]
  29:     )
  30: 
  31: 
  32: def run_yosys(files_win: List[str], config: YosysConfig) -> None:
  33:     if not files_win:
  34:         raise RuntimeError("No HDL files found.")
  35: 
  36:     files_wsl = [win_to_wsl_path(f) for f in files_win]
  37:     out_json_wsl = win_to_wsl_path(config.out_json_win)
  38:     yosys_script = build_yosys_script(files_wsl, config.top_module, out_json_wsl)
  39: 
  40:     subprocess.run(["wsl", "yosys", "-p", yosys_script], check=True)
  41: 
  42: 
  43: def load_yosys_json(out_json_win: str) -> dict:
  44:     with open(out_json_win, "r", encoding="utf-8") as f:
  45:         return json.load(f)
  46: 
  47: 
  48: def parse_yosys(config: YosysConfig) -> dict:
  49:     files_win = collect_hdl_files(config.src_dir_win)
  50:     run_yosys(files_win, config)
  51:     return load_yosys_json(config.out_json_win)



FILE: dkg\pipeline\pipeline.py

   1: from __future__ import annotations
   2: 
   3: from pathlib import Path
   4: from typing import Dict, List, Optional
   5: from ..utils.config import YosysConfig
   6: from ..core.graph import DKGEdge, DKGNode
   7: from ..builders.graph_build import build_nodes_and_edges, build_wires_and_cells
   8: from ..builders.graph_updater import GraphUpdater
   9: from ..cache import GraphSnapshot, GraphVersion, load_snapshot, save_snapshot
  10: from ..parsers import ConstraintParser
  11: from ..parsers.sdc_parser import SdcParser
  12: from ..parsers.tcl_parser import TclParser
  13: from ..parsers.timing_report_parser import TimingReportParser
  14: from ..parsers.xdc_parser import XdcParser
  15: from ..parsers.bd_parser import BdParser
  16: from .stages import FieldSource, ParsingStage
  17: from ..builders.supergraph import SuperGraph, GraphContext, ViewBuilder, GraphViewType
  18: from ..utils import compute_file_hash
  19: from ..parsers.yosys_parser import parse_yosys
  20: 
  21: 
  22: class DKGPipeline:
  23:     """
  24:     ì „ì²´ DKG êµ¬ì¶• íŒŒì´í”„ë¼ì¸.
  25:     
  26:     Usage:
  27:         pipeline = DKGPipeline(yosys_config)
  28:         
  29:         # Stage 1: RTL íŒŒì‹±
  30:         pipeline.run_rtl_stage()
  31:         
  32:         # Stage 2: Constraint ì¶”ê°€
  33:         pipeline.add_constraints("design.sdc")
  34:         pipeline.add_constraints("design.xdc")
  35:         
  36:         # Stage 3: íƒ€ì´ë° ë¦¬í¬íŠ¸ ì¶”ê°€
  37:         pipeline.add_timing_report("timing.rpt")
  38:         
  39:         # ìµœì¢… ê·¸ë˜í”„ ë°˜í™˜
  40:         nodes, edges = pipeline.get_graph()
  41:     """
  42:     
  43:     def __init__(self, yosys_config: YosysConfig):
  44:         self.yosys_config = yosys_config
  45:         
  46:         self.nodes: Optional[Dict[str, DKGNode]] = None
  47:         self.edges: Optional[Dict[str, DKGEdge]] = None
  48:         self.updater: Optional[GraphUpdater] = None
  49:         self.supergraph: Optional[SuperGraph] = None
  50:         
  51:         self.current_stage = None
  52:         self.completed_stages: List[ParsingStage] = []
  53:         
  54:         # ì…ë ¥ íŒŒì¼ ì¶”ì  (ë²„ì „ ê³„ì‚°ìš©)
  55:         self.rtl_files: List[str] = []
  56:         self.constraint_files: List[str] = []
  57:         self.timing_files: List[str] = []
  58:         
  59:         # íŒŒì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬
  60:         self.parsers: Dict[str, ConstraintParser] = {
  61:             "sdc": SdcParser(),
  62:             "xdc": XdcParser(),
  63:             # TODO: ì¶”ê°€ íŒŒì„œ ë“±ë¡
  64:         }
  65:     
  66:     def run_rtl_stage(self) -> None:
  67:         """Stage 1: RTL íŒŒì‹± (Yosys)"""
  68:         yosys = parse_yosys(self.yosys_config)
  69:         wires, cells = build_wires_and_cells(yosys)
  70:         self.nodes, self.edges = build_nodes_and_edges(wires, cells)
  71:         
  72:         # RTL íŒŒì¼ ì¶”ì 
  73:         if self.yosys_config.out_json_win:
  74:             self.rtl_files.append(self.yosys_config.out_json_win)
  75:         
  76:         self.updater = GraphUpdater(self.nodes, self.edges)
  77:         self.current_stage = ParsingStage.RTL
  78:         self.completed_stages.append(ParsingStage.RTL)
  79:         
  80:         # ì´ˆê¸° ë©”íƒ€ë°ì´í„° ì„¤ì • (ëª¨ë‘ INFERRED)
  81:         self._mark_initial_fields_as_inferred()
  82:     
  83:     def add_constraints(self, filepath: str) -> None:
  84:         """Stage 2: Constraint íŒŒì¼ ì¶”ê°€"""
  85:         if self.updater is None or self.nodes is None or self.edges is None:
  86:             raise RuntimeError("RTL stage must be run first")
  87:         
  88:         # íŒŒì¼ í™•ì¥ìë¡œ íŒŒì„œ ì„ íƒ
  89:         ext = Path(filepath).suffix.lower().lstrip(".")
  90:         
  91:         if ext not in self.parsers:
  92:             raise ValueError(f"Unsupported constraint format: {ext}")
  93:         
  94:         parser = self.parsers[ext]
  95:         parser.parse_and_update(filepath, self.updater, self.nodes, self.edges)
  96:         
  97:         # ì œì•½ íŒŒì¼ ì¶”ì 
  98:         self.constraint_files.append(filepath)
  99:         
 100:         if ParsingStage.CONSTRAINTS not in self.completed_stages:
 101:             self.completed_stages.append(ParsingStage.CONSTRAINTS)
 102:     
 103:     def add_timing_report(self, filepath: str) -> None:
 104:         """Stage 3: íƒ€ì´ë° ë¦¬í¬íŠ¸ ì¶”ê°€"""
 105:         if self.updater is None or self.nodes is None or self.edges is None:
 106:             raise RuntimeError("RTL stage must be run first")
 107:         
 108:         # íƒ€ì´ë° ë¦¬í¬íŠ¸ íŒŒì‹±
 109:         parser = TimingReportParser()
 110:         paths = parser.parse_file(filepath)
 111:         
 112:         # ê·¸ë˜í”„ì— ë°˜ì˜
 113:         parser.apply_to_graph(self.nodes, self.edges, self.updater)
 114:         
 115:         # íŒŒì¼ ì¶”ì 
 116:         self.timing_files.append(filepath)
 117:         
 118:         if ParsingStage.TIMING not in self.completed_stages:
 119:             self.completed_stages.append(ParsingStage.TIMING)
 120:         
 121:         # ìš”ì•½ ì¶œë ¥
 122:         summary = parser.get_summary()
 123:         print(f"âœ… íƒ€ì´ë° ë¦¬í¬íŠ¸ íŒŒì‹± ì™„ë£Œ: {filepath}")
 124:         print(f"   - ê²½ë¡œ ìˆ˜: {summary['total_paths']}")
 125:         if summary.get('worst_slack') is not None:
 126:             print(f"   - ìµœì•… slack: {summary['worst_slack']:.2f} ns")
 127:     
 128:     def add_floorplan(self, filepath: str) -> None:
 129:         """Stage 4: Floorplan TCL ì¶”ê°€"""
 130:         if self.updater is None or self.nodes is None or self.edges is None:
 131:             raise RuntimeError("RTL stage must be run first")
 132: 
 133:         parser = TclParser()
 134:         parser.parse_and_update(filepath, self.updater, self.nodes, self.edges)
 135: 
 136:         if ParsingStage.FLOORPLAN not in self.completed_stages:
 137:             self.completed_stages.append(ParsingStage.FLOORPLAN)
 138: 
 139:     def add_board(self, filepath: str) -> None:
 140:         """Stage 5: BD/board constraints ì¶”ê°€"""
 141:         if self.updater is None or self.nodes is None or self.edges is None:
 142:             raise RuntimeError("RTL stage must be run first")
 143: 
 144:         parser = BdParser()
 145:         parser.parse_and_update(filepath, self.updater, self.nodes, self.edges)
 146: 
 147:         if ParsingStage.BOARD not in self.completed_stages:
 148:             self.completed_stages.append(ParsingStage.BOARD)
 149:     
 150:     def get_graph(self) -> tuple[Dict[str, DKGNode], Dict[str, DKGEdge]]:
 151:         """ìµœì¢… ê·¸ë˜í”„ ë°˜í™˜"""
 152:         if self.nodes is None or self.edges is None:
 153:             raise RuntimeError("No graph available. Run RTL stage first.")
 154:         return self.nodes, self.edges
 155:     
 156:     def get_updater(self) -> GraphUpdater:
 157:         """GraphUpdater ë°˜í™˜ (ê³ ê¸‰ ì‚¬ìš©ììš©)"""
 158:         if self.updater is None:
 159:             raise RuntimeError("No updater available. Run RTL stage first.")
 160:         return self.updater
 161:     
 162:     def export_metadata(self) -> dict:
 163:         """ë©”íƒ€ë°ì´í„° ìš”ì•½ ë°˜í™˜ (ìºì‹±/ë””ë²„ê¹…ìš©)"""
 164:         if self.updater is None:
 165:             return {}
 166:         return self.updater.export_metadata_summary()
 167:     
 168:     def _mark_initial_fields_as_inferred(self) -> None:
 169:         """RTL stageì—ì„œ ì¶”ë¡ í•œ í•„ë“œë“¤ì„ INFERREDë¡œ ë§ˆí‚¹"""
 170:         if self.nodes is None or self.edges is None or self.updater is None:
 171:             return
 172:         
 173:         # clock_domain, flow_type ë“± íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì±„ìš´ í•„ë“œë“¤
 174:         for node_id, node in self.nodes.items():
 175:             if node.clock_domain:
 176:                 self.updater.node_metadata[node_id].set(
 177:                     "clock_domain",
 178:                     node.clock_domain,
 179:                     FieldSource.INFERRED,
 180:                     ParsingStage.RTL,
 181:                 )
 182:         
 183:         for edge_id, edge in self.edges.items():
 184:             if edge.flow_type:
 185:                 self.updater.edge_metadata[edge_id].set(
 186:                     "flow_type",
 187:                     edge.flow_type.value,
 188:                     FieldSource.INFERRED,
 189:                     ParsingStage.RTL,
 190:                 )
 191:     
 192:     def compute_version(self) -> GraphVersion:
 193:         """í˜„ì¬ ìƒíƒœì˜ GraphVersion ê³„ì‚°"""
 194:         import hashlib
 195:         
 196:         # RTL í•´ì‹œ (ëª¨ë“  RTL íŒŒì¼ì˜ ì¡°í•©)
 197:         rtl_hash = ""
 198:         if self.rtl_files:
 199:             combined = "".join(compute_file_hash(f) for f in self.rtl_files)
 200:             rtl_hash = hashlib.sha256(combined.encode()).hexdigest()[:16]
 201:         
 202:         # Constraint í•´ì‹œ
 203:         constraint_hash = None
 204:         if self.constraint_files:
 205:             combined = "".join(compute_file_hash(f) for f in self.constraint_files)
 206:             constraint_hash = hashlib.sha256(combined.encode()).hexdigest()[:16]
 207:         
 208:         # Timing í•´ì‹œ
 209:         timing_hash = None
 210:         if self.timing_files:
 211:             combined = "".join(compute_file_hash(f) for f in self.timing_files)
 212:             timing_hash = hashlib.sha256(combined.encode()).hexdigest()[:16]
 213:         
 214:         # ì •ì±… ë²„ì „ (í–¥í›„ í™•ì¥)
 215:         policy_versions = {}
 216:         
 217:         return GraphVersion(
 218:             rtl_hash=rtl_hash,
 219:             constraint_hash=constraint_hash,
 220:             timing_hash=timing_hash,
 221:             policy_versions=policy_versions,
 222:         )
 223:     
 224:     def save_cache(self, filepath: str | Path, indent: Optional[int] = None) -> None:
 225:         """í˜„ì¬ ê·¸ë˜í”„ë¥¼ ìºì‹œ íŒŒì¼ë¡œ ì €ì¥"""
 226:         if self.nodes is None or self.edges is None:
 227:             raise RuntimeError("No graph available. Run RTL stage first.")
 228:         
 229:         version = self.compute_version()
 230:         snapshot = GraphSnapshot(
 231:             version=version,
 232:             dkg_nodes=self.nodes,
 233:             dkg_edges=self.edges,
 234:             supergraph=self.supergraph,
 235:         )
 236:         save_snapshot(snapshot, filepath, indent=indent)
 237:     
 238:     @classmethod
 239:     def load_from_cache(cls, filepath: str | Path, yosys_config: Optional[YosysConfig] = None) -> "DKGPipeline":
 240:         """ìºì‹œ íŒŒì¼ì—ì„œ íŒŒì´í”„ë¼ì¸ ë³µì›"""
 241:         snapshot = load_snapshot(filepath)
 242:         
 243:         # ë”ë¯¸ config (ìºì‹œ ë¡œë”© ì‹œì—ëŠ” í•„ìš” ì—†ìŒ)
 244:         if yosys_config is None:
 245:             yosys_config = YosysConfig(src_dir_win="", out_json_win="", top_module="")
 246:         
 247:         pipeline = cls(yosys_config)
 248:         pipeline.nodes = snapshot.dkg_nodes
 249:         pipeline.edges = snapshot.dkg_edges
 250:         pipeline.supergraph = snapshot.supergraph
 251:         
 252:         # ë©”íƒ€ë°ì´í„°ëŠ” ì¬ìƒì„±í•˜ì§€ ì•ŠìŒ (ì½ê¸° ì „ìš© ëª¨ë“œ)
 253:         pipeline.updater = None
 254:         
 255:         return pipeline
 256: 
 257:     def build_supergraph(self, view: GraphViewType = GraphViewType.Connectivity) -> None:
 258: 
 259:             if self.nodes is None or self.edges is None:
 260:                 raise RuntimeError("Run RTL stage first.")
 261: 
 262:             context = GraphContext.DESIGN
 263:             
 264: 
 265:             for node in self.nodes.values():
 266:                 if node.attributes.get("design_context") == "sim":
 267:                     context = GraphContext.SIMULATION
 268:                     print(f"â„¹ï¸ Simulation context detected from node attributes.")
 269:                     break
 270:             
 271:             print(f"ğŸ—ï¸ Building SuperGraph (View: {view.value}, Context: {context.value})...")
 272:             
 273:             view_builder = ViewBuilder(
 274:                 self.nodes, 
 275:                 self.edges, 
 276:                 view, 
 277:                 context=context  
 278:             )
 279:             
 280:             self.supergraph = view_builder.build()
 281:             print(f"âœ… SuperGraph built: {len(self.supergraph.super_nodes)} nodes, {len(self.supergraph.super_edges)} edges.")


FILE: dkg\pipeline\stages.py

   1: from __future__ import annotations
   2: 
   3: from enum import Enum
   4: from typing import FrozenSet
   5: 
   6: 
   7: class ParsingStage(str, Enum):
   8:     """íŒŒì‹± ë‹¨ê³„ ì •ì˜. ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ë¨."""
   9:     
  10:     RTL = "rtl"                    # Yosys JSON (êµ¬ì¡° ì •ë³´)
  11:     SYNTHESIS = "synthesis"        # í•©ì„± í›„ netlist
  12:     CONSTRAINTS = "constraints"    # SDC/XDC (íƒ€ì´ë°/í´ëŸ­ ì œì•½)
  13:     FLOORPLAN = "floorplan"        # TCL/Pblock (ë¬¼ë¦¬ì  ë°°ì¹˜)
  14:     TIMING = "timing"              # íƒ€ì´ë° ë¦¬í¬íŠ¸
  15:     BOARD = "board"                # BD/board constraints
  16: 
  17: 
  18: class FieldSource(str, Enum):
  19:     """í•„ë“œ ê°’ì˜ ì¶œì²˜"""
  20:     
  21:     INFERRED = "inferred"          # íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì¶”ë¡ 
  22:     DECLARED = "declared"          # ëª…ì‹œì ìœ¼ë¡œ ì„ ì–¸ë¨
  23:     ANALYZED = "analyzed"          # ë„êµ¬ ë¶„ì„ ê²°ê³¼
  24:     USER_OVERRIDE = "user_override"  # ì‚¬ìš©ì ì§ì ‘ ì„¤ì •
  25: 
  26: 
  27: # ê° í•„ë“œê°€ ì–´ëŠ stageì—ì„œ í™•ì •ë˜ëŠ”ì§€ ì •ì˜
  28: FIELD_STAGES: dict[str, FrozenSet[ParsingStage]] = {
  29:     # Node fields
  30:     "entity_class": frozenset([ParsingStage.RTL, ParsingStage.SYNTHESIS]),
  31:     "hier_path": frozenset([ParsingStage.RTL, ParsingStage.SYNTHESIS]),
  32:     "clock_domain": frozenset([ParsingStage.RTL, ParsingStage.CONSTRAINTS]),
  33:     "arrival_time": frozenset([ParsingStage.TIMING]),
  34:     "required_time": frozenset([ParsingStage.TIMING]),
  35:     "slack": frozenset([ParsingStage.TIMING]),
  36:     
  37:     # Edge fields
  38:     "signal_name": frozenset([ParsingStage.RTL, ParsingStage.SYNTHESIS]),
  39:     "flow_type": frozenset([ParsingStage.RTL, ParsingStage.CONSTRAINTS]),
  40:     "clock_signal": frozenset([ParsingStage.CONSTRAINTS]),
  41:     "reset_signal": frozenset([ParsingStage.CONSTRAINTS]),
  42:     "delay": frozenset([ParsingStage.TIMING]),
  43:     "timing_exception": frozenset([ParsingStage.CONSTRAINTS]),
  44: }
  45: 
  46: 
  47: def get_priority(source: FieldSource) -> int:
  48:     """ê°’ ìš°ì„ ìˆœìœ„. ë†’ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ."""
  49:     priorities = {
  50:         FieldSource.INFERRED: 1,
  51:         FieldSource.DECLARED: 3,
  52:         FieldSource.ANALYZED: 2,
  53:         FieldSource.USER_OVERRIDE: 4,
  54:     }
  55:     return priorities[source]
  56: 
  57: 
  58: def should_update_field(
  59:     current_source: FieldSource | None,
  60:     new_source: FieldSource,
  61: ) -> bool:
  62:     """ê¸°ì¡´ ê°’ì„ ìƒˆ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì •"""
  63:     if current_source is None:
  64:         return True
  65:     return get_priority(new_source) >= get_priority(current_source)



FILE: dkg\timing\timing_aggregator.py

   1: """
   2: Timing Aggregator: DKG â†’ SuperGraph Timing Metrics ì§‘ê³„
   3: 
   4: ì´ ëª¨ë“ˆì€ DKG ë…¸ë“œ/ì—£ì§€ì˜ raw timing ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ì—¬
   5: SuperNode/SuperEdgeì— ë¶€ì°©í•  TimingNodeMetrics/TimingEdgeMetricsë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
   6: 
   7: ì›ì¹™:
   8: - ì§‘ê³„ë§Œ ìˆ˜í–‰, êµ¬ì¡° ë³€ê²½ ì—†ìŒ
   9: - Immutable snapshot ìƒì„±
  10: - ìµœì•…ê°’(worst-case) ë° percentile í†µê³„ ê³„ì‚°
  11: """
  12: from __future__ import annotations
  13: 
  14: from typing import Dict, List, Optional
  15: 
  16: from ..core.graph import DKGEdge, DKGNode, EdgeFlowType
  17: from ..builders.supergraph import (
  18:     SuperEdge,
  19:     SuperGraph,
  20:     SuperNode,
  21:     TimingAlert,
  22:     TimingAlertSeverity,
  23:     TimingEdgeMetrics,
  24:     TimingNodeMetrics,
  25:     TimingSummary,
  26:     attach_timing_analysis_to_superedge,
  27:     attach_timing_analysis_to_supernode,
  28:     get_timing_analysis_from_supernode,
  29: )
  30: 
  31: 
  32: def percentile(values: List[float], p: float) -> float:
  33:     """
  34:     ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚° (0 <= p <= 1)
  35:     
  36:     Args:
  37:         values: ì •ë ¬ë˜ì§€ ì•Šì€ ê°’ ë¦¬ìŠ¤íŠ¸
  38:         p: ë°±ë¶„ìœ„ìˆ˜ (0.05 = 5th percentile, 0.95 = 95th percentile)
  39:     
  40:     Returns:
  41:         ê³„ì‚°ëœ ë°±ë¶„ìœ„ìˆ˜ ê°’
  42:     """
  43:     if not values:
  44:         return 0.0
  45:     
  46:     sorted_values = sorted(values)
  47:     n = len(sorted_values)
  48:     
  49:     if n == 1:
  50:         return sorted_values[0]
  51:     
  52:     # Linear interpolation between closest ranks
  53:     rank = p * (n - 1)
  54:     lower = int(rank)
  55:     upper = min(lower + 1, n - 1)
  56:     weight = rank - lower
  57:     
  58:     return sorted_values[lower] * (1 - weight) + sorted_values[upper] * weight
  59: 
  60: 
  61: def compute_timing_node_metrics(
  62:     supernode: SuperNode,
  63:     nodes: Dict[str, DKGNode],
  64:     clock_period: float = 10.0,  # ê¸°ë³¸ í´ëŸ­ ì£¼ê¸° (ns)
  65:     critical_threshold: float = 0.0,  # slack < 0ì´ë©´ critical
  66:     near_critical_alpha: float = 0.1,  # clock_periodì˜ 10% ì´ë‚´ë©´ near-critical
  67: ) -> Optional[TimingNodeMetrics]:
  68:     """
  69:     SuperNodeì˜ member ë…¸ë“œë“¤ë¡œë¶€í„° TimingNodeMetricsë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
  70:     
  71:     Args:
  72:         supernode: íƒ€ì´ë° ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  SuperNode
  73:         nodes: ì „ì²´ DKG ë…¸ë“œ ë”•ì…”ë„ˆë¦¬
  74:         clock_period: í´ëŸ­ ì£¼ê¸° (ns)
  75:         critical_threshold: critical íŒì • ì„ê³„ê°’
  76:         near_critical_alpha: near-critical íŒì • ë°°ìœ¨
  77:     
  78:     Returns:
  79:         TimingNodeMetrics ë˜ëŠ” None (íƒ€ì´ë° ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°)
  80:     """
  81:     # Member ë…¸ë“œë“¤ì˜ íƒ€ì´ë° ì •ë³´ ìˆ˜ì§‘
  82:     slack_values: List[float] = []
  83:     arrival_times: List[float] = []
  84:     required_times: List[float] = []
  85:     
  86:     for node_id in supernode.member_nodes:
  87:         node = nodes.get(node_id)
  88:         if not node:
  89:             continue
  90:         
  91:         if node.slack is not None:
  92:             slack_values.append(node.slack)
  93:         
  94:         if node.arrival_time is not None:
  95:             arrival_times.append(node.arrival_time)
  96:         
  97:         if node.required_time is not None:
  98:             required_times.append(node.required_time)
  99:     
 100:     # íƒ€ì´ë° ì •ë³´ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ None ë°˜í™˜
 101:     if not slack_values and not arrival_times and not required_times:
 102:         return None
 103:     
 104:     # Slack í†µê³„ ê³„ì‚°
 105:     min_slack = min(slack_values) if slack_values else 0.0
 106:     p5_slack = percentile(slack_values, 0.05) if len(slack_values) >= 2 else min_slack
 107:     
 108:     # Arrival/Required Time í†µê³„
 109:     max_arrival_time = max(arrival_times) if arrival_times else 0.0
 110:     min_required_time = min(required_times) if required_times else 0.0
 111:     
 112:     # Critical/Near-Critical ë¹„ìœ¨ ê³„ì‚°
 113:     total_nodes = len(slack_values)
 114:     critical_count = sum(1 for s in slack_values if s < critical_threshold)
 115:     near_critical_count = sum(
 116:         1 for s in slack_values if s < near_critical_alpha * clock_period
 117:     )
 118:     
 119:     critical_node_ratio = critical_count / total_nodes if total_nodes > 0 else 0.0
 120:     near_critical_ratio = near_critical_count / total_nodes if total_nodes > 0 else 0.0
 121:     
 122:     # Timing Risk Score ê³„ì‚° (ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±)
 123:     # ìŒìˆ˜ slackì´ ë§ì„ìˆ˜ë¡, slackì´ ì‘ì„ìˆ˜ë¡ ìœ„í—˜ë„ ì¦ê°€
 124:     timing_risk_score = None
 125:     if slack_values:
 126:         # Risk = critical_ratio * 10 + (1 - normalized_min_slack) * 5
 127:         normalized_min_slack = max(0, min(1, (min_slack + clock_period) / clock_period))
 128:         timing_risk_score = (
 129:             critical_node_ratio * 10.0 + (1 - normalized_min_slack) * 5.0
 130:         )
 131:     
 132:     return TimingNodeMetrics(
 133:         min_slack=min_slack,
 134:         p5_slack=p5_slack,
 135:         max_arrival_time=max_arrival_time,
 136:         min_required_time=min_required_time,
 137:         critical_node_ratio=critical_node_ratio,
 138:         near_critical_ratio=near_critical_ratio,
 139:         timing_risk_score=timing_risk_score,
 140:     )
 141: 
 142: 
 143: def compute_timing_edge_metrics(
 144:     superedge: SuperEdge,
 145:     edges: Dict[str, DKGEdge],
 146: ) -> Optional[TimingEdgeMetrics]:
 147:     """
 148:     SuperEdgeì˜ member ì—£ì§€ë“¤ë¡œë¶€í„° TimingEdgeMetricsë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
 149:     
 150:     Args:
 151:         superedge: íƒ€ì´ë° ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  SuperEdge
 152:         edges: ì „ì²´ DKG ì—£ì§€ ë”•ì…”ë„ˆë¦¬
 153:     
 154:     Returns:
 155:         TimingEdgeMetrics ë˜ëŠ” None (íƒ€ì´ë° ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°)
 156:     """
 157:     # Member ì—£ì§€ë“¤ì˜ íƒ€ì´ë° ì •ë³´ ìˆ˜ì§‘
 158:     delay_values: List[float] = []
 159:     flow_type_counts: Dict[EdgeFlowType, int] = {}
 160:     fanout_values: List[int] = []
 161:     
 162:     for edge_id in superedge.member_edges:
 163:         edge = edges.get(edge_id)
 164:         if not edge:
 165:             continue
 166:         
 167:         if edge.delay is not None:
 168:             delay_values.append(edge.delay)
 169:         
 170:         # Flow type íˆìŠ¤í† ê·¸ë¨
 171:         flow_type_counts[edge.flow_type] = flow_type_counts.get(edge.flow_type, 0) + 1
 172:         
 173:         if edge.fanout_count is not None:
 174:             fanout_values.append(edge.fanout_count)
 175:     
 176:     # íƒ€ì´ë° ì •ë³´ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ None ë°˜í™˜
 177:     if not delay_values and not flow_type_counts:
 178:         return None
 179:     
 180:     # Delay í†µê³„ ê³„ì‚°
 181:     max_delay = max(delay_values) if delay_values else 0.0
 182:     p95_delay = percentile(delay_values, 0.95) if len(delay_values) >= 2 else max_delay
 183:     
 184:     # Fanout í†µê³„ (ì„ íƒì )
 185:     fanout_max = max(fanout_values) if fanout_values else None
 186:     fanout_p95 = (
 187:         percentile([float(f) for f in fanout_values], 0.95)
 188:         if len(fanout_values) >= 2
 189:         else (float(fanout_max) if fanout_max is not None else None)
 190:     )
 191:     
 192:     return TimingEdgeMetrics(
 193:         max_delay=max_delay,
 194:         p95_delay=p95_delay,
 195:         flow_type_histogram=flow_type_counts,
 196:         fanout_max=fanout_max,
 197:         fanout_p95=fanout_p95,
 198:     )
 199: 
 200: 
 201: def aggregate_timing_to_supergraph(
 202:     supergraph: SuperGraph,
 203:     nodes: Dict[str, DKGNode],
 204:     edges: Dict[str, DKGEdge],
 205:     clock_period: float = 10.0,
 206:     critical_threshold: float = 0.0,
 207:     near_critical_alpha: float = 0.1,
 208: ) -> None:
 209:     """
 210:     SuperGraphì˜ ëª¨ë“  SuperNode/SuperEdgeì— íƒ€ì´ë° ë©”íŠ¸ë¦­ì„ ì§‘ê³„í•˜ê³  ë¶€ì°©í•©ë‹ˆë‹¤.
 211:     
 212:     Args:
 213:         supergraph: íƒ€ì´ë° ë©”íŠ¸ë¦­ì„ ë¶€ì°©í•  SuperGraph
 214:         nodes: DKG ë…¸ë“œ ë”•ì…”ë„ˆë¦¬
 215:         edges: DKG ì—£ì§€ ë”•ì…”ë„ˆë¦¬
 216:         clock_period: í´ëŸ­ ì£¼ê¸° (ns)
 217:         critical_threshold: critical íŒì • ì„ê³„ê°’
 218:         near_critical_alpha: near-critical íŒì • ë°°ìœ¨
 219:     """
 220:     # SuperNodeì— íƒ€ì´ë° ë©”íŠ¸ë¦­ ë¶€ì°©
 221:     for sn in supergraph.super_nodes.values():
 222:         metrics = compute_timing_node_metrics(
 223:             sn, nodes, clock_period, critical_threshold, near_critical_alpha
 224:         )
 225:         if metrics:
 226:             attach_timing_analysis_to_supernode(sn, metrics)
 227:     
 228:     # SuperEdgeì— íƒ€ì´ë° ë©”íŠ¸ë¦­ ë¶€ì°©
 229:     for se in supergraph.super_edges.values():
 230:         metrics = compute_timing_edge_metrics(se, edges)
 231:         if metrics:
 232:             attach_timing_analysis_to_superedge(se, metrics)
 233: 
 234: 
 235: def compute_timing_summary(
 236:     nodes: Dict[str, DKGNode],
 237:     clock_period: float = 10.0,
 238:     analysis_mode: str = "setup",
 239: ) -> TimingSummary:
 240:     """
 241:     ì „ì²´ ê·¸ë˜í”„ì˜ Timing ìš”ì•½ ì •ë³´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
 242:     
 243:     Args:
 244:         nodes: DKG ë…¸ë“œ ë”•ì…”ë„ˆë¦¬
 245:         clock_period: í´ëŸ­ ì£¼ê¸° (ns)
 246:         analysis_mode: "setup" / "hold" / "both"
 247:     
 248:     Returns:
 249:         TimingSummary ê°ì²´
 250:     """
 251:     from datetime import datetime
 252:     
 253:     slack_values = [n.slack for n in nodes.values() if n.slack is not None]
 254:     
 255:     worst_slack = min(slack_values) if slack_values else 0.0
 256:     violation_count = sum(1 for s in slack_values if s < 0)
 257:     near_critical_count = sum(1 for s in slack_values if 0 <= s < 0.1 * clock_period)
 258:     
 259:     return TimingSummary(
 260:         worst_slack=worst_slack,
 261:         violation_count=violation_count,
 262:         near_critical_count=near_critical_count,
 263:         clock_period=clock_period,
 264:         analysis_mode=analysis_mode,
 265:         timestamp=datetime.now().isoformat(),
 266:     )
 267: 
 268: 
 269: def generate_timing_alerts(
 270:     supergraph: SuperGraph,
 271:     nodes: Dict[str, DKGNode],
 272:     critical_threshold: float = 0.0,
 273:     warn_threshold: float = 0.5,
 274: ) -> List[TimingAlert]:
 275:     """
 276:     SuperGraphì—ì„œ íƒ€ì´ë° ë¬¸ì œë¥¼ ì°¾ì•„ Alertë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
 277:     
 278:     Args:
 279:         supergraph: ë¶„ì„í•  SuperGraph
 280:         nodes: DKG ë…¸ë“œ ë”•ì…”ë„ˆë¦¬
 281:         critical_threshold: ERROR ë ˆë²¨ ì„ê³„ê°’
 282:         warn_threshold: WARN ë ˆë²¨ ì„ê³„ê°’
 283:     
 284:     Returns:
 285:         TimingAlert ë¦¬ìŠ¤íŠ¸
 286:     """
 287:     alerts: List[TimingAlert] = []
 288:     
 289:     for sn in supergraph.super_nodes.values():
 290:         metrics = get_timing_analysis_from_supernode(sn)
 291:         if not metrics:
 292:             continue
 293:         
 294:         # Slack violation ì²´í¬
 295:         if metrics.min_slack < critical_threshold:
 296:             alerts.append(
 297:                 TimingAlert(
 298:                     entity_ref=sn.node_id,
 299:                     entity_type="supernode",
 300:                     severity=TimingAlertSeverity.ERROR,
 301:                     reason=f"Timing violation: min_slack={metrics.min_slack:.3f}ns",
 302:                     metrics_snapshot={
 303:                         "min_slack": metrics.min_slack,
 304:                         "p5_slack": metrics.p5_slack,
 305:                         "max_arrival_time": metrics.max_arrival_time,
 306:                     },
 307:                 )
 308:             )
 309:         elif metrics.min_slack < warn_threshold:
 310:             alerts.append(
 311:                 TimingAlert(
 312:                     entity_ref=sn.node_id,
 313:                     entity_type="supernode",
 314:                     severity=TimingAlertSeverity.WARN,
 315:                     reason=f"Near-critical path: min_slack={metrics.min_slack:.3f}ns",
 316:                     metrics_snapshot={
 317:                         "min_slack": metrics.min_slack,
 318:                         "critical_node_ratio": metrics.critical_node_ratio,
 319:                     },
 320:                 )
 321:             )
 322:         
 323:         # High timing risk ì²´í¬
 324:         if metrics.timing_risk_score and metrics.timing_risk_score > 10.0:
 325:             alerts.append(
 326:                 TimingAlert(
 327:                     entity_ref=sn.node_id,
 328:                     entity_type="supernode",
 329:                     severity=TimingAlertSeverity.WARN,
 330:                     reason=f"High timing risk: score={metrics.timing_risk_score:.2f}",
 331:                     metrics_snapshot={
 332:                         "timing_risk_score": metrics.timing_risk_score,
 333:                         "critical_node_ratio": metrics.critical_node_ratio,
 334:                         "near_critical_ratio": metrics.near_critical_ratio,
 335:                     },
 336:                 )
 337:             )
 338:     
 339:     return alerts



FILE: dkg\timing\timing_analysis_example.py

   1: """
   2: Timing Analysis ì‚¬ìš© ì˜ˆì œ
   3: 
   4: ì´ ì˜ˆì œëŠ” ì „ì²´ Timing ë¶„ì„ íŒŒì´í”„ë¼ì¸ì˜ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:
   5: 1. DKG ê·¸ë˜í”„ ìƒì„±
   6: 2. Timing Report íŒŒì‹±
   7: 3. Constraint íŒŒì¼ ì²˜ë¦¬
   8: 4. SuperGraph ìƒì„±
   9: 5. Timing Metrics ì§‘ê³„ ë° ë¶€ì°©
  10: 6. ê²°ê³¼ ì¡°íšŒ ë° ì¶œë ¥
  11: """
  12: import sys
  13: from pathlib import Path
  14: 
  15: # Add parent directory to path
  16: sys.path.insert(0, str(Path(__file__).parent.parent))
  17: 
  18: from dkg.builders.constraint_projector import (
  19:     ClockConstraint,
  20:     ConstraintProjector,
  21:     FalsePathConstraint,
  22: )
  23: from dkg.core.graph import DKGEdge, DKGNode, EdgeFlowType, EntityClass, RelationType
  24: from dkg.builders.graph_updater import GraphUpdater
  25: from dkg.builders.supergraph import (
  26:     GraphViewType,
  27:     SuperGraph,
  28:     ViewBuilder,
  29:     get_timing_analysis_from_superedge,
  30:     get_timing_analysis_from_supernode,
  31: )
  32: from dkg.timing.timing_aggregator import aggregate_timing_to_supergraph
  33: from dkg.timing.timing_integration import TimingAnalysisPipeline, quick_timing_analysis
  34: 
  35: 
  36: def example_basic_timing_analysis():
  37:     """ê¸°ë³¸ íƒ€ì´ë° ë¶„ì„ ì˜ˆì œ"""
  38:     print("=" * 80)
  39:     print("Example 1: Basic Timing Analysis")
  40:     print("=" * 80)
  41: 
  42:     # 1. ê°„ë‹¨í•œ DKG ê·¸ë˜í”„ ìƒì„±
  43:     nodes = {
  44:         "n1": DKGNode(
  45:             node_id="n1",
  46:             entity_class=EntityClass.FLIP_FLOP,
  47:             hier_path="cpu/pc_reg",
  48:             local_name="pc_reg",
  49:             slack=1.5,
  50:             arrival_time=8.5,
  51:             required_time=10.0,
  52:         ),
  53:         "n2": DKGNode(
  54:             node_id="n2",
  55:             entity_class=EntityClass.LUT,
  56:             hier_path="cpu/alu/add",
  57:             local_name="add",
  58:             slack=0.2,
  59:             arrival_time=9.8,
  60:             required_time=10.0,
  61:         ),
  62:         "n3": DKGNode(
  63:             node_id="n3",
  64:             entity_class=EntityClass.FLIP_FLOP,
  65:             hier_path="cpu/result_reg",
  66:             local_name="result_reg",
  67:             slack=-0.5,  # Timing violation!
  68:             arrival_time=10.5,
  69:             required_time=10.0,
  70:         ),
  71:     }
  72: 
  73:     edges = {
  74:         "e1": DKGEdge(
  75:             edge_id="e1",
  76:             src_node="n1",
  77:             dst_node="n2",
  78:             relation_type=RelationType.DATA,
  79:             flow_type=EdgeFlowType.COMBINATIONAL,
  80:             signal_name="pc_out",
  81:             canonical_name="pc_reg â†’ add",
  82:             delay=1.3,
  83:         ),
  84:         "e2": DKGEdge(
  85:             edge_id="e2",
  86:             src_node="n2",
  87:             dst_node="n3",
  88:             relation_type=RelationType.DATA,
  89:             flow_type=EdgeFlowType.SEQ_CAPTURE,
  90:             signal_name="result",
  91:             canonical_name="add â†’ result_reg",
  92:             delay=0.7,
  93:         ),
  94:     }
  95: 
  96:     # GraphUpdater ì´ˆê¸°í™”
  97:     updater = GraphUpdater(nodes, edges)
  98: 
  99:     # 2. SuperGraph ìƒì„±
 100:     view_builder = ViewBuilder(nodes, edges, GraphViewType.Connectivity)
 101:     supergraph = view_builder.build()
 102: 
 103:     print(f"\nSuperGraph created:")
 104:     print(f"  SuperNodes: {len(supergraph.super_nodes)}")
 105:     print(f"  SuperEdges: {len(supergraph.super_edges)}")
 106: 
 107:     # 3. Timing Metrics ì§‘ê³„ ë° ë¶€ì°©
 108:     aggregate_timing_to_supergraph(
 109:         supergraph,
 110:         nodes,
 111:         edges,
 112:         clock_period=10.0,
 113:         critical_threshold=0.0,
 114:         near_critical_alpha=0.1,
 115:     )
 116: 
 117:     # 4. ê²°ê³¼ ì¡°íšŒ
 118:     print("\n[SuperNode Timing Metrics]")
 119:     for sn_id, sn in supergraph.super_nodes.items():
 120:         metrics = get_timing_analysis_from_supernode(sn)
 121:         if metrics:
 122:             print(f"\n  {sn_id} ({sn.super_class.value}):")
 123:             print(f"    Min Slack:            {metrics.min_slack:.3f} ns")
 124:             print(f"    P5 Slack:             {metrics.p5_slack:.3f} ns")
 125:             print(f"    Max Arrival Time:     {metrics.max_arrival_time:.3f} ns")
 126:             print(f"    Critical Node Ratio:  {metrics.critical_node_ratio:.2%}")
 127:             print(f"    Timing Risk Score:    {metrics.timing_risk_score:.2f}")
 128: 
 129:     print("\n[SuperEdge Timing Metrics]")
 130:     for (src, dst), se in supergraph.super_edges.items():
 131:         metrics = get_timing_analysis_from_superedge(se)
 132:         if metrics:
 133:             print(f"\n  {src} â†’ {dst}:")
 134:             print(f"    Max Delay:     {metrics.max_delay:.3f} ns")
 135:             print(f"    P95 Delay:     {metrics.p95_delay:.3f} ns")
 136:             print(f"    Flow Types:    {dict(metrics.flow_type_histogram)}")
 137: 
 138: 
 139: def example_constraint_projection():
 140:     """Constraint íˆ¬ì˜ ì˜ˆì œ"""
 141:     print("\n\n" + "=" * 80)
 142:     print("Example 2: Constraint Projection")
 143:     print("=" * 80)
 144: 
 145:     # 1. DKG ê·¸ë˜í”„ ìƒì„±
 146:     nodes = {
 147:         "clk_port": DKGNode(
 148:             node_id="clk_port",
 149:             entity_class=EntityClass.IO_PORT,
 150:             hier_path="clk",
 151:             local_name="clk",
 152:         ),
 153:         "reset_ff": DKGNode(
 154:             node_id="reset_ff",
 155:             entity_class=EntityClass.FLIP_FLOP,
 156:             hier_path="system/reset_reg",
 157:             local_name="reset_reg",
 158:         ),
 159:         "data_ff": DKGNode(
 160:             node_id="data_ff",
 161:             entity_class=EntityClass.FLIP_FLOP,
 162:             hier_path="system/data_reg",
 163:             local_name="data_reg",
 164:         ),
 165:     }
 166: 
 167:     edges = {
 168:         "e_reset": DKGEdge(
 169:             edge_id="e_reset",
 170:             src_node="reset_ff",
 171:             dst_node="data_ff",
 172:             relation_type=RelationType.RESET,
 173:             flow_type=EdgeFlowType.ASYNC_RESET,
 174:             signal_name="reset",
 175:             canonical_name="reset_reg â†’ data_reg",
 176:         ),
 177:     }
 178: 
 179:     updater = GraphUpdater(nodes, edges)
 180: 
 181:     # 2. Constraint Projector ìƒì„±
 182:     projector = ConstraintProjector(nodes, edges, updater)
 183: 
 184:     # 3. Clock Constraint íˆ¬ì˜
 185:     print("\n[Clock Constraint]")
 186:     clock_constraint = ClockConstraint(
 187:         clock_name="sys_clk", period=10.0, target_ports=["clk"]
 188:     )
 189:     projector.project_clock_constraint(clock_constraint, "example.sdc", 1)
 190: 
 191:     clk_node = nodes["clk_port"]
 192:     print(f"  Clock Domain: {clk_node.clock_domain}")
 193:     print(f"  Clock Period: {clk_node.attributes.get('clock_period', 'N/A')} ns")
 194: 
 195:     # 4. False Path Constraint íˆ¬ì˜
 196:     print("\n[False Path Constraint]")
 197:     false_path = FalsePathConstraint(
 198:         from_targets=["system/reset_reg"], to_targets=["system/data_reg"]
 199:     )
 200:     projector.project_false_path_constraint(false_path, "example.sdc", 5)
 201: 
 202:     reset_edge = edges["e_reset"]
 203:     print(f"  Timing Exception: {reset_edge.timing_exception}")
 204: 
 205: 
 206: def example_full_pipeline():
 207:     """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì˜ˆì œ"""
 208:     print("\n\n" + "=" * 80)
 209:     print("Example 3: Full Timing Analysis Pipeline")
 210:     print("=" * 80)
 211: 
 212:     # 1. DKG ê·¸ë˜í”„ ìƒì„±
 213:     nodes = {
 214:         "ff1": DKGNode(
 215:             node_id="ff1",
 216:             entity_class=EntityClass.FLIP_FLOP,
 217:             hier_path="cpu/stage1/ff",
 218:             local_name="ff1",
 219:         ),
 220:         "lut1": DKGNode(
 221:             node_id="lut1",
 222:             entity_class=EntityClass.LUT,
 223:             hier_path="cpu/stage2/lut",
 224:             local_name="lut1",
 225:         ),
 226:         "ff2": DKGNode(
 227:             node_id="ff2",
 228:             entity_class=EntityClass.FLIP_FLOP,
 229:             hier_path="cpu/stage3/ff",
 230:             local_name="ff2",
 231:         ),
 232:     }
 233: 
 234:     edges = {
 235:         "e1": DKGEdge(
 236:             edge_id="e1",
 237:             src_node="ff1",
 238:             dst_node="lut1",
 239:             relation_type=RelationType.DATA,
 240:             flow_type=EdgeFlowType.COMBINATIONAL,
 241:             signal_name="data1",
 242:             canonical_name="ff1 â†’ lut1",
 243:         ),
 244:         "e2": DKGEdge(
 245:             edge_id="e2",
 246:             src_node="lut1",
 247:             dst_node="ff2",
 248:             relation_type=RelationType.DATA,
 249:             flow_type=EdgeFlowType.SEQ_CAPTURE,
 250:             signal_name="data2",
 251:             canonical_name="lut1 â†’ ff2",
 252:         ),
 253:     }
 254: 
 255:     updater = GraphUpdater(nodes, edges)
 256: 
 257:     # 2. TimingAnalysisPipeline ìƒì„±
 258:     pipeline = TimingAnalysisPipeline(nodes, edges, updater)
 259: 
 260:     print("\n[Pipeline Created]")
 261: 
 262:     # 3. SuperGraph ìƒì„±
 263:     view_builder = ViewBuilder(nodes, edges, GraphViewType.Connectivity)
 264:     supergraph = view_builder.build()
 265: 
 266:     print(f"  SuperNodes: {len(supergraph.super_nodes)}")
 267:     print(f"  SuperEdges: {len(supergraph.super_edges)}")
 268: 
 269:     # 4. Timing Metrics ë¶€ì°©
 270:     # (ì‹¤ì œë¡œëŠ” timing reportë¥¼ íŒŒì‹±í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ë…¸ë“œì— ë°ì´í„°ê°€ ìˆë‹¤ê³  ê°€ì •)
 271:     pipeline.attach_timing_to_supergraph(supergraph, clock_period=10.0)
 272: 
 273:     # 5. Timing Summary ì¶œë ¥
 274:     pipeline.print_timing_report(supergraph, clock_period=10.0)
 275: 
 276: 
 277: def example_quick_api():
 278:     """Quick Start API ì˜ˆì œ"""
 279:     print("\n\n" + "=" * 80)
 280:     print("Example 4: Quick Timing Analysis API")
 281:     print("=" * 80)
 282: 
 283:     # ê°„ë‹¨í•œ ê·¸ë˜í”„ ìƒì„±
 284:     nodes = {
 285:         "n1": DKGNode(
 286:             node_id="n1",
 287:             entity_class=EntityClass.FLIP_FLOP,
 288:             hier_path="top/ff1",
 289:             local_name="ff1",
 290:             slack=2.0,
 291:         ),
 292:     }
 293: 
 294:     edges = {}
 295:     updater = GraphUpdater(nodes, edges)
 296: 
 297:     view_builder = ViewBuilder(nodes, edges, GraphViewType.Connectivity)
 298:     supergraph = view_builder.build()
 299: 
 300:     # Quick API ì‚¬ìš©
 301:     # summary = quick_timing_analysis(
 302:     #     nodes, edges, updater, supergraph,
 303:     #     timing_report_path="design.timing_rpt",  # ì‹¤ì œ íŒŒì¼ì´ ìˆì–´ì•¼ í•¨
 304:     #     constraint_path="design.sdc",            # ì‹¤ì œ íŒŒì¼ì´ ìˆì–´ì•¼ í•¨
 305:     #     clock_period=10.0
 306:     # )
 307: 
 308:     print("\n[Quick API Usage]")
 309:     print("  quick_timing_analysis() can process:")
 310:     print("    - Timing reports (Vivado/PrimeTime)")
 311:     print("    - Constraint files (SDC/XDC)")
 312:     print("    - Automatic metrics aggregation")
 313:     print("    - Formatted report output")
 314: 
 315: 
 316: if __name__ == "__main__":
 317:     example_basic_timing_analysis()
 318:     example_constraint_projection()
 319:     example_full_pipeline()
 320:     example_quick_api()
 321: 
 322:     print("\n\n" + "=" * 80)
 323:     print("All examples completed!")
 324:     print("=" * 80)



FILE: dkg\timing\timing_integration.py

   1: """
   2: Timing Integration: ì „ì²´ Timing ë¶„ì„ íŒŒì´í”„ë¼ì¸ í†µí•©
   3: 
   4: ì´ ëª¨ë“ˆì€ ë‹¤ìŒ ë‹¨ê³„ë“¤ì„ í†µí•©í•©ë‹ˆë‹¤:
   5: 1. Timing Report íŒŒì‹± â†’ DKG ë…¸ë“œ/ì—£ì§€ì— raw timing ì €ì¥
   6: 2. Constraint íŒŒì‹± â†’ DKG ê·¸ë˜í”„ì— ì œì•½ íˆ¬ì˜
   7: 3. SuperGraph ìƒì„±
   8: 4. Timing Metrics ì§‘ê³„ â†’ SuperNode/SuperEdgeì— ë¶€ì°©
   9: 5. Timing Summary ë° Alert ìƒì„±
  10: 
  11: ì‚¬ìš© ì˜ˆì‹œ:
  12:     ```python
  13:     from dkg.timing_integration import TimingAnalysisPipeline
  14:     
  15:     pipeline = TimingAnalysisPipeline(nodes, edges, updater)
  16:     
  17:     # 1. Timing Report ì²˜ë¦¬
  18:     pipeline.process_timing_report("design.timing_rpt")
  19:     
  20:     # 2. ì œì•½ íŒŒì¼ ì²˜ë¦¬
  21:     pipeline.process_constraint_file("design.sdc")
  22:     
  23:     # 3. SuperGraphì— Timing Metrics ë¶€ì°©
  24:     supergraph = build_supergraph(...)
  25:     pipeline.attach_timing_to_supergraph(supergraph, clock_period=10.0)
  26:     
  27:     # 4. Timing ê²°ê³¼ ì¡°íšŒ
  28:     summary = pipeline.get_timing_summary()
  29:     alerts = pipeline.get_timing_alerts(supergraph)
  30:     ```
  31: """
  32: from __future__ import annotations
  33: 
  34: from pathlib import Path
  35: from typing import Dict, List, Optional
  36: 
  37: from ..builders.constraint_projector import (
  38:     ConstraintProjector,
  39:     parse_sdc_create_clock,
  40:     parse_sdc_false_path,
  41:     parse_sdc_multicycle_path,
  42: )
  43: from ..core.graph import DKGEdge, DKGNode
  44: from ..builders.graph_updater import GraphUpdater
  45: from ..parsers.timing_report_parser import TimingReportParser
  46: from ..builders.supergraph import SuperGraph, TimingAlert, TimingSummary
  47: from .timing_aggregator import (
  48:     aggregate_timing_to_supergraph,
  49:     compute_timing_summary,
  50:     generate_timing_alerts,
  51: )
  52: 
  53: 
  54: class TimingAnalysisPipeline:
  55:     """
  56:     Timing ë¶„ì„ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤.
  57:     
  58:     ì´ í´ë˜ìŠ¤ëŠ” timing report íŒŒì‹±, constraint íˆ¬ì˜, metrics ì§‘ê³„ë¥¼
  59:     ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
  60:     """
  61: 
  62:     def __init__(
  63:         self,
  64:         nodes: Dict[str, DKGNode],
  65:         edges: Dict[str, DKGEdge],
  66:         updater: GraphUpdater,
  67:     ):
  68:         """
  69:         Args:
  70:             nodes: DKG ë…¸ë“œ ë”•ì…”ë„ˆë¦¬
  71:             edges: DKG ì—£ì§€ ë”•ì…”ë„ˆë¦¬
  72:             updater: GraphUpdater ì¸ìŠ¤í„´ìŠ¤
  73:         """
  74:         self.nodes = nodes
  75:         self.edges = edges
  76:         self.updater = updater
  77:         
  78:         # Timing report parser
  79:         self.timing_parser = TimingReportParser()
  80:         
  81:         # Constraint projector
  82:         self.constraint_projector = ConstraintProjector(nodes, edges, updater)
  83:         
  84:         # ë¶„ì„ ê²°ê³¼ ìºì‹œ
  85:         self._timing_summary: Optional[TimingSummary] = None
  86:         self._timing_alerts: List[TimingAlert] = []
  87: 
  88:     # ========================================================================
  89:     # Step 1: Timing Report ì²˜ë¦¬
  90:     # ========================================================================
  91: 
  92:     def process_timing_report(self, filepath: str | Path) -> None:
  93:         """
  94:         Timing Report íŒŒì¼ì„ íŒŒì‹±í•˜ê³  DKG ê·¸ë˜í”„ì— ë°˜ì˜í•©ë‹ˆë‹¤.
  95:         
  96:         ì´ í•¨ìˆ˜ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
  97:         1. Timing Report íŒŒì¼ íŒŒì‹± (Vivado/PrimeTime í˜•ì‹)
  98:         2. ê° ë…¸ë“œì˜ slack, arrival_time, required_time ì—…ë°ì´íŠ¸
  99:         3. ê° ì—£ì§€ì˜ delay ì—…ë°ì´íŠ¸
 100:         
 101:         Args:
 102:             filepath: Timing Report íŒŒì¼ ê²½ë¡œ
 103:         """
 104:         # íƒ€ì´ë° ë¦¬í¬íŠ¸ íŒŒì‹±
 105:         paths = self.timing_parser.parse_file(filepath)
 106:         
 107:         if not paths:
 108:             print(f"Warning: No timing paths found in {filepath}")
 109:             return
 110:         
 111:         print(f"Parsed {len(paths)} timing paths from {filepath}")
 112:         
 113:         # DKG ê·¸ë˜í”„ì— íƒ€ì´ë° ì •ë³´ ë°˜ì˜
 114:         self.timing_parser.apply_to_graph(self.nodes, self.edges, self.updater)
 115:         
 116:         print(f"Applied timing information to graph")
 117: 
 118:     # ========================================================================
 119:     # Step 2: Constraint íŒŒì¼ ì²˜ë¦¬
 120:     # ========================================================================
 121: 
 122:     def process_constraint_file(
 123:         self, filepath: str | Path, file_type: str = "sdc"
 124:     ) -> None:
 125:         """
 126:         Constraint íŒŒì¼ì„ íŒŒì‹±í•˜ê³  DKG ê·¸ë˜í”„ì— íˆ¬ì˜í•©ë‹ˆë‹¤.
 127:         
 128:         ì§€ì› íŒŒì¼ íƒ€ì…:
 129:         - "sdc": SDC (Synopsys Design Constraints)
 130:         - "xdc": XDC (Xilinx Design Constraints)
 131:         
 132:         Args:
 133:             filepath: Constraint íŒŒì¼ ê²½ë¡œ
 134:             file_type: íŒŒì¼ íƒ€ì… ("sdc" ë˜ëŠ” "xdc")
 135:         """
 136:         filepath = Path(filepath)
 137:         
 138:         if not filepath.exists():
 139:             print(f"Warning: Constraint file not found: {filepath}")
 140:             return
 141:         
 142:         if file_type == "sdc":
 143:             self._process_sdc_file(filepath)
 144:         elif file_type == "xdc":
 145:             self._process_xdc_file(filepath)
 146:         else:
 147:             print(f"Warning: Unsupported constraint file type: {file_type}")
 148: 
 149:     def _process_sdc_file(self, filepath: Path) -> None:
 150:         """SDC íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ì œì•½ì„ íˆ¬ì˜í•©ë‹ˆë‹¤."""
 151:         with open(filepath, "r", encoding="utf-8") as f:
 152:             lines = f.readlines()
 153:         
 154:         constraint_count = 0
 155:         
 156:         for line_num, line in enumerate(lines, start=1):
 157:             line = line.strip()
 158:             
 159:             # create_clock ì²˜ë¦¬
 160:             if line.startswith("create_clock"):
 161:                 constraint = parse_sdc_create_clock(line)
 162:                 if constraint:
 163:                     self.constraint_projector.project_clock_constraint(
 164:                         constraint, str(filepath), line_num
 165:                     )
 166:                     constraint_count += 1
 167:             
 168:             # set_false_path ì²˜ë¦¬
 169:             elif line.startswith("set_false_path"):
 170:                 constraint = parse_sdc_false_path(line)
 171:                 if constraint:
 172:                     self.constraint_projector.project_false_path_constraint(
 173:                         constraint, str(filepath), line_num
 174:                     )
 175:                     constraint_count += 1
 176:             
 177:             # set_multicycle_path ì²˜ë¦¬
 178:             elif line.startswith("set_multicycle_path"):
 179:                 constraint = parse_sdc_multicycle_path(line)
 180:                 if constraint:
 181:                     self.constraint_projector.project_multicycle_path_constraint(
 182:                         constraint, str(filepath), line_num
 183:                     )
 184:                     constraint_count += 1
 185:         
 186:         print(f"Projected {constraint_count} constraints from {filepath}")
 187: 
 188:     def _process_xdc_file(self, filepath: Path) -> None:
 189:         """XDC íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ì œì•½ì„ íˆ¬ì˜í•©ë‹ˆë‹¤."""
 190:         # XDCëŠ” SDCì™€ ìœ ì‚¬í•˜ì§€ë§Œ Xilinx íŠ¹í™” ëª…ë ¹ í¬í•¨
 191:         # ì—¬ê¸°ì„œëŠ” SDCì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
 192:         self._process_sdc_file(filepath)
 193: 
 194:     # ========================================================================
 195:     # Step 3: SuperGraphì— Timing Metrics ë¶€ì°©
 196:     # ========================================================================
 197: 
 198:     def attach_timing_to_supergraph(
 199:         self,
 200:         supergraph: SuperGraph,
 201:         clock_period: float = 10.0,
 202:         critical_threshold: float = 0.0,
 203:         near_critical_alpha: float = 0.1,
 204:     ) -> None:
 205:         """
 206:         SuperGraphì˜ ëª¨ë“  SuperNode/SuperEdgeì— íƒ€ì´ë° ë©”íŠ¸ë¦­ì„ ì§‘ê³„í•˜ê³  ë¶€ì°©í•©ë‹ˆë‹¤.
 207:         
 208:         ì´ í•¨ìˆ˜ëŠ” DKG ë…¸ë“œ/ì—£ì§€ì— ì €ì¥ëœ raw timing ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ì—¬
 209:         SuperNode/SuperEdgeì— TimingNodeMetrics/TimingEdgeMetricsë¥¼ ìƒì„±í•˜ê³  ë¶€ì°©í•©ë‹ˆë‹¤.
 210:         
 211:         Args:
 212:             supergraph: íƒ€ì´ë° ë©”íŠ¸ë¦­ì„ ë¶€ì°©í•  SuperGraph
 213:             clock_period: í´ëŸ­ ì£¼ê¸° (ns)
 214:             critical_threshold: critical íŒì • ì„ê³„ê°’ (slack < 0ì´ë©´ critical)
 215:             near_critical_alpha: near-critical íŒì • ë°°ìœ¨ (clock_periodì˜ 10% ì´ë‚´)
 216:         """
 217:         aggregate_timing_to_supergraph(
 218:             supergraph,
 219:             self.nodes,
 220:             self.edges,
 221:             clock_period,
 222:             critical_threshold,
 223:             near_critical_alpha,
 224:         )
 225:         
 226:         print(f"Attached timing metrics to {len(supergraph.super_nodes)} supernodes and {len(supergraph.super_edges)} superedges")
 227: 
 228:     # ========================================================================
 229:     # Step 4: Timing ê²°ê³¼ ì¡°íšŒ
 230:     # ========================================================================
 231: 
 232:     def get_timing_summary(
 233:         self, clock_period: float = 10.0, analysis_mode: str = "setup"
 234:     ) -> TimingSummary:
 235:         """
 236:         ì „ì²´ ê·¸ë˜í”„ì˜ Timing ìš”ì•½ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 237:         
 238:         Args:
 239:             clock_period: í´ëŸ­ ì£¼ê¸° (ns)
 240:             analysis_mode: "setup" / "hold" / "both"
 241:         
 242:         Returns:
 243:             TimingSummary ê°ì²´
 244:         """
 245:         if self._timing_summary is None:
 246:             self._timing_summary = compute_timing_summary(
 247:                 self.nodes, clock_period, analysis_mode
 248:             )
 249:         
 250:         return self._timing_summary
 251: 
 252:     def get_timing_alerts(
 253:         self,
 254:         supergraph: SuperGraph,
 255:         critical_threshold: float = 0.0,
 256:         warn_threshold: float = 0.5,
 257:     ) -> List[TimingAlert]:
 258:         """
 259:         SuperGraphì—ì„œ íƒ€ì´ë° ë¬¸ì œë¥¼ ì°¾ì•„ Alert ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 260:         
 261:         Args:
 262:             supergraph: ë¶„ì„í•  SuperGraph
 263:             critical_threshold: ERROR ë ˆë²¨ ì„ê³„ê°’
 264:             warn_threshold: WARN ë ˆë²¨ ì„ê³„ê°’
 265:         
 266:         Returns:
 267:             TimingAlert ë¦¬ìŠ¤íŠ¸ (ì‹¬ê°ë„ ìˆœ ì •ë ¬)
 268:         """
 269:         self._timing_alerts = generate_timing_alerts(
 270:             supergraph, self.nodes, critical_threshold, warn_threshold
 271:         )
 272:         
 273:         # ì‹¬ê°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ERROR â†’ WARN â†’ INFO)
 274:         severity_order = {"error": 0, "warn": 1, "info": 2}
 275:         self._timing_alerts.sort(
 276:             key=lambda a: (severity_order.get(a.severity.lower(), 3), a.entity_ref)
 277:         )
 278:         
 279:         return self._timing_alerts
 280: 
 281:     def print_timing_report(
 282:         self,
 283:         supergraph: Optional[SuperGraph] = None,
 284:         clock_period: float = 10.0,
 285:         show_alerts: bool = True,
 286:     ) -> None:
 287:         """
 288:         íƒ€ì´ë° ë¶„ì„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤.
 289:         
 290:         Args:
 291:             supergraph: SuperGraph (Alertë¥¼ í‘œì‹œí•˜ë ¤ë©´ í•„ìš”)
 292:             clock_period: í´ëŸ­ ì£¼ê¸° (ns)
 293:             show_alerts: Alertë¥¼ í‘œì‹œí• ì§€ ì—¬ë¶€
 294:         """
 295:         print("\n" + "=" * 80)
 296:         print("TIMING ANALYSIS REPORT")
 297:         print("=" * 80)
 298:         
 299:         # Timing Summary
 300:         summary = self.get_timing_summary(clock_period)
 301:         print(f"\n[Timing Summary]")
 302:         print(f"  Worst Slack:         {summary.worst_slack:.3f} ns")
 303:         print(f"  Violation Count:     {summary.violation_count}")
 304:         print(f"  Near-Critical Count: {summary.near_critical_count}")
 305:         print(f"  Clock Period:        {summary.clock_period:.3f} ns")
 306:         print(f"  Analysis Mode:       {summary.analysis_mode}")
 307:         print(f"  Timestamp:           {summary.timestamp}")
 308:         
 309:         # Timing Alerts
 310:         if show_alerts and supergraph:
 311:             alerts = self.get_timing_alerts(supergraph)
 312:             
 313:             if alerts:
 314:                 print(f"\n[Timing Alerts] ({len(alerts)} issues found)")
 315:                 
 316:                 for i, alert in enumerate(alerts[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
 317:                     severity = alert.severity.upper()
 318:                     print(f"\n  {i}. [{severity}] {alert.entity_type}: {alert.entity_ref}")
 319:                     print(f"     Reason: {alert.reason}")
 320:                 
 321:                 if len(alerts) > 10:
 322:                     print(f"\n  ... and {len(alerts) - 10} more alerts")
 323:             else:
 324:                 print(f"\n[Timing Alerts] No issues found")
 325:         
 326:         print("\n" + "=" * 80 + "\n")
 327: 
 328: 
 329: # ============================================================================
 330: # Quick Start API
 331: # ============================================================================
 332: 
 333: 
 334: def quick_timing_analysis(
 335:     nodes: Dict[str, DKGNode],
 336:     edges: Dict[str, DKGEdge],
 337:     updater: GraphUpdater,
 338:     supergraph: SuperGraph,
 339:     timing_report_path: Optional[str] = None,
 340:     constraint_path: Optional[str] = None,
 341:     clock_period: float = 10.0,
 342: ) -> TimingSummary:
 343:     """
 344:     íƒ€ì´ë° ë¶„ì„ì„ í•œ ë²ˆì— ìˆ˜í–‰í•˜ëŠ” ê°„í¸ API.
 345:     
 346:     ì‚¬ìš© ì˜ˆì‹œ:
 347:         ```python
 348:         summary = quick_timing_analysis(
 349:             nodes, edges, updater, supergraph,
 350:             timing_report_path="design.timing_rpt",
 351:             constraint_path="design.sdc",
 352:             clock_period=10.0
 353:         )
 354:         ```
 355:     
 356:     Args:
 357:         nodes: DKG ë…¸ë“œ ë”•ì…”ë„ˆë¦¬
 358:         edges: DKG ì—£ì§€ ë”•ì…”ë„ˆë¦¬
 359:         updater: GraphUpdater ì¸ìŠ¤í„´ìŠ¤
 360:         supergraph: SuperGraph
 361:         timing_report_path: Timing Report íŒŒì¼ ê²½ë¡œ (ì˜µì…˜)
 362:         constraint_path: Constraint íŒŒì¼ ê²½ë¡œ (ì˜µì…˜)
 363:         clock_period: í´ëŸ­ ì£¼ê¸° (ns)
 364:     
 365:     Returns:
 366:         TimingSummary ê°ì²´
 367:     """
 368:     pipeline = TimingAnalysisPipeline(nodes, edges, updater)
 369:     
 370:     # 1. Timing Report ì²˜ë¦¬
 371:     if timing_report_path:
 372:         pipeline.process_timing_report(timing_report_path)
 373:     
 374:     # 2. Constraint ì²˜ë¦¬
 375:     if constraint_path:
 376:         file_type = "xdc" if constraint_path.endswith(".xdc") else "sdc"
 377:         pipeline.process_constraint_file(constraint_path, file_type)
 378:     
 379:     # 3. SuperGraphì— Timing Metrics ë¶€ì°©
 380:     pipeline.attach_timing_to_supergraph(supergraph, clock_period)
 381:     
 382:     # 4. ê²°ê³¼ ì¶œë ¥
 383:     pipeline.print_timing_report(supergraph, clock_period)
 384:     
 385:     return pipeline.get_timing_summary(clock_period)



FILE: dkg\utils\config.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass
   4: 
   5: 
   6: @dataclass
   7: class YosysConfig:
   8:     src_dir_win: str
   9:     out_json_win: str
  10:     top_module: str



FILE: dkg\utils\debug.py

   1: from __future__ import annotations
   2: 
   3: import random
   4: from typing import Dict, Iterable, List
   5: 
   6: from ..core.graph import DKGEdge, DKGNode
   7: from ..core.ir import CellIR, Wire
   8: 
   9: 
  10: def print_graph_summary(wires: Dict[int, Wire], cells: List[CellIR], nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge]) -> None:
  11:     print("===== GRAPH SUMMARY =====")
  12:     print(f"Total wires   : {len(wires)}")
  13:     print(f"Total cells   : {len(cells)}")
  14:     print(f"Total nodes   : {len(nodes)}")
  15:     print(f"Total edges   : {len(edges)}")
  16:     print("=========================")
  17: 
  18: 
  19: def print_sample_node(nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge], max_edges: int = 5) -> None:
  20:     if not nodes:
  21:         print("No nodes available.")
  22:         return
  23: 
  24:     sample = random.choice(list(nodes.values()))
  25:     print("\n===== SAMPLE NODE =====")
  26:     print("Node:", sample.node_id, sample.entity_class)
  27:     print("IN edges:", len(sample.in_edges))
  28:     print("OUT edges:", len(sample.out_edges))
  29: 
  30:     for eid in sample.out_edges[:max_edges]:
  31:         e = edges[eid]
  32:         print("  ->", e.signal_name, "->", e.dst_node)
  33:     print("=========================")
  34: 
  35: 
  36: def print_fanout_summary(wires: Dict[int, Wire]) -> None:
  37:     fanouts = [len(w.loads) for w in wires.values() if w.loads]
  38:     if not fanouts:
  39:         print("\nMax fanout: 0")
  40:         print("Avg fanout: 0")
  41:         print("=========================")
  42:         return
  43: 
  44:     print("\nMax fanout:", max(fanouts))
  45:     print("Avg fanout:", sum(fanouts) / len(fanouts))
  46:     print("=========================")
  47: 
  48: 
  49: def trace_signal(wires: Dict[int, Wire], target: str) -> None:
  50:     print("\n===== TRACE SIGNAL:", target, "=====")
  51:     for w in wires.values():
  52:         if w.name == target:
  53:             print("Drivers:", w.drivers)
  54:             print("Loads  :", w.loads)
  55:     print("=========================")
  56: 
  57: 
  58: def plot_subgraph(nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge], limit: int = 30) -> None:
  59:     try:
  60:         import networkx as nx
  61:         import matplotlib.pyplot as plt
  62:     except Exception as exc:
  63:         print("Plot skipped:", exc)
  64:         return
  65: 
  66:     g = nx.DiGraph()
  67:     for nid in nodes:
  68:         g.add_node(nid)
  69: 
  70:     for e in edges.values():
  71:         g.add_edge(e.src_node, e.dst_node, label=e.signal_name)
  72: 
  73:     sub_nodes = list(nodes.keys())[:limit]
  74: 
  75:     def clean_label(name: str) -> str:
  76:         return name.replace("\\", "").replace("$", "")
  77: 
  78:     h = g.subgraph(sub_nodes)
  79:     labels = {n: clean_label(n) for n in h.nodes()}
  80: 
  81:     nx.draw(h, labels=labels, with_labels=True, node_size=500, font_size=6)
  82:     plt.show()



FILE: dkg\utils\utils.py

   1: from __future__ import annotations
   2: 
   3: import hashlib
   4: import re
   5: from pathlib import Path
   6: from typing import Optional, Tuple
   7: 
   8: 
   9: def is_clock_name(name: str) -> bool:
  10:     n = name.lower()
  11:     return n == "clk" or n.startswith("clk") or n.endswith("_clk") or "clock" in n
  12: 
  13: 
  14: def is_reset_name(name: str) -> bool:
  15:     n = name.lower()
  16:     return n == "rst" or n.startswith("rst") or n.startswith("reset")
  17: 
  18: 
  19: def is_active_low(name: str) -> bool:
  20:     return name.lower().endswith("_n")
  21: 
  22: 
  23: def is_ff_cell(cell_type: str) -> bool:
  24:     return cell_type in {"$dff", "$adff", "$sdff", "$dffe", "$sdffe"}
  25: 
  26: 
  27: def is_async_reset_ff(cell_type: str) -> bool:
  28:     return cell_type == "$adff"
  29: 
  30: 
  31: def is_sync_reset_ff(cell_type: str) -> bool:
  32:     return cell_type == "$sdff"
  33: 
  34: 
  35: def win_to_wsl_path(win_path: str) -> str:
  36:     p = Path(win_path).resolve()
  37:     drive = p.drive[0].lower()
  38:     path_no_drive = p.as_posix()[2:]
  39:     return f"/mnt/{drive}{path_no_drive}"
  40: 
  41: 
  42: def parse_src(src_str: Optional[str]) -> Tuple[Optional[str], Optional[int]]:
  43:     if not src_str:
  44:         return None, None
  45:     try:
  46:         file_part, line_part = src_str.split(":")
  47:         line = int(line_part.split(".")[0])
  48:         return file_part, line
  49:     except Exception:
  50:         return None, None
  51: 
  52: 
  53: def split_signal_bit(sig: str) -> Tuple[str, Optional[int]]:
  54:     m = re.match(r"(.+)\[(\d+)\]$", sig)
  55:     if m:
  56:         return m.group(1), int(m.group(2))
  57:     return sig, None
  58: 
  59: 
  60: def stable_hash(s: str, length: int = 12) -> str:
  61:     return hashlib.sha1(s.encode()).hexdigest()[:length]
  62: 
  63: 
  64: def compute_file_hash(filepath: str | Path) -> str:
  65:     """íŒŒì¼ ë‚´ìš©ì˜ SHA-256 í•´ì‹œ ê³„ì‚°"""
  66:     filepath = Path(filepath)
  67:     if not filepath.exists():
  68:         raise FileNotFoundError(f"File not found: {filepath}")
  69:     
  70:     sha256 = hashlib.sha256()
  71:     with open(filepath, "rb") as f:
  72:         for chunk in iter(lambda: f.read(8192), b""):
  73:             sha256.update(chunk)
  74:     return sha256.hexdigest()[:16]  # ì²˜ìŒ 16ìë§Œ ì‚¬ìš©


