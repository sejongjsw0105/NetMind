
FILE: dkg\__init__.py




FILE: dkg\graphcache\__init__.py

   1: """
   2: 그래프 캐싱 모듈
   3: 
   4: - graph_version: 그래프 버전 정보
   5: - snapshot: 그래프 스냅샷 저장/로딩
   6: """
   7: from .graph_version import GraphVersion
   8: from .snapshot import GraphSnapshot, load_snapshot, save_snapshot
   9: 
  10: __all__ = [
  11:     "GraphVersion",
  12:     "GraphSnapshot",
  13:     "save_snapshot",
  14:     "load_snapshot",
  15: ]



FILE: dkg\parsers\__init__.py

   1: from __future__ import annotations
   2: 
   3: from abc import ABC, abstractmethod
   4: from typing import Dict
   5: 
   6: from ..graph import DKGEdge, DKGNode
   7: from ..graph_updater import GraphUpdater
   8: from ..stages import ParsingStage
   9: 
  10: 
  11: class ConstraintParser(ABC):
  12:     """제약 파일 파서 베이스 클래스"""
  13:     
  14:     @abstractmethod
  15:     def get_stage(self) -> ParsingStage:
  16:         """이 파서가 속한 stage 반환"""
  17:         pass
  18:     
  19:     @abstractmethod
  20:     def parse_and_update(
  21:         self,
  22:         filepath: str,
  23:         updater: GraphUpdater,
  24:         nodes: Dict[str, DKGNode],
  25:         edges: Dict[str, DKGEdge],
  26:     ) -> None:
  27:         """
  28:         파일을 파싱하고 그래프를 업데이트.
  29:         
  30:         Args:
  31:             filepath: 파싱할 파일 경로
  32:             updater: GraphUpdater 인스턴스
  33:             nodes: 기존 노드 딕셔너리
  34:             edges: 기존 엣지 딕셔너리
  35:         """
  36:         pass



FILE: main.py

   1: from __future__ import annotations
   2: from dkg.config import YosysConfig
   3: from dkg.debug import (
   4:     plot_subgraph,
   5:     print_fanout_summary,
   6:     print_graph_summary,
   7:     print_sample_node,
   8:     trace_signal,
   9: )
  10: from dkg.graph_build import build_nodes_and_edges, build_wires_and_cells
  11: from dkg.yosys_parser import parse_yosys
  12: 
  13: 
  14: DEFAULT_CONFIG = YosysConfig(
  15:     src_dir_win=r"C:\Users\User\NetMind\구현\예시",
  16:     out_json_win=r"C:\Users\User\NetMind\구현\design.json",
  17:     top_module="riscvsingle",
  18: )
  19: 
  20: 
  21: def main(config: YosysConfig, debug: bool = True) -> None:
  22:     yosys = parse_yosys(config)
  23:     wires, cells = build_wires_and_cells(yosys)
  24:     nodes, edges = build_nodes_and_edges(wires, cells)
  25: 
  26:     if debug:
  27:         print_graph_summary(wires, cells, nodes, edges)
  28:         print_sample_node(nodes, edges)
  29:         print_fanout_summary(wires)
  30:         trace_signal(wires, "clk")
  31:         plot_subgraph(nodes, edges, limit=30)
  32: 
  33: 
  34: if __name__ == "__main__":
  35:     main(DEFAULT_CONFIG, debug=True)



FILE: AICollector.py

   1: from pathlib import Path
   2: 
   3: def collect_py_files(output_txt: str = "result.txt"):
   4:     base_path = Path(__file__).parent
   5:     py_files = list(base_path.rglob("*.py"))
   6: 
   7:     def sort_key(path: Path):
   8:         name = path.name.lower()
   9:         if name == "__init__.py":
  10:             return (0, str(path))
  11:         if name == "main.py":
  12:             return (1, str(path))
  13:         return (2, str(path))
  14: 
  15:     py_files.sort(key=sort_key)
  16: 
  17:     with open(output_txt, "w", encoding="utf-8") as out:
  18:         for py_file in py_files:
  19:             relative_path = py_file.relative_to(base_path)
  20: 
  21:             out.write("\n")
  22:             out.write(f"FILE: {relative_path}\n")
  23:             out.write("\n")
  24: 
  25:             try:
  26:                 with open(py_file, "r", encoding="utf-8") as f:
  27:                     for idx, line in enumerate(f, start=1):
  28:                         out.write(f"{idx:4d}: {line}")
  29:             except Exception as e:
  30:                 out.write(f"[ERROR] 파일을 읽을 수 없습니다: {e}\n")
  31: 
  32:             out.write("\n\n")
  33: 
  34: 
  35: if __name__ == "__main__":
  36:     collect_py_files()



FILE: cache_example.py

   1: """
   2: 그래프 캐싱 사용 예제
   3: 
   4: Usage:
   5:     # 1. 그래프 구축 후 캐시 저장
   6:     pipeline = DKGPipeline(yosys_config)
   7:     pipeline.run_rtl_stage()
   8:     pipeline.add_constraints("design.sdc")
   9:     pipeline.save_cache("graph_cache.json")
  10:     
  11:     # 2. 캐시에서 로딩 (빠른 시작)
  12:     pipeline = DKGPipeline.load_from_cache("graph_cache.json")
  13:     nodes, edges = pipeline.get_graph()
  14: """
  15: from pathlib import Path
  16: 
  17: from dkg.config import YosysConfig
  18: from dkg.pipeline import DKGPipeline
  19: 
  20: 
  21: def save_example():
  22:     """그래프 구축 후 캐시 저장"""
  23:     # Yosys JSON 파일 경로 설정
  24:     yosys_config = YosysConfig(
  25:         src_dir_win=".",
  26:         out_json_win="output.json",  # Yosys로 생성한 JSON 파일
  27:         top_module="top",
  28:     )
  29:     
  30:     # 파이프라인 실행
  31:     pipeline = DKGPipeline(yosys_config)
  32:     
  33:     # Stage 1: RTL 파싱
  34:     pipeline.run_rtl_stage()
  35:     
  36:     # Stage 2: Constraint 추가 (선택)
  37:     # pipeline.add_constraints("design.sdc")
  38:     # pipeline.add_constraints("design.xdc")
  39:     
  40:     # Stage 3: SuperGraph 생성 (선택)
  41:     # from dkg.supergraph import build_supergraph
  42:     # pipeline.supergraph = build_supergraph(pipeline.nodes, pipeline.edges)
  43:     
  44:     # 캐시 저장
  45:     cache_path = Path("graph_cache.json")
  46:     pipeline.save_cache(cache_path, indent=2)  # indent=2는 디버깅용, 생략하면 압축됨
  47:     
  48:     print(f"✅ 그래프 캐시 저장 완료: {cache_path}")
  49:     if pipeline.nodes and pipeline.edges:
  50:         print(f"   - 노드 수: {len(pipeline.nodes)}")
  51:         print(f"   - 엣지 수: {len(pipeline.edges)}")
  52:     
  53:     # 버전 정보 확인
  54:     version = pipeline.compute_version()
  55:     print(f"   - RTL 해시: {version.rtl_hash}")
  56:     if version.constraint_hash:
  57:         print(f"   - Constraint 해시: {version.constraint_hash}")
  58: 
  59: 
  60: def load_example():
  61:     """캐시에서 그래프 로딩"""
  62:     cache_path = Path("graph_cache.json")
  63:     
  64:     if not cache_path.exists():
  65:         print(f"❌ 캐시 파일이 없습니다: {cache_path}")
  66:         print("   save_example()을 먼저 실행하세요.")
  67:         return
  68:     
  69:     # 캐시에서 로딩 (매우 빠름!)
  70:     pipeline = DKGPipeline.load_from_cache(cache_path)
  71:     
  72:     print(f"✅ 그래프 캐시 로딩 완료: {cache_path}")
  73:     if pipeline.nodes and pipeline.edges:
  74:         print(f"   - 노드 수: {len(pipeline.nodes)}")
  75:         print(f"   - 엣지 수: {len(pipeline.edges)}")
  76:     
  77:     # SuperGraph가 있으면 표시
  78:     if pipeline.supergraph:
  79:         print(f"   - SuperNode 수: {len(pipeline.supergraph.super_nodes)}")
  80:         print(f"   - SuperEdge 수: {len(pipeline.supergraph.super_edges)}")
  81:     
  82:     # 그래프 사용
  83:     nodes, edges = pipeline.get_graph()
  84:     
  85:     # 예: 첫 10개 노드 출력
  86:     print("\n처음 10개 노드:")
  87:     for i, (node_id, node) in enumerate(nodes.items()):
  88:         if i >= 10:
  89:             break
  90:         print(f"  - {node.canonical_name or node_id}: {node.entity_class.value}")
  91: 
  92: 
  93: if __name__ == "__main__":
  94:     import sys
  95:     
  96:     if len(sys.argv) > 1 and sys.argv[1] == "load":
  97:         load_example()
  98:     else:
  99:         save_example()



FILE: dkg\config.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass
   4: 
   5: 
   6: @dataclass
   7: class YosysConfig:
   8:     src_dir_win: str
   9:     out_json_win: str
  10:     top_module: str



FILE: dkg\debug.py

   1: from __future__ import annotations
   2: 
   3: import random
   4: from typing import Dict, Iterable, List
   5: 
   6: from .graph import DKGEdge, DKGNode
   7: from .ir import CellIR, Wire
   8: 
   9: 
  10: def print_graph_summary(wires: Dict[int, Wire], cells: List[CellIR], nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge]) -> None:
  11:     print("===== GRAPH SUMMARY =====")
  12:     print(f"Total wires   : {len(wires)}")
  13:     print(f"Total cells   : {len(cells)}")
  14:     print(f"Total nodes   : {len(nodes)}")
  15:     print(f"Total edges   : {len(edges)}")
  16:     print("=========================")
  17: 
  18: 
  19: def print_sample_node(nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge], max_edges: int = 5) -> None:
  20:     if not nodes:
  21:         print("No nodes available.")
  22:         return
  23: 
  24:     sample = random.choice(list(nodes.values()))
  25:     print("\n===== SAMPLE NODE =====")
  26:     print("Node:", sample.node_id, sample.entity_class)
  27:     print("IN edges:", len(sample.in_edges))
  28:     print("OUT edges:", len(sample.out_edges))
  29: 
  30:     for eid in sample.out_edges[:max_edges]:
  31:         e = edges[eid]
  32:         print("  ->", e.signal_name, "->", e.dst_node)
  33:     print("=========================")
  34: 
  35: 
  36: def print_fanout_summary(wires: Dict[int, Wire]) -> None:
  37:     fanouts = [len(w.loads) for w in wires.values() if w.loads]
  38:     if not fanouts:
  39:         print("\nMax fanout: 0")
  40:         print("Avg fanout: 0")
  41:         print("=========================")
  42:         return
  43: 
  44:     print("\nMax fanout:", max(fanouts))
  45:     print("Avg fanout:", sum(fanouts) / len(fanouts))
  46:     print("=========================")
  47: 
  48: 
  49: def trace_signal(wires: Dict[int, Wire], target: str) -> None:
  50:     print("\n===== TRACE SIGNAL:", target, "=====")
  51:     for w in wires.values():
  52:         if w.name == target:
  53:             print("Drivers:", w.drivers)
  54:             print("Loads  :", w.loads)
  55:     print("=========================")
  56: 
  57: 
  58: def plot_subgraph(nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge], limit: int = 30) -> None:
  59:     try:
  60:         import networkx as nx
  61:         import matplotlib.pyplot as plt
  62:     except Exception as exc:
  63:         print("Plot skipped:", exc)
  64:         return
  65: 
  66:     g = nx.DiGraph()
  67:     for nid in nodes:
  68:         g.add_node(nid)
  69: 
  70:     for e in edges.values():
  71:         g.add_edge(e.src_node, e.dst_node, label=e.signal_name)
  72: 
  73:     sub_nodes = list(nodes.keys())[:limit]
  74: 
  75:     def clean_label(name: str) -> str:
  76:         return name.replace("\\", "").replace("$", "")
  77: 
  78:     h = g.subgraph(sub_nodes)
  79:     labels = {n: clean_label(n) for n in h.nodes()}
  80: 
  81:     nx.draw(h, labels=labels, with_labels=True, node_size=500, font_size=6)
  82:     plt.show()



FILE: dkg\graph.py

   1: # NOTE:
   2: # canonical_name is a human-readable debug label.
   3: # It is intended for debugging, tracing, and debug-like queries only.
   4: # It is NOT guaranteed to be stable across views, abstractions, or builds,
   5: # and MUST NOT be used as a persistent identifier, cache key, or external reference.
   6: from __future__ import annotations
   7: 
   8: from dataclasses import dataclass, field
   9: from enum import Enum
  10: from typing import Any, Dict, List, Optional, Tuple
  11: 
  12: from .provenance import Provenance
  13: 
  14: 
  15: class EntityClass(str, Enum):
  16:     MODULE_INSTANCE = "ModuleInstance"
  17:     RTL_BLOCK = "RTLBlock"
  18:     FSM = "FSM"
  19: 
  20:     FLIP_FLOP = "FlipFlop"
  21:     LUT = "LUT"
  22:     MUX = "MUX"
  23:     DSP = "DSP"
  24:     BRAM = "BRAM"
  25: 
  26:     IO_PORT = "IOPort"
  27:     PACKAGE_PIN = "PackagePin"
  28:     PBLOCK = "Pblock"
  29:     BOARD_CONNECTOR = "BoardConnector"
  30: 
  31: 
  32: class RelationType(str, Enum):
  33:     DATA = "DataRelation"
  34:     CLOCK = "ClockRelation"
  35:     RESET = "ResetRelation"
  36:     PARAMETER = "ParameterRelation"
  37:     CONSTRAINT = "ConstraintRelation"
  38:     PHYSICAL_MAP = "PhysicalMappingRelation"
  39: 
  40: 
  41: class EdgeFlowType(str, Enum):
  42:     COMBINATIONAL = "combinational"
  43:     SEQ_LAUNCH = "sequential_launch"
  44:     SEQ_CAPTURE = "sequential_capture"
  45:     CLOCK_TREE = "clock_tree"
  46:     ASYNC_RESET = "async_reset"
  47: 
  48: 
  49: @dataclass
  50: class DKGNode:
  51:     node_id: str
  52:     entity_class: EntityClass
  53:     hier_path: str
  54:     local_name: str
  55:     canonical_name: Optional[str] = None
  56: 
  57:     short_alias: Optional[str] = None
  58: 
  59:     parameters: Dict[str, str] = field(default_factory=dict)
  60:     attributes: Dict[str, str] = field(default_factory=dict)
  61: 
  62:     clock_domain: Optional[str] = None
  63:     arrival_time: Optional[float] = None
  64:     required_time: Optional[float] = None
  65:     slack: Optional[float] = None
  66: 
  67:     in_edges: List[str] = field(default_factory=list)
  68:     out_edges: List[str] = field(default_factory=list)
  69: 
  70:     provenances: List[Provenance] = field(default_factory=list)
  71:     primary_provenance: Optional[Provenance] = None
  72: 
  73: 
  74: @dataclass
  75: class DKGEdge:
  76:     edge_id: str
  77:     src_node: str
  78:     dst_node: str
  79: 
  80:     relation_type: RelationType
  81:     flow_type: EdgeFlowType
  82: 
  83:     signal_name: str
  84:     canonical_name: str
  85:     bit_range: Optional[Tuple[int, int]] = None
  86:     net_id: Optional[str] = None
  87: 
  88:     driver_type: Optional[str] = None
  89:     fanout_count: Optional[int] = None
  90: 
  91:     clock_signal: Optional[str] = None
  92:     reset_signal: Optional[str] = None
  93:     clock_domain_id: Optional[str] = None
  94: 
  95:     timing_exception: Optional[str] = None
  96:     parameters: Dict[str, Any] = field(default_factory=dict)
  97: 
  98:     delay: Optional[float] = None
  99:     arrival_time: Optional[float] = None
 100:     required_time: Optional[float] = None
 101:     slack: Optional[float] = None
 102: 
 103:     attributes: Dict[str, Any] = field(default_factory=dict)
 104: 
 105:     provenances: List[Provenance] = field(default_factory=list)
 106:     primary_provenance: Optional[Provenance] = None
 107: 
 108: 
 109: def make_node_canonical_name(node: DKGNode) -> str:
 110:     base = node.hier_path
 111: 
 112:     cls = node.entity_class
 113:     if cls == EntityClass.FLIP_FLOP:
 114:         suffix = f"reg_{node.local_name}"
 115:     elif cls == EntityClass.MUX:
 116:         suffix = "mux"
 117:     elif cls == EntityClass.LUT:
 118:         suffix = "comb"
 119:     elif cls == EntityClass.BRAM:
 120:         suffix = "bram"
 121:     elif cls == EntityClass.DSP:
 122:         suffix = "dsp"
 123:     elif cls == EntityClass.IO_PORT:
 124:         suffix = f"port_{node.local_name}"
 125:     else:
 126:         suffix = node.local_name or cls.value.lower()
 127: 
 128:     return f"{base}.{suffix}"
 129: 
 130: 
 131: def make_node_display_name(node: DKGNode) -> str:
 132:     if node.entity_class == EntityClass.FLIP_FLOP:
 133:         return f"Reg {node.local_name}"
 134:     if node.entity_class == EntityClass.BRAM:
 135:         return "BRAM"
 136:     if node.entity_class == EntityClass.MUX:
 137:         return "MUX"
 138:     if node.entity_class == EntityClass.LUT:
 139:         return "Logic"
 140:     if node.entity_class == EntityClass.DSP:
 141:         return "DSP"
 142:     if node.entity_class == EntityClass.IO_PORT:
 143:         return f"Port {node.local_name}"
 144: 
 145:     return node.local_name or node.entity_class.value
 146: 
 147: 
 148: def make_edge_canonical_name(e: DKGEdge, nodes: dict[str, DKGNode]) -> str:
 149:     src = nodes[e.src_node].canonical_name
 150:     dst = nodes[e.dst_node].canonical_name
 151: 
 152:     if e.bit_range:
 153:         msb, lsb = e.bit_range
 154:         sig = f"{e.signal_name}[{msb}:{lsb}]"
 155:     else:
 156:         sig = e.signal_name
 157: 
 158:     return f"{src} -> {dst} : {sig}"
 159: 
 160: 
 161: def make_edge_display_name(e: DKGEdge) -> str:
 162:     if e.bit_range:
 163:         msb, lsb = e.bit_range
 164:         return f"{e.signal_name}[{msb}:{lsb}]"
 165:     return e.signal_name



FILE: dkg\graph_build.py

   1: from __future__ import annotations
   2: 
   3: from collections import defaultdict
   4: from typing import Dict, Iterable, List, Optional, Tuple
   5: 
   6: from .graph import (
   7:     DKGEdge,
   8:     DKGNode,
   9:     EdgeFlowType,
  10:     EntityClass,
  11:     RelationType,
  12:     make_node_canonical_name,
  13: )
  14: from .ir import CellIR, Wire
  15: from .provenance import Provenance, add_provenance, merge_provenances_edges
  16: from .utils import (
  17:     is_active_low,
  18:     is_clock_name,
  19:     is_reset_name,
  20:     parse_src,
  21:     split_signal_bit,
  22:     stable_hash,
  23: )
  24: 
  25: 
  26: def get_wire(wires: Dict[int, Wire], wid) -> Optional[Wire]:
  27:     if isinstance(wid, str):
  28:         return None
  29:     if wid not in wires:
  30:         wires[wid] = Wire(wid)
  31:     return wires[wid]
  32: 
  33: 
  34: def build_wires_and_cells(yosys: dict) -> Tuple[Dict[int, Wire], List[CellIR]]:
  35:     wires: Dict[int, Wire] = {}
  36:     cells: List[CellIR] = []
  37: 
  38:     for mod in yosys.get("modules", {}).values():
  39:         for netname, netinfo in mod.get("netnames", {}).items():
  40:             src = netinfo.get("src")
  41:             for wid in netinfo.get("bits", []):
  42:                 w = get_wire(wires, wid)
  43:                 if w:
  44:                     w.name = netname
  45:                     w.src = src
  46: 
  47:     for mod_name, mod in yosys.get("modules", {}).items():
  48:         for cname, c in mod.get("cells", {}).items():
  49:             cells.append(
  50:                 CellIR(
  51:                     name=cname,
  52:                     type=c["type"],
  53:                     module=mod_name,
  54:                     port_dirs=c["port_directions"],
  55:                     connections=c["connections"],
  56:                     src=c.get("src"),
  57:                 )
  58:             )
  59: 
  60:     return wires, cells
  61: 
  62: 
  63: def map_cell_type(t: str) -> EntityClass:
  64:     if t in ["$adff", "$dff"]:
  65:         return EntityClass.FLIP_FLOP
  66:     if t in ["$mux", "$pmux"]:
  67:         return EntityClass.MUX
  68:     if t in ["$add", "$sub", "$and", "$or"]:
  69:         return EntityClass.RTL_BLOCK
  70:     return EntityClass.RTL_BLOCK
  71: 
  72: 
  73: def cell_signature(cell: CellIR) -> str:
  74:     ports = sorted(
  75:         f"{p}:{cell.port_dirs[p]}:{len(bits)}"
  76:         for p, bits in cell.connections.items()
  77:     )
  78:     return "|".join(
  79:         [
  80:             cell.type,
  81:             cell.module,
  82:             ",".join(ports),
  83:         ]
  84:     )
  85: 
  86: 
  87: def signal_signature(e: DKGEdge) -> str:
  88:     if e.bit_range:
  89:         msb, lsb = e.bit_range
  90:         return f"{e.signal_name}[{msb}:{lsb}]"
  91:     return e.signal_name
  92: 
  93: 
  94: def edge_signature(e: DKGEdge) -> str:
  95:     return "|".join(
  96:         [
  97:             e.src_node,
  98:             e.dst_node,
  99:             e.relation_type.value,
 100:             e.flow_type.value,
 101:             signal_signature(e),
 102:         ]
 103:     )
 104: 
 105: 
 106: def make_edge_id(e: DKGEdge) -> str:
 107:     sig = edge_signature(e)
 108:     h = stable_hash(sig)
 109:     return f"E_{e.relation_type.value}_{h}"
 110: 
 111: 
 112: def make_node_id(cell: CellIR) -> str:
 113:     sig = cell_signature(cell)
 114:     return f"N_{map_cell_type(cell.type).value}_{stable_hash(sig)}"
 115: 
 116: 
 117: def connect_wires_to_cells(wires: Dict[int, Wire], cells: Iterable[CellIR]) -> None:
 118:     cell_id_map: Dict[str, str] = {}
 119:     for cell in cells:
 120:         cell_key = f"{cell.module}.{cell.name}"
 121:         cell_id_map[cell_key] = make_node_id(cell)
 122: 
 123:     for cell in cells:
 124:         node_id = cell_id_map[f"{cell.module}.{cell.name}"]
 125:         for port, bits in cell.connections.items():
 126:             direction = cell.port_dirs[port]
 127:             for wid in bits:
 128:                 w = get_wire(wires, wid)
 129:                 if not w:
 130:                     continue
 131:                 if direction == "output":
 132:                     w.drivers.append(node_id)
 133:                 else:
 134:                     w.loads.append(node_id)
 135: 
 136: 
 137: def detect_clock_reset_from_ff_cells(
 138:     cells: List[CellIR],
 139:     wires: Dict[int, Wire],
 140: ) -> Tuple[set[str], set[str]]:
 141:     """
 142:     Yosys FF cell 포트 정보에서 clock/reset 신호 직접 추출.
 143:     
 144:     구조적 분석을 통해 신뢰도 높은 식별:
 145:     - $dff, $adff, $sdff 등의 CLK 포트 → clock
 146:     - ARST, SRST 포트 → reset
 147:     """
 148:     clock_nets: set[str] = set()
 149:     reset_nets: set[str] = set()
 150:     
 151:     # FF cell 타입 정의
 152:     ff_cell_types = {
 153:         "$dff", "$adff", "$sdff",
 154:         "$dffe", "$sdffe", "$aldff", "$aldffe",
 155:     }
 156:     
 157:     for cell in cells:
 158:         if cell.type not in ff_cell_types:
 159:             continue
 160:         
 161:         # CLK 포트 찾기
 162:         if "CLK" in cell.connections:
 163:             clk_wids = cell.connections["CLK"]
 164:             for wid in clk_wids:
 165:                 w = get_wire(wires, wid)
 166:                 if w and w.name:
 167:                     clock_nets.add(w.name)
 168:         
 169:         # 비동기 리셋 포트 (ARST, ARST_N 등)
 170:         async_reset_ports = {"ARST", "ARST_N", "NRST", "NRESET"}
 171:         for port in async_reset_ports:
 172:             if port in cell.connections:
 173:                 rst_wids = cell.connections[port]
 174:                 for wid in rst_wids:
 175:                     w = get_wire(wires, wid)
 176:                     if w and w.name:
 177:                         reset_nets.add(w.name)
 178:         
 179:         # 동기 리셋 포트 (SRST, SRST_N 등)
 180:         sync_reset_ports = {"SRST", "SRST_N", "SR", "R", "RST"}
 181:         for port in sync_reset_ports:
 182:             if port in cell.connections:
 183:                 rst_wids = cell.connections[port]
 184:                 for wid in rst_wids:
 185:                     w = get_wire(wires, wid)
 186:                     if w and w.name:
 187:                         reset_nets.add(w.name)
 188:     
 189:     return clock_nets, reset_nets
 190: 
 191: 
 192: def detect_clock_reset_signals(
 193:     nodes: Dict[str, DKGNode],
 194:     edges: Dict[str, DKGEdge],
 195:     cells: List[CellIR],
 196:     wires: Dict[int, Wire],
 197: ) -> Tuple[set[str], set[str]]:
 198:     """
 199:     Clock/Reset 신호 식별 (다단계 우선순위).
 200:     
 201:     1️⃣ 구조적 분석: FF cell 포트 정보 (높은 신뢰도)
 202:     2️⃣ 신호 분석: edge의 flow 정보
 203:     3️⃣ 이름 기반 휴리스틱: 패턴 매칭 (낮은 신뢰도)
 204:     """
 205:     # Stage 1: 구조적 분석 (FF cell 포트)
 206:     clock_nets, reset_nets = detect_clock_reset_from_ff_cells(cells, wires)
 207:     
 208:     # Stage 2: 엣지 신호 이름 기반 (추론)
 209:     for e in edges.values():
 210:         if is_clock_name(e.signal_name):
 211:             clock_nets.add(e.signal_name)
 212:         if is_reset_name(e.signal_name):
 213:             reset_nets.add(e.signal_name)
 214:     
 215:     # Stage 3: FF 입력 신호 확인 (구조적 재검증)
 216:     for n in nodes.values():
 217:         if n.entity_class != EntityClass.FLIP_FLOP:
 218:             continue
 219:         for eid in n.in_edges:
 220:             e = edges[eid]
 221:             # 이미 식별된 것은 확인, 아니면 추가 확인
 222:             if is_clock_name(e.signal_name) and e.signal_name not in clock_nets:
 223:                 clock_nets.add(e.signal_name)
 224:             if is_reset_name(e.signal_name) and e.signal_name not in reset_nets:
 225:                 reset_nets.add(e.signal_name)
 226:     
 227:     return clock_nets, reset_nets
 228: 
 229: 
 230: def assign_edge_flow_types(
 231:     nodes: Dict[str, DKGNode],
 232:     edges: Dict[str, DKGEdge],
 233:     clock_nets: set[str],
 234:     reset_nets: set[str],
 235: ) -> None:
 236:     for e in edges.values():
 237:         if e.signal_name in clock_nets:
 238:             e.flow_type = EdgeFlowType.CLOCK_TREE
 239:             continue
 240:         if e.signal_name in reset_nets:
 241:             e.flow_type = EdgeFlowType.ASYNC_RESET
 242:             continue
 243: 
 244:         src = nodes[e.src_node]
 245:         dst = nodes[e.dst_node]
 246: 
 247:         if src.entity_class == EntityClass.FLIP_FLOP:
 248:             e.flow_type = EdgeFlowType.SEQ_LAUNCH
 249:         elif dst.entity_class == EntityClass.FLIP_FLOP:
 250:             e.flow_type = EdgeFlowType.SEQ_CAPTURE
 251:         else:
 252:             e.flow_type = EdgeFlowType.COMBINATIONAL
 253: 
 254: 
 255: def assign_clock_domains(
 256:     nodes: Dict[str, DKGNode],
 257:     edges: Dict[str, DKGEdge],
 258:     clock_nets: set[str],
 259: ) -> None:
 260:     for n in nodes.values():
 261:         if n.entity_class != EntityClass.FLIP_FLOP:
 262:             continue
 263:         for eid in n.in_edges:
 264:             e = edges[eid]
 265:             if e.signal_name in clock_nets:
 266:                 n.clock_domain = e.signal_name
 267:                 break
 268: 
 269: 
 270: def merge_bit_edges_to_bus(edges: Dict[str, DKGEdge]) -> Dict[str, DKGEdge]:
 271:     groups: Dict[Tuple[str, str, RelationType, EdgeFlowType, str], List[Tuple[Optional[int], DKGEdge]]] = defaultdict(list)
 272: 
 273:     for e in edges.values():
 274:         base, bit = split_signal_bit(e.signal_name)
 275:         key = (e.src_node, e.dst_node, e.relation_type, e.flow_type, base)
 276:         groups[key].append((bit, e))
 277: 
 278:     new_edges: Dict[str, DKGEdge] = {}
 279:     new_eid = 0
 280: 
 281:     for key, items in groups.items():
 282:         _, _, _, _, base = key
 283: 
 284:         if all(bit is None for bit, _ in items):
 285:             for _, e in items:
 286:                 new_edges[e.edge_id] = e
 287:             continue
 288: 
 289:         items.sort(key=lambda x: (-1 if x[0] is None else x[0]))
 290: 
 291:         current_bucket: List[Tuple[Optional[int], DKGEdge]] = []
 292:         prev_bit: Optional[int] = None
 293: 
 294:         def flush_bucket(bucket: List[Tuple[Optional[int], DKGEdge]]) -> None:
 295:             nonlocal new_eid
 296:             if not bucket:
 297:                 return
 298: 
 299:             bits = [b for b, _ in bucket if b is not None]
 300:             edges_in_bucket = [e for _, e in bucket]
 301: 
 302:             if len(bits) <= 1:
 303:                 e = edges_in_bucket[0]
 304:                 new_edges[e.edge_id] = e
 305:                 return
 306: 
 307:             msb = max(bits)
 308:             lsb = min(bits)
 309:             base_edge = edges_in_bucket[0]
 310:             merged = DKGEdge(
 311:                 edge_id=f"bus_e{new_eid}",
 312:                 src_node=base_edge.src_node,
 313:                 dst_node=base_edge.dst_node,
 314:                 relation_type=base_edge.relation_type,
 315:                 flow_type=base_edge.flow_type,
 316:                 signal_name=f"{base}[{msb}:{lsb}]",
 317:                 canonical_name=base_edge.canonical_name,
 318:                 bit_range=(msb, lsb),
 319:                 net_id=base_edge.net_id,
 320:                 driver_type=base_edge.driver_type,
 321:                 fanout_count=base_edge.fanout_count,
 322:                 clock_signal=base_edge.clock_signal,
 323:                 reset_signal=base_edge.reset_signal,
 324:                 clock_domain_id=base_edge.clock_domain_id,
 325:                 timing_exception=base_edge.timing_exception,
 326:                 delay=base_edge.delay,
 327:                 arrival_time=base_edge.arrival_time,
 328:                 required_time=base_edge.required_time,
 329:                 slack=base_edge.slack,
 330:                 attributes=dict(base_edge.attributes),
 331:                 provenances=[],
 332:                 primary_provenance=None,
 333:             )
 334: 
 335:             primary, provs = merge_provenances_edges(edges_in_bucket)
 336:             merged.provenances = provs
 337:             merged.primary_provenance = primary
 338:             merged.attributes["merged_bits"] = sorted(bits)
 339: 
 340:             new_edges[merged.edge_id] = merged
 341:             new_eid += 1
 342: 
 343:         for bit, e in items:
 344:             if bit is None:
 345:                 flush_bucket(current_bucket)
 346:                 current_bucket = []
 347:                 new_edges[e.edge_id] = e
 348:                 prev_bit = None
 349:                 continue
 350: 
 351:             if prev_bit is None or bit == prev_bit - 1:
 352:                 current_bucket.append((bit, e))
 353:             else:
 354:                 flush_bucket(current_bucket)
 355:                 current_bucket = [(bit, e)]
 356: 
 357:             prev_bit = bit
 358: 
 359:         flush_bucket(current_bucket)
 360: 
 361:     return new_edges
 362: 
 363: 
 364: def reindex_node_edges(nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge]) -> None:
 365:     for n in nodes.values():
 366:         n.in_edges = []
 367:         n.out_edges = []
 368: 
 369:     for e in edges.values():
 370:         nodes[e.src_node].out_edges.append(e.edge_id)
 371:         nodes[e.dst_node].in_edges.append(e.edge_id)
 372: 
 373: 
 374: def build_nodes_and_edges(
 375:     wires: Dict[int, Wire],
 376:     cells: List[CellIR],
 377: ) -> Tuple[Dict[str, DKGNode], Dict[str, DKGEdge]]:
 378:     connect_wires_to_cells(wires, cells)
 379: 
 380:     nodes: Dict[str, DKGNode] = {}
 381:     for cell in cells:
 382:         node_id = make_node_id(cell)
 383:         node = DKGNode(
 384:             node_id=node_id,
 385:             entity_class=map_cell_type(cell.type),
 386:             hier_path=cell.module,
 387:             local_name=cell.name,
 388:         )
 389:         node.canonical_name = make_node_canonical_name(node)
 390: 
 391:         file, line = parse_src(cell.src)
 392:         prov = Provenance(
 393:             origin_file=file,
 394:             origin_line=line,
 395:             tool_stage="rtl",
 396:             confidence="exact",
 397:         )
 398:         add_provenance(node, prov, make_primary=True)
 399:         nodes[node_id] = node
 400: 
 401:     edges: Dict[str, DKGEdge] = {}
 402:     eid = 0
 403: 
 404:     for w in wires.values():
 405:         for src in w.drivers:
 406:             for dst in w.loads:
 407:                 edge_id = f"e{eid}"
 408:                 eid += 1
 409: 
 410:                 edge = DKGEdge(
 411:                     edge_id=edge_id,
 412:                     src_node=src,
 413:                     dst_node=dst,
 414:                     relation_type=RelationType.DATA,
 415:                     flow_type=EdgeFlowType.COMBINATIONAL,
 416:                     signal_name=w.name or f"wire_{w.wire_id}",
 417:                     canonical_name=f"{src}->{dst}",
 418:                 )
 419: 
 420:                 file, line = parse_src(w.src)
 421:                 prov = Provenance(
 422:                     origin_file=file,
 423:                     origin_line=line,
 424:                     tool_stage="rtl",
 425:                     confidence="exact",
 426:                 )
 427:                 add_provenance(edge, prov, make_primary=True)
 428: 
 429:                 edges[edge_id] = edge
 430: 
 431:     edges = merge_bit_edges_to_bus(edges)
 432: 
 433:     new_edges: Dict[str, DKGEdge] = {}
 434:     for e in edges.values():
 435:         new_id = make_edge_id(e)
 436:         e.edge_id = new_id
 437:         new_edges[new_id] = e
 438: 
 439:     edges = new_edges
 440:     reindex_node_edges(nodes, edges)
 441: 
 442:     clock_nets, reset_nets = detect_clock_reset_signals(nodes, edges, cells, wires)
 443:     assign_clock_domains(nodes, edges, clock_nets)
 444:     assign_edge_flow_types(nodes, edges, clock_nets, reset_nets)
 445: 
 446:     return nodes, edges



FILE: dkg\graph_metadata.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass, field
   4: from typing import Any, Dict, Optional
   5: 
   6: from .stages import FieldSource, ParsingStage
   7: 
   8: 
   9: @dataclass
  10: class FieldMetadata:
  11:     """각 필드 값의 메타데이터"""
  12:     
  13:     value: Any
  14:     source: FieldSource
  15:     stage: ParsingStage
  16:     origin_file: Optional[str] = None
  17:     origin_line: Optional[int] = None
  18:     timestamp: Optional[float] = None  # 업데이트 시각
  19: 
  20: 
  21: @dataclass
  22: class NodeMetadata:
  23:     """노드의 모든 필드에 대한 메타데이터"""
  24:     
  25:     fields: Dict[str, FieldMetadata] = field(default_factory=dict)
  26:     
  27:     def get(self, field_name: str, default: Any = None) -> Any:
  28:         """필드 값 반환"""
  29:         if field_name in self.fields:
  30:             return self.fields[field_name].value
  31:         return default
  32:     
  33:     def get_source(self, field_name: str) -> Optional[FieldSource]:
  34:         """필드의 출처 반환"""
  35:         if field_name in self.fields:
  36:             return self.fields[field_name].source
  37:         return None
  38:     
  39:     def set(
  40:         self,
  41:         field_name: str,
  42:         value: Any,
  43:         source: FieldSource,
  44:         stage: ParsingStage,
  45:         origin_file: Optional[str] = None,
  46:         origin_line: Optional[int] = None,
  47:     ) -> None:
  48:         """필드 값 설정"""
  49:         self.fields[field_name] = FieldMetadata(
  50:             value=value,
  51:             source=source,
  52:             stage=stage,
  53:             origin_file=origin_file,
  54:             origin_line=origin_line,
  55:         )
  56:     
  57:     def should_update(self, field_name: str, new_source: FieldSource) -> bool:
  58:         """필드를 업데이트해야 하는지 판단"""
  59:         from .stages import should_update_field
  60:         
  61:         current_source = self.get_source(field_name)
  62:         return should_update_field(current_source, new_source)
  63: 
  64: 
  65: @dataclass
  66: class EdgeMetadata:
  67:     """엣지의 모든 필드에 대한 메타데이터"""
  68:     
  69:     fields: Dict[str, FieldMetadata] = field(default_factory=dict)
  70:     
  71:     def get(self, field_name: str, default: Any = None) -> Any:
  72:         if field_name in self.fields:
  73:             return self.fields[field_name].value
  74:         return default
  75:     
  76:     def get_source(self, field_name: str) -> Optional[FieldSource]:
  77:         if field_name in self.fields:
  78:             return self.fields[field_name].source
  79:         return None
  80:     
  81:     def set(
  82:         self,
  83:         field_name: str,
  84:         value: Any,
  85:         source: FieldSource,
  86:         stage: ParsingStage,
  87:         origin_file: Optional[str] = None,
  88:         origin_line: Optional[int] = None,
  89:     ) -> None:
  90:         self.fields[field_name] = FieldMetadata(
  91:             value=value,
  92:             source=source,
  93:             stage=stage,
  94:             origin_file=origin_file,
  95:             origin_line=origin_line,
  96:         )
  97:     
  98:     def should_update(self, field_name: str, new_source: FieldSource) -> bool:
  99:         from .stages import should_update_field
 100:         
 101:         current_source = self.get_source(field_name)
 102:         return should_update_field(current_source, new_source)



FILE: dkg\graph_updater.py

   1: from __future__ import annotations
   2: 
   3: from typing import Any, Dict, Optional
   4: 
   5: from .graph import DKGEdge, DKGNode
   6: from .graph_metadata import EdgeMetadata, NodeMetadata
   7: from .stages import FieldSource, ParsingStage
   8: 
   9: 
  10: class GraphUpdater:
  11:     """
  12:     그래프를 점진적으로 업데이트하는 엔진.
  13:     각 파싱 stage가 기존 그래프에 새 정보를 merge할 때 사용.
  14:     """
  15:     
  16:     def __init__(
  17:         self,
  18:         nodes: Dict[str, DKGNode],
  19:         edges: Dict[str, DKGEdge],
  20:     ):
  21:         self.nodes = nodes
  22:         self.edges = edges
  23:         
  24:         # 메타데이터 저장소 (node_id/edge_id -> metadata)
  25:         self.node_metadata: Dict[str, NodeMetadata] = {
  26:             nid: NodeMetadata() for nid in nodes
  27:         }
  28:         self.edge_metadata: Dict[str, EdgeMetadata] = {
  29:             eid: EdgeMetadata() for eid in edges
  30:         }
  31:     
  32:     def update_node_field(
  33:         self,
  34:         node_id: str,
  35:         field_name: str,
  36:         value: Any,
  37:         source: FieldSource,
  38:         stage: ParsingStage,
  39:         origin_file: Optional[str] = None,
  40:         origin_line: Optional[int] = None,
  41:     ) -> bool:
  42:         """
  43:         노드 필드를 업데이트.
  44:         
  45:         Returns:
  46:             True if updated, False if skipped (lower priority)
  47:         """
  48:         if node_id not in self.nodes:
  49:             return False
  50:         
  51:         meta = self.node_metadata[node_id]
  52:         
  53:         if not meta.should_update(field_name, source):
  54:             return False
  55:         
  56:         # 메타데이터 업데이트
  57:         meta.set(field_name, value, source, stage, origin_file, origin_line)
  58:         
  59:         # 실제 노드 객체 업데이트
  60:         if hasattr(self.nodes[node_id], field_name):
  61:             setattr(self.nodes[node_id], field_name, value)
  62:         
  63:         return True
  64:     
  65:     def update_edge_field(
  66:         self,
  67:         edge_id: str,
  68:         field_name: str,
  69:         value: Any,
  70:         source: FieldSource,
  71:         stage: ParsingStage,
  72:         origin_file: Optional[str] = None,
  73:         origin_line: Optional[int] = None,
  74:     ) -> bool:
  75:         """엣지 필드를 업데이트"""
  76:         if edge_id not in self.edges:
  77:             return False
  78:         
  79:         meta = self.edge_metadata[edge_id]
  80:         
  81:         if not meta.should_update(field_name, source):
  82:             return False
  83:         
  84:         meta.set(field_name, value, source, stage, origin_file, origin_line)
  85:         
  86:         if hasattr(self.edges[edge_id], field_name):
  87:             setattr(self.edges[edge_id], field_name, value)
  88:         
  89:         return True
  90:     
  91:     def batch_update_clock_domains(
  92:         self,
  93:         clock_assignments: Dict[str, str],  # node_id -> clock_domain
  94:         source: FieldSource,
  95:         stage: ParsingStage,
  96:         origin_file: Optional[str] = None,
  97:     ) -> int:
  98:         """클럭 도메인 일괄 업데이트"""
  99:         count = 0
 100:         for node_id, clock_domain in clock_assignments.items():
 101:             if self.update_node_field(
 102:                 node_id, "clock_domain", clock_domain, source, stage, origin_file
 103:             ):
 104:                 count += 1
 105:         return count
 106:     
 107:     def batch_update_timing_exceptions(
 108:         self,
 109:         exceptions: Dict[str, str],  # edge_id -> exception_type
 110:         source: FieldSource,
 111:         stage: ParsingStage,
 112:         origin_file: Optional[str] = None,
 113:     ) -> int:
 114:         """타이밍 예외 일괄 업데이트"""
 115:         count = 0
 116:         for edge_id, exception_type in exceptions.items():
 117:             if self.update_edge_field(
 118:                 edge_id, "timing_exception", exception_type, source, stage, origin_file
 119:             ):
 120:                 count += 1
 121:         return count
 122:     
 123:     def get_field_history(self, node_id: str, field_name: str) -> Optional[list]:
 124:         """필드의 변경 이력 반환 (향후 확장용)"""
 125:         # TODO: 이력 추적이 필요하면 metadata에 history 추가
 126:         pass
 127:     
 128:     def export_metadata_summary(self) -> dict:
 129:         """메타데이터 요약 반환 (디버깅/캐싱 용)"""
 130:         return {
 131:             "nodes": {
 132:                 nid: {
 133:                     field: {
 134:                         "source": meta.source.value,
 135:                         "stage": meta.stage.value,
 136:                     }
 137:                     for field, meta in nm.fields.items()
 138:                 }
 139:                 for nid, nm in self.node_metadata.items()
 140:             },
 141:             "edges": {
 142:                 eid: {
 143:                     field: {
 144:                         "source": meta.source.value,
 145:                         "stage": meta.stage.value,
 146:                     }
 147:                     for field, meta in em.fields.items()
 148:                 }
 149:                 for eid, em in self.edge_metadata.items()
 150:             },
 151:         }



FILE: dkg\graphcache\graph_version.py

   1: from dataclasses import dataclass
   2: from typing import Optional
   3: @dataclass
   4: class GraphVersion:
   5:     rtl_hash: str
   6:     constraint_hash: Optional[str]
   7:     timing_hash: Optional[str]
   8:     policy_versions: dict
   9: # 캐싱을 위한 그래프 버전 정보
  10: # TODO: 향후 그래프 구성 요소별 해시 추가 고려


FILE: dkg\graphcache\snapshot.py

   1: """
   2: 그래프 스냅샷 저장/로딩 (읽기 전용 캐싱용)
   3: 
   4: 메타데이터는 제외하고 필수 데이터만 JSON으로 저장:
   5: - DKG 그래프 (nodes + edges)
   6: - SuperGraph (supernodes + superedges + node_to_super)
   7: - GraphVersion (메타데이터)
   8: """
   9: from __future__ import annotations
  10: 
  11: import json
  12: from dataclasses import asdict, dataclass
  13: from pathlib import Path
  14: from typing import Any, Dict, Optional, Set, Tuple
  15: 
  16: from ..graph import DKGEdge, DKGNode, EdgeFlowType, EntityClass, RelationType
  17: from ..provenance import Provenance
  18: from ..supergraph import SuperClass, SuperEdge, SuperGraph, SuperNode
  19: from .graph_version import GraphVersion
  20: 
  21: 
  22: @dataclass
  23: class GraphSnapshot:
  24:     """전체 그래프 스냅샷"""
  25:     version: GraphVersion
  26:     dkg_nodes: Dict[str, DKGNode]
  27:     dkg_edges: Dict[str, DKGEdge]
  28:     supergraph: Optional[SuperGraph] = None
  29: 
  30: 
  31: def _serialize_node(node: DKGNode) -> dict:
  32:     """DKGNode를 JSON 직렬화 가능한 dict로 변환 (얕게)"""
  33:     return {
  34:         "node_id": node.node_id,
  35:         "entity_class": node.entity_class.value,
  36:         "hier_path": node.hier_path,
  37:         "local_name": node.local_name,
  38:         "canonical_name": node.canonical_name,
  39:         "short_alias": node.short_alias,
  40:         "parameters": node.parameters,
  41:         "attributes": node.attributes,
  42:         "clock_domain": node.clock_domain,
  43:         "arrival_time": node.arrival_time,
  44:         "required_time": node.required_time,
  45:         "slack": node.slack,
  46:         "in_edges": node.in_edges,
  47:         "out_edges": node.out_edges,
  48:         # provenance는 얕게: 기본 정보만
  49:         "provenances": [
  50:             {
  51:                 "origin_file": p.origin_file,
  52:                 "origin_line": p.origin_line,
  53:                 "tool_stage": p.tool_stage,
  54:                 "confidence": p.confidence,
  55:             }
  56:             for p in node.provenances
  57:         ] if node.provenances else [],
  58:         "primary_provenance": {
  59:             "origin_file": node.primary_provenance.origin_file,
  60:             "origin_line": node.primary_provenance.origin_line,
  61:             "tool_stage": node.primary_provenance.tool_stage,
  62:             "confidence": node.primary_provenance.confidence,
  63:         } if node.primary_provenance else None,
  64:     }
  65: 
  66: 
  67: def _deserialize_node(data: dict) -> DKGNode:
  68:     """dict에서 DKGNode 복원"""
  69:     from ..stages import ParsingStage
  70:     
  71:     # provenance 복원
  72:     provenances = []
  73:     if data.get("provenances"):
  74:         for p in data["provenances"]:
  75:             provenances.append(Provenance(
  76:                 origin_file=p.get("origin_file"),
  77:                 origin_line=p.get("origin_line"),
  78:                 tool_stage=p.get("tool_stage", "rtl"),
  79:                 confidence=p.get("confidence", "exact"),
  80:             ))
  81:     
  82:     primary_provenance = None
  83:     if data.get("primary_provenance"):
  84:         p = data["primary_provenance"]
  85:         primary_provenance = Provenance(
  86:             origin_file=p.get("origin_file"),
  87:             origin_line=p.get("origin_line"),
  88:             tool_stage=p.get("tool_stage", "rtl"),
  89:             confidence=p.get("confidence", "exact"),
  90:         )
  91:     
  92:     return DKGNode(
  93:         node_id=data["node_id"],
  94:         entity_class=EntityClass(data["entity_class"]),
  95:         hier_path=data["hier_path"],
  96:         local_name=data["local_name"],
  97:         canonical_name=data.get("canonical_name"),
  98:         short_alias=data.get("short_alias"),
  99:         parameters=data.get("parameters", {}),
 100:         attributes=data.get("attributes", {}),
 101:         clock_domain=data.get("clock_domain"),
 102:         arrival_time=data.get("arrival_time"),
 103:         required_time=data.get("required_time"),
 104:         slack=data.get("slack"),
 105:         in_edges=data.get("in_edges", []),
 106:         out_edges=data.get("out_edges", []),
 107:         provenances=provenances,
 108:         primary_provenance=primary_provenance,
 109:     )
 110: 
 111: 
 112: def _serialize_edge(edge: DKGEdge) -> dict:
 113:     """DKGEdge를 JSON 직렬화 가능한 dict로 변환"""
 114:     return {
 115:         "edge_id": edge.edge_id,
 116:         "src_node": edge.src_node,
 117:         "dst_node": edge.dst_node,
 118:         "relation_type": edge.relation_type.value,
 119:         "flow_type": edge.flow_type.value,
 120:         "signal_name": edge.signal_name,
 121:         "canonical_name": edge.canonical_name,
 122:         "bit_range": list(edge.bit_range) if edge.bit_range else None,
 123:         "net_id": edge.net_id,
 124:         "driver_type": edge.driver_type,
 125:         "fanout_count": edge.fanout_count,
 126:         "clock_signal": edge.clock_signal,
 127:         "reset_signal": edge.reset_signal,
 128:         "clock_domain_id": edge.clock_domain_id,
 129:         "timing_exception": edge.timing_exception,
 130:         "parameters": edge.parameters,
 131:         "delay": edge.delay,
 132:         "arrival_time": edge.arrival_time,
 133:         "required_time": edge.required_time,
 134:     }
 135: 
 136: 
 137: def _deserialize_edge(data: dict) -> DKGEdge:
 138:     """dict에서 DKGEdge 복원"""
 139:     return DKGEdge(
 140:         edge_id=data["edge_id"],
 141:         src_node=data["src_node"],
 142:         dst_node=data["dst_node"],
 143:         relation_type=RelationType(data["relation_type"]),
 144:         flow_type=EdgeFlowType(data["flow_type"]),
 145:         signal_name=data["signal_name"],
 146:         canonical_name=data["canonical_name"],
 147:         bit_range=tuple(data["bit_range"]) if data.get("bit_range") else None,
 148:         net_id=data.get("net_id"),
 149:         driver_type=data.get("driver_type"),
 150:         fanout_count=data.get("fanout_count"),
 151:         clock_signal=data.get("clock_signal"),
 152:         reset_signal=data.get("reset_signal"),
 153:         clock_domain_id=data.get("clock_domain_id"),
 154:         timing_exception=data.get("timing_exception"),
 155:         parameters=data.get("parameters", {}),
 156:         delay=data.get("delay"),
 157:         arrival_time=data.get("arrival_time"),
 158:         required_time=data.get("required_time"),
 159:     )
 160: 
 161: 
 162: def _serialize_supernode(sn: SuperNode) -> dict:
 163:     """SuperNode를 JSON 직렬화"""
 164:     return {
 165:         "node_id": sn.node_id,
 166:         "super_class": sn.super_class.value,
 167:         "member_nodes": list(sn.member_nodes),
 168:         "member_edges": list(sn.member_edges),
 169:         "aggregated_attrs": sn.aggregated_attrs,
 170:         "canonical_name": sn.canonical_name,
 171:         "display_name": sn.display_name,
 172:         "provenances": [
 173:             {"origin_file": p.origin_file, "origin_line": p.origin_line, "tool_stage": p.tool_stage}
 174:             for p in sn.provenances
 175:         ] if sn.provenances else [],
 176:     }
 177: 
 178: 
 179: def _deserialize_supernode(data: dict) -> SuperNode:
 180:     """dict에서 SuperNode 복원"""
 181:     provenances = []
 182:     if data.get("provenances"):
 183:         for p in data["provenances"]:
 184:             provenances.append(Provenance(
 185:                 origin_file=p.get("origin_file"),
 186:                 origin_line=p.get("origin_line"),
 187:                 tool_stage=p.get("tool_stage", "rtl"),
 188:             ))
 189:     
 190:     return SuperNode(
 191:         node_id=data["node_id"],
 192:         super_class=SuperClass(data["super_class"]),
 193:         member_nodes=set(data["member_nodes"]),
 194:         member_edges=set(data["member_edges"]),
 195:         aggregated_attrs=data.get("aggregated_attrs", {}),
 196:         canonical_name=data.get("canonical_name"),
 197:         display_name=data.get("display_name"),
 198:         provenances=provenances,
 199:     )
 200: 
 201: 
 202: def _serialize_superedge(se: SuperEdge) -> dict:
 203:     """SuperEdge를 JSON 직렬화"""
 204:     return {
 205:         "edge_id": se.edge_id,
 206:         "src_node": se.src_node,
 207:         "dst_node": se.dst_node,
 208:         "member_edges": list(se.member_edges),
 209:         "member_nodes": list(se.member_nodes),
 210:         "relation_types": [rt.value for rt in se.relation_types],
 211:         "flow_types": [ft.value for ft in se.flow_types],
 212:         "canonical_name": se.canonical_name,
 213:         "display_name": se.display_name,
 214:         "provenances": [
 215:             {"origin_file": p.origin_file, "origin_line": p.origin_line, "tool_stage": p.tool_stage}
 216:             for p in se.provenances
 217:         ] if se.provenances else [],
 218:     }
 219: 
 220: 
 221: def _deserialize_superedge(data: dict) -> SuperEdge:
 222:     """dict에서 SuperEdge 복원"""
 223:     provenances = []
 224:     if data.get("provenances"):
 225:         for p in data["provenances"]:
 226:             provenances.append(Provenance(
 227:                 origin_file=p.get("origin_file"),
 228:                 origin_line=p.get("origin_line"),
 229:                 tool_stage=p.get("tool_stage", "rtl"),
 230:             ))
 231:     
 232:     return SuperEdge(
 233:         edge_id=data["edge_id"],
 234:         src_node=data["src_node"],
 235:         dst_node=data["dst_node"],
 236:         member_edges=set(data["member_edges"]),
 237:         member_nodes=set(data["member_nodes"]),
 238:         relation_types={RelationType(rt) for rt in data["relation_types"]},
 239:         flow_types={EdgeFlowType(ft) for ft in data["flow_types"]},
 240:         canonical_name=data.get("canonical_name"),
 241:         display_name=data.get("display_name"),
 242:         provenances=provenances,
 243:     )
 244: 
 245: 
 246: def save_snapshot(
 247:     snapshot: GraphSnapshot,
 248:     filepath: Path | str,
 249:     indent: Optional[int] = None,
 250: ) -> None:
 251:     """스냅샷을 JSON 파일로 저장"""
 252:     filepath = Path(filepath)
 253:     filepath.parent.mkdir(parents=True, exist_ok=True)
 254:     
 255:     # 직렬화
 256:     data = {
 257:         "version": {
 258:             "rtl_hash": snapshot.version.rtl_hash,
 259:             "constraint_hash": snapshot.version.constraint_hash,
 260:             "timing_hash": snapshot.version.timing_hash,
 261:             "policy_versions": snapshot.version.policy_versions,
 262:         },
 263:         "dkg": {
 264:             "nodes": {
 265:                 node_id: _serialize_node(node)
 266:                 for node_id, node in snapshot.dkg_nodes.items()
 267:             },
 268:             "edges": {
 269:                 edge_id: _serialize_edge(edge)
 270:                 for edge_id, edge in snapshot.dkg_edges.items()
 271:             },
 272:         },
 273:     }
 274:     
 275:     # SuperGraph가 있으면 추가
 276:     if snapshot.supergraph:
 277:         sg = snapshot.supergraph
 278:         data["supergraph"] = {
 279:             "super_nodes": {
 280:                 node_id: _serialize_supernode(sn)
 281:                 for node_id, sn in sg.super_nodes.items()
 282:             },
 283:             "super_edges": {
 284:                 f"{src}→{dst}": _serialize_superedge(se)
 285:                 for (src, dst), se in sg.super_edges.items()
 286:             },
 287:             "node_to_super": sg.node_to_super,
 288:         }
 289:     
 290:     # JSON 저장
 291:     with open(filepath, "w", encoding="utf-8") as f:
 292:         json.dump(data, f, indent=indent, ensure_ascii=False)
 293: 
 294: 
 295: def load_snapshot(filepath: Path | str) -> GraphSnapshot:
 296:     """JSON 파일에서 스냅샷 로딩"""
 297:     filepath = Path(filepath)
 298:     
 299:     with open(filepath, "r", encoding="utf-8") as f:
 300:         data = json.load(f)
 301:     
 302:     # GraphVersion 복원
 303:     version_data = data["version"]
 304:     version = GraphVersion(
 305:         rtl_hash=version_data["rtl_hash"],
 306:         constraint_hash=version_data.get("constraint_hash"),
 307:         timing_hash=version_data.get("timing_hash"),
 308:         policy_versions=version_data.get("policy_versions", {}),
 309:     )
 310:     
 311:     # DKG 그래프 복원
 312:     dkg_data = data["dkg"]
 313:     dkg_nodes = {
 314:         node_id: _deserialize_node(node_data)
 315:         for node_id, node_data in dkg_data["nodes"].items()
 316:     }
 317:     dkg_edges = {
 318:         edge_id: _deserialize_edge(edge_data)
 319:         for edge_id, edge_data in dkg_data["edges"].items()
 320:     }
 321:     
 322:     # SuperGraph 복원 (있으면)
 323:     supergraph = None
 324:     if "supergraph" in data:
 325:         sg_data = data["supergraph"]
 326:         super_nodes = {
 327:             node_id: _deserialize_supernode(sn_data)
 328:             for node_id, sn_data in sg_data["super_nodes"].items()
 329:         }
 330:         super_edges = {
 331:             tuple(key.split("→")): _deserialize_superedge(se_data)
 332:             for key, se_data in sg_data["super_edges"].items()
 333:         }
 334:         node_to_super = sg_data["node_to_super"]
 335:         
 336:         supergraph = SuperGraph(
 337:             super_nodes=super_nodes,
 338:             super_edges=super_edges,
 339:             node_to_super=node_to_super,
 340:         )
 341:     
 342:     return GraphSnapshot(
 343:         version=version,
 344:         dkg_nodes=dkg_nodes,
 345:         dkg_edges=dkg_edges,
 346:         supergraph=supergraph,
 347:     )



FILE: dkg\ir.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass, field
   4: from typing import Optional
   5: 
   6: 
   7: @dataclass
   8: class Wire:
   9:     wire_id: int
  10:     name: str | None = None
  11:     drivers: list[str] = field(default_factory=list)
  12:     loads: list[str] = field(default_factory=list)
  13:     src: Optional[str] = None
  14: 
  15: 
  16: @dataclass
  17: class CellIR:
  18:     name: str
  19:     type: str
  20:     module: str
  21:     port_dirs: dict[str, str]
  22:     connections: dict[str, list[int]]
  23:     src: Optional[str] = None



FILE: dkg\parsers\bd_parser.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Dict
   5: 
   6: from ..graph import DKGEdge, DKGNode
   7: from ..graph_updater import GraphUpdater
   8: from ..stages import FieldSource, ParsingStage
   9: from . import ConstraintParser
  10: 
  11: 
  12: class BdParser(ConstraintParser):
  13:     """
  14:     BD (block design) parser for IP instance grouping seeds.
  15:     """
  16: 
  17:     def get_stage(self) -> ParsingStage:
  18:         return ParsingStage.BOARD
  19: 
  20:     def parse_and_update(
  21:         self,
  22:         filepath: str,
  23:         updater: GraphUpdater,
  24:         nodes: Dict[str, DKGNode],
  25:         edges: Dict[str, DKGEdge],
  26:     ) -> None:
  27:         with open(filepath, "r", encoding="utf-8") as f:
  28:             lines = f.readlines()
  29: 
  30:         for line_num, line in enumerate(lines, start=1):
  31:             raw = line.strip()
  32:             if not raw or raw.startswith("#"):
  33:                 continue
  34: 
  35:             if raw.startswith("create_bd_cell"):
  36:                 self._parse_create_bd_cell(raw, line_num, filepath, updater, nodes)
  37: 
  38:     def _parse_create_bd_cell(
  39:         self,
  40:         line: str,
  41:         line_num: int,
  42:         filepath: str,
  43:         updater: GraphUpdater,
  44:         nodes: Dict[str, DKGNode],
  45:     ) -> None:
  46:         match = re.search(r"create_bd_cell\s+-type\s+ip\s+-vlnv\s+(\S+)\s+(\S+)", line)
  47:         if not match:
  48:             return
  49: 
  50:         vlnv = match.group(1)
  51:         inst = match.group(2)
  52: 
  53:         for node_id, node in nodes.items():
  54:             candidates = [node.local_name, node.hier_path, node.canonical_name]
  55:             if not any(inst == cand or (cand and inst in cand) for cand in candidates):
  56:                 continue
  57: 
  58:             new_attrs = dict(node.attributes)
  59:             new_attrs["bd_ip"] = vlnv
  60:             new_attrs["bd_group"] = vlnv
  61: 
  62:             updater.update_node_field(
  63:                 node_id,
  64:                 "attributes",
  65:                 new_attrs,
  66:                 FieldSource.DECLARED,
  67:                 ParsingStage.BOARD,
  68:                 filepath,
  69:                 line_num,
  70:             )



FILE: dkg\parsers\parser_utils.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Iterable, List
   5: 
   6: 
   7: def _split_target_list(raw: str) -> List[str]:
   8:     text = raw.strip()
   9:     if text.startswith("{") and text.endswith("}"):
  10:         text = text[1:-1].strip()
  11:     parts = re.split(r"\s+", text)
  12:     return [p.strip('"') for p in parts if p.strip('"')]
  13: 
  14: 
  15: def extract_bracket_targets(line: str, object_types: Iterable[str]) -> List[str]:
  16:     targets: List[str] = []
  17:     pattern = r"\[get_(%s)\s+([^\]]+)\]" % "|".join(object_types)
  18:     for match in re.finditer(pattern, line):
  19:         targets.extend(_split_target_list(match.group(2)))
  20:     return targets
  21: 
  22: 
  23: def extract_option_targets(line: str, option: str, object_types: Iterable[str]) -> List[str]:
  24:     targets: List[str] = []
  25:     pattern = r"%s\s+\[get_(%s)\s+([^\]]+)\]" % (re.escape(option), "|".join(object_types))
  26:     for match in re.finditer(pattern, line):
  27:         targets.extend(_split_target_list(match.group(2)))
  28:     return targets
  29: 
  30: 
  31: def pattern_match(pattern: str, candidate: str) -> bool:
  32:     if not pattern:
  33:         return False
  34:     if "*" not in pattern and "?" not in pattern:
  35:         if pattern == candidate:
  36:             return True
  37:         if pattern in candidate or candidate in pattern:
  38:             return True
  39:     escaped = re.escape(pattern)
  40:     escaped = escaped.replace(r"\*", ".*").replace(r"\?", ".")
  41:     regex = re.compile(r"^%s$" % escaped)
  42:     return bool(regex.match(candidate))
  43: 
  44: 
  45: def match_any(patterns: Iterable[str], candidates: Iterable[str]) -> bool:
  46:     for pattern in patterns:
  47:         for cand in candidates:
  48:             if cand and pattern_match(pattern, cand):
  49:                 return True
  50:     return False



FILE: dkg\parsers\sdc_parser.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Dict
   5: 
   6: from ..graph import DKGEdge, DKGNode
   7: from ..graph_updater import GraphUpdater
   8: from ..stages import FieldSource, ParsingStage
   9: from . import ConstraintParser
  10: from .parser_utils import extract_option_targets, match_any
  11: 
  12: 
  13: class SdcParser(ConstraintParser):
  14:     """
  15:     SDC (Synopsys Design Constraints) 파서.
  16:     
  17:     처리하는 명령:
  18:     - create_clock: 클럭 정의
  19:     - set_input_delay / set_output_delay: I/O 타이밍
  20:     - set_false_path: false path 제약
  21:     - set_multicycle_path: multicycle 제약
  22:     - set_max_delay / set_min_delay: 지연 제약
  23:     """
  24:     
  25:     def get_stage(self) -> ParsingStage:
  26:         return ParsingStage.CONSTRAINTS
  27:     
  28:     def parse_and_update(
  29:         self,
  30:         filepath: str,
  31:         updater: GraphUpdater,
  32:         nodes: Dict[str, DKGNode],
  33:         edges: Dict[str, DKGEdge],
  34:     ) -> None:
  35:         with open(filepath, "r", encoding="utf-8") as f:
  36:             lines = f.readlines()
  37:         
  38:         for line_num, line in enumerate(lines, start=1):
  39:             line = line.strip()
  40:             
  41:             # create_clock 처리
  42:             if line.startswith("create_clock"):
  43:                 self._parse_create_clock(
  44:                     line, line_num, filepath, updater, nodes, edges
  45:                 )
  46:             
  47:             # set_false_path 처리
  48:             elif line.startswith("set_false_path"):
  49:                 self._parse_false_path(
  50:                     line, line_num, filepath, updater, nodes, edges
  51:                 )
  52:             
  53:             # set_multicycle_path 처리
  54:             elif line.startswith("set_multicycle_path"):
  55:                 self._parse_multicycle_path(
  56:                     line, line_num, filepath, updater, nodes, edges
  57:                 )
  58:     
  59:     def _parse_create_clock(
  60:         self,
  61:         line: str,
  62:         line_num: int,
  63:         filepath: str,
  64:         updater: GraphUpdater,
  65:         nodes: Dict[str, DKGNode],
  66:         edges: Dict[str, DKGEdge],
  67:     ) -> None:
  68:         """
  69:         create_clock 명령 파싱.
  70:         예: create_clock -name clk -period 10 [get_ports clk]
  71:         """
  72:         # 간단한 정규식 (실제로는 더 정교해야 함)
  73:         match = re.search(r"-name\s+(\w+)", line)
  74:         if not match:
  75:             return
  76:         
  77:         clock_name = match.group(1)
  78:         
  79:         # get_ports로 포트 이름 추출
  80:         port_match = re.search(r"get_ports\s+(\w+)", line)
  81:         if not port_match:
  82:             return
  83:         
  84:         port_name = port_match.group(1)
  85:         
  86:         # 해당 포트를 가진 노드들 찾기
  87:         for node_id, node in nodes.items():
  88:             if node.local_name == port_name:
  89:                 updater.update_node_field(
  90:                     node_id,
  91:                     "clock_domain",
  92:                     clock_name,
  93:                     FieldSource.DECLARED,
  94:                     ParsingStage.CONSTRAINTS,
  95:                     filepath,
  96:                     line_num,
  97:                 )
  98:         
  99:         # 해당 신호를 가진 엣지들도 업데이트
 100:         for edge_id, edge in edges.items():
 101:             if edge.signal_name == port_name:
 102:                 updater.update_edge_field(
 103:                     edge_id,
 104:                     "clock_signal",
 105:                     clock_name,
 106:                     FieldSource.DECLARED,
 107:                     ParsingStage.CONSTRAINTS,
 108:                     filepath,
 109:                     line_num,
 110:                 )
 111:     
 112:     def _parse_false_path(
 113:         self,
 114:         line: str,
 115:         line_num: int,
 116:         filepath: str,
 117:         updater: GraphUpdater,
 118:         nodes: Dict[str, DKGNode],
 119:         edges: Dict[str, DKGEdge],
 120:     ) -> None:
 121:         """
 122:         set_false_path 명령 파싱.
 123:         예: set_false_path -from [get_pins src/*] -to [get_pins dst/*]
 124:         """
 125:         from_patterns = extract_option_targets(line, "-from", ("ports", "pins", "cells"))
 126:         to_patterns = extract_option_targets(line, "-to", ("ports", "pins", "cells"))
 127: 
 128:         if not from_patterns and not to_patterns:
 129:             return
 130: 
 131:         for edge_id, edge in edges.items():
 132:             src_node = nodes.get(edge.src_node)
 133:             dst_node = nodes.get(edge.dst_node)
 134:             if not src_node or not dst_node:
 135:                 continue
 136: 
 137:             src_candidates = [src_node.local_name, src_node.hier_path, src_node.canonical_name]
 138:             dst_candidates = [dst_node.local_name, dst_node.hier_path, dst_node.canonical_name]
 139: 
 140:             src_match = True if not from_patterns else match_any(from_patterns, src_candidates)
 141:             dst_match = True if not to_patterns else match_any(to_patterns, dst_candidates)
 142: 
 143:             if src_match and dst_match:
 144:                 updater.update_edge_field(
 145:                     edge_id,
 146:                     "timing_exception",
 147:                     "false_path",
 148:                     FieldSource.DECLARED,
 149:                     ParsingStage.CONSTRAINTS,
 150:                     filepath,
 151:                     line_num,
 152:                 )
 153:     
 154:     def _parse_multicycle_path(
 155:         self,
 156:         line: str,
 157:         line_num: int,
 158:         filepath: str,
 159:         updater: GraphUpdater,
 160:         nodes: Dict[str, DKGNode],
 161:         edges: Dict[str, DKGEdge],
 162:     ) -> None:
 163:         """
 164:         set_multicycle_path 명령 파싱.
 165:         예: set_multicycle_path 2 -from [get_pins ...] -to [get_pins ...]
 166:         """
 167:         value_match = re.search(r"set_multicycle_path\s+(-?\d+)", line)
 168:         if not value_match:
 169:             return
 170: 
 171:         multicycle = int(value_match.group(1))
 172:         mc_type = None
 173:         if "-hold" in line:
 174:             mc_type = "hold"
 175:         elif "-setup" in line:
 176:             mc_type = "setup"
 177: 
 178:         from_patterns = extract_option_targets(line, "-from", ("ports", "pins", "cells"))
 179:         to_patterns = extract_option_targets(line, "-to", ("ports", "pins", "cells"))
 180: 
 181:         if not from_patterns and not to_patterns:
 182:             return
 183: 
 184:         for edge_id, edge in edges.items():
 185:             src_node = nodes.get(edge.src_node)
 186:             dst_node = nodes.get(edge.dst_node)
 187:             if not src_node or not dst_node:
 188:                 continue
 189: 
 190:             src_candidates = [src_node.local_name, src_node.hier_path, src_node.canonical_name]
 191:             dst_candidates = [dst_node.local_name, dst_node.hier_path, dst_node.canonical_name]
 192: 
 193:             src_match = True if not from_patterns else match_any(from_patterns, src_candidates)
 194:             dst_match = True if not to_patterns else match_any(to_patterns, dst_candidates)
 195: 
 196:             if not (src_match and dst_match):
 197:                 continue
 198: 
 199:             new_params = dict(edge.parameters)
 200:             existing = new_params.get("multicycle")
 201:             if existing is None or multicycle > existing:
 202:                 new_params["multicycle"] = multicycle
 203:             if mc_type:
 204:                 new_params["multicycle_type"] = mc_type
 205: 
 206:             updater.update_edge_field(
 207:                 edge_id,
 208:                 "parameters",
 209:                 new_params,
 210:                 FieldSource.DECLARED,
 211:                 ParsingStage.CONSTRAINTS,
 212:                 filepath,
 213:                 line_num,
 214:             )



FILE: dkg\parsers\tcl_parser.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Dict, Optional
   5: 
   6: from ..graph import DKGEdge, DKGNode
   7: from ..graph_updater import GraphUpdater
   8: from ..stages import FieldSource, ParsingStage
   9: from . import ConstraintParser
  10: 
  11: 
  12: class TclParser(ConstraintParser):
  13:     """
  14:     TCL floorplan parser.
  15: 
  16:     - design vs sim flag
  17:     - top scope
  18:     """
  19: 
  20:     def get_stage(self) -> ParsingStage:
  21:         return ParsingStage.FLOORPLAN
  22: 
  23:     def parse_and_update(
  24:         self,
  25:         filepath: str,
  26:         updater: GraphUpdater,
  27:         nodes: Dict[str, DKGNode],
  28:         edges: Dict[str, DKGEdge],
  29:     ) -> None:
  30:         with open(filepath, "r", encoding="utf-8") as f:
  31:             lines = f.readlines()
  32: 
  33:         top_scope: Optional[str] = None
  34:         design_context: Optional[str] = None
  35: 
  36:         for line in lines:
  37:             raw = line.strip()
  38:             if not raw or raw.startswith("#"):
  39:                 continue
  40: 
  41:             top_scope = top_scope or self._parse_top_scope(raw)
  42:             design_context = design_context or self._parse_design_context(raw)
  43: 
  44:         if not top_scope and not design_context:
  45:             return
  46: 
  47:         for node_id, node in nodes.items():
  48:             if top_scope and node.hier_path != top_scope:
  49:                 continue
  50: 
  51:             new_attrs = dict(node.attributes)
  52:             if top_scope:
  53:                 new_attrs["top_scope"] = top_scope
  54:             if design_context:
  55:                 new_attrs["design_context"] = design_context
  56: 
  57:             updater.update_node_field(
  58:                 node_id,
  59:                 "attributes",
  60:                 new_attrs,
  61:                 FieldSource.DECLARED,
  62:                 ParsingStage.FLOORPLAN,
  63:                 filepath,
  64:                 None,
  65:             )
  66: 
  67:     def _parse_top_scope(self, line: str) -> Optional[str]:
  68:         match = re.search(r"set_property\s+top\s+(\S+)", line)
  69:         if match:
  70:             return match.group(1)
  71: 
  72:         match = re.search(r"set\s+top_(?:module|scope)\s+(\S+)", line)
  73:         if match:
  74:             return match.group(1)
  75: 
  76:         return None
  77: 
  78:     def _parse_design_context(self, line: str) -> Optional[str]:
  79:         if "-simset" in line or "simulation" in line.lower():
  80:             return "sim"
  81:         if "-constrset" in line or "synth" in line.lower():
  82:             return "design"
  83: 
  84:         match = re.search(r"set_property\s+design_mode\s+(\S+)", line)
  85:         if match:
  86:             value = match.group(1).lower()
  87:             return "sim" if "sim" in value else "design"
  88: 
  89:         return None



FILE: dkg\parsers\timing_report_parser.py

   1: """
   2: 타이밍 리포트 파서 (Vivado/PrimeTime)
   3: 
   4: 타이밍 리포트에서 DKG 그래프 구축에 필요한 정보 추출:
   5: - Startpoint/Endpoint
   6: - 경로상의 모든 셀
   7: - 각 단계별 delay
   8: - 최종 slack
   9: - 클럭 도메인
  10: 
  11: ⚠️ 중요: 한 노드/엣지는 여러 타이밍 경로에 나타날 수 있음
  12: - Setup path와 Hold path가 다름
  13: - 여러 클럭 도메인 존재
  14: - 따라서 worst-case 값만 저장하고, 상세 정보는 메타데이터에 누적
  15: """
  16: from __future__ import annotations
  17: 
  18: import re
  19: from dataclasses import dataclass, field
  20: from pathlib import Path
  21: from typing import Dict, List, Optional
  22: 
  23: from ..graph import DKGEdge, DKGNode
  24: from ..graph_updater import GraphUpdater
  25: 
  26: 
  27: @dataclass
  28: class TimingStage:
  29:     """타이밍 경로의 한 단계"""
  30:     point: str              # 셀/핀 이름
  31:     incr_delay: float       # 증분 delay (ns)
  32:     cumulative_delay: float # 누적 delay (ns)
  33:     transition: str         # 'r' (rising) or 'f' (falling)
  34: 
  35: 
  36: @dataclass
  37: class TimingPath:
  38:     """하나의 타이밍 경로"""
  39:     startpoint: str
  40:     endpoint: str
  41:     clock: str
  42:     path_type: str  # 'Setup' or 'Hold'
  43:     
  44:     slack: Optional[float] = None
  45:     arrival_time: Optional[float] = None
  46:     required_time: Optional[float] = None
  47:     
  48:     stages: List[TimingStage] = field(default_factory=list)
  49: 
  50: 
  51: class TimingReportParser:
  52:     """타이밍 리포트 파서 (Vivado/PrimeTime 형식)"""
  53:     
  54:     def __init__(self):
  55:         self.paths: List[TimingPath] = []
  56:     
  57:     def parse_file(self, filepath: str | Path) -> List[TimingPath]:
  58:         """타이밍 리포트 파일 전체 파싱"""
  59:         filepath = Path(filepath)
  60:         with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
  61:             content = f.read()
  62:         
  63:         # "Startpoint:"로 시작하는 각 경로 섹션 분리
  64:         # Vivado: "Startpoint: ..."
  65:         # PrimeTime: "Point ..."로 시작하지만 헤더가 다름
  66:         
  67:         # Vivado 형식 감지
  68:         if 'Startpoint:' in content:
  69:             return self._parse_vivado_format(content)
  70:         else:
  71:             # TODO: PrimeTime 형식 지원
  72:             return []
  73:     
  74:     def _parse_vivado_format(self, content: str) -> List[TimingPath]:
  75:         """Vivado 타이밍 리포트 파싱"""
  76:         paths = []
  77:         
  78:         # "Startpoint:"로 구분
  79:         sections = re.split(r'\n(?=Startpoint:)', content)
  80:         
  81:         for section in sections:
  82:             if 'Startpoint:' not in section:
  83:                 continue
  84:             
  85:             path = self._parse_single_path(section)
  86:             if path:
  87:                 paths.append(path)
  88:         
  89:         self.paths = paths
  90:         return paths
  91:     
  92:     def _parse_single_path(self, section: str) -> Optional[TimingPath]:
  93:         """개별 타이밍 경로 파싱"""
  94:         path = TimingPath(
  95:             startpoint='',
  96:             endpoint='',
  97:             clock='',
  98:             path_type='Setup',
  99:         )
 100:         
 101:         # Startpoint 추출
 102:         # 예: "Startpoint: cpu/pc_reg[0] (rising edge-triggered flip-flop clocked by sys_clk)"
 103:         start_match = re.search(r'Startpoint:\s+(\S+)', section)
 104:         if not start_match:
 105:             return None
 106:         path.startpoint = start_match.group(1)
 107:         
 108:         # Endpoint 추출
 109:         end_match = re.search(r'Endpoint:\s+(\S+)', section)
 110:         if end_match:
 111:             path.endpoint = end_match.group(1)
 112:         
 113:         # Clock 추출
 114:         clock_match = re.search(r'clocked by (\w+)', section)
 115:         if clock_match:
 116:             path.clock = clock_match.group(1)
 117:         
 118:         # Path Type 추출
 119:         type_match = re.search(r'Path Type:\s+(\w+)', section)
 120:         if type_match:
 121:             path.path_type = type_match.group(1)
 122:         
 123:         # Slack 추출
 124:         # 예: "slack (MET)                                         9.37"
 125:         slack_match = re.search(r'slack.*?([-\d.]+)', section, re.IGNORECASE)
 126:         if slack_match:
 127:             path.slack = float(slack_match.group(1))
 128:         
 129:         # Arrival/Required time 추출
 130:         arrival_match = re.search(r'data arrival time\s+([\d.]+)', section)
 131:         if arrival_match:
 132:             path.arrival_time = float(arrival_match.group(1))
 133:         
 134:         required_match = re.search(r'data required time\s+([\d.]+)', section)
 135:         if required_match:
 136:             path.required_time = float(required_match.group(1))
 137:         
 138:         # 타이밍 테이블 파싱
 139:         # 형식:
 140:         #   Point                                    Incr       Path
 141:         #   --------------------------------------------------------
 142:         #   cpu/pc_reg[0]/Q (DFFQX1)                 0.15       0.65 r
 143:         #   cpu/decode_inst/U123/Y (AND2X1)          0.08       0.73 r
 144:         
 145:         table_pattern = r'Point\s+Incr\s+Path\s*\n\s*-+\s*\n(.*?)\n\s*data arrival time'
 146:         table_match = re.search(table_pattern, section, re.DOTALL)
 147:         
 148:         if table_match:
 149:             table_content = table_match.group(1)
 150:             path.stages = self._parse_timing_table(table_content)
 151:         
 152:         return path
 153:     
 154:     def _parse_timing_table(self, table_content: str) -> List[TimingStage]:
 155:         """타이밍 테이블 파싱"""
 156:         stages = []
 157:         lines = table_content.strip().split('\n')
 158:         
 159:         for line in lines:
 160:             line = line.strip()
 161:             if not line or line.startswith('-'):
 162:                 continue
 163:             
 164:             stage = self._parse_timing_line(line)
 165:             if stage:
 166:                 stages.append(stage)
 167:         
 168:         return stages
 169:     
 170:     def _parse_timing_line(self, line: str) -> Optional[TimingStage]:
 171:         """타이밍 테이블의 한 줄 파싱"""
 172:         # 여러 형식 지원:
 173:         # 1. "cpu/pc_reg[0]/Q (DFFQX1)     0.15       0.65 r"
 174:         # 2. "clock network delay (ideal)  0.50       0.50"
 175:         # 3. "U123/Y (AND2X1)               0.08       0.73 r"
 176:         
 177:         # 패턴: 셀 이름, 증분 delay, 누적 delay, transition (옵션)
 178:         pattern = r'^\s*(\S+(?:\s+\([^)]+\))?)\s+([-\d.]+)\s+([-\d.]+)\s*([rf])?'
 179:         match = re.match(pattern, line)
 180:         
 181:         if not match:
 182:             return None
 183:         
 184:         point = match.group(1)
 185:         # 괄호 안의 셀 타입 제거
 186:         point = re.sub(r'\s*\([^)]+\)', '', point).strip()
 187:         
 188:         incr = float(match.group(2))
 189:         path = float(match.group(3))
 190:         transition = match.group(4) or ''
 191:         
 192:         return TimingStage(
 193:             point=point,
 194:             incr_delay=incr,
 195:             cumulative_delay=path,
 196:             transition=transition,
 197:         )
 198:     
 199:     def apply_to_graph(
 200:         self,
 201:         nodes: Dict[str, DKGNode],
 202:         edges: Dict[str, DKGEdge],
 203:         updater: GraphUpdater,
 204:     ) -> None:
 205:         """파싱한 타이밍 정보를 DKG 그래프에 반영"""
 206:         from ..stages import FieldSource, ParsingStage
 207:         
 208:         for path in self.paths:
 209:             # 1. Startpoint/Endpoint 노드 업데이트
 210:             self._update_node_timing(
 211:                 path.startpoint, path, nodes, updater, is_endpoint=False
 212:             )
 213:             self._update_node_timing(
 214:                 path.endpoint, path, nodes, updater, is_endpoint=True
 215:             )
 216:             
 217:             # 2. 경로상 각 엣지에 delay 설정
 218:             for i in range(len(path.stages) - 1):
 219:                 src_stage = path.stages[i]
 220:                 dst_stage = path.stages[i + 1]
 221:                 
 222:                 self._update_edge_timing(
 223:                     src_stage, dst_stage, path, edges, updater
 224:                 )
 225:     
 226:     def _update_node_timing(
 227:         self,
 228:         node_name: str,
 229:         path: TimingPath,
 230:         nodes: Dict[str, DKGNode],
 231:         updater: GraphUpdater,
 232:         is_endpoint: bool,
 233:     ) -> None:
 234:         """노드의 타이밍 정보 업데이트
 235:         
 236:         주의: 한 노드는 여러 경로에 나타날 수 있으므로:
 237:         - slack은 최악값(worst-case)만 저장
 238:         - 상세 정보는 메타데이터에 누적
 239:         """
 240:         from ..stages import FieldSource, ParsingStage
 241:         
 242:         # 노드 이름 정규화 (hier_path 또는 canonical_name 매칭)
 243:         node = self._find_node_by_name(node_name, nodes)
 244:         if not node:
 245:             return
 246:         
 247:         node_id = node.node_id
 248:         
 249:         # Slack 업데이트 (startpoint에만, 최악값만)
 250:         if not is_endpoint and path.slack is not None:
 251:             # 기존 slack보다 나쁘면(작으면) 업데이트
 252:             if node.slack is None or path.slack < node.slack:
 253:                 node.slack = path.slack
 254:             
 255:             # 메타데이터에는 경로별로 누적 저장 (리스트)
 256:             metadata = updater.node_metadata[node_id]
 257:             existing_slacks = metadata.get('timing_slacks', [])
 258:             existing_slacks.append({
 259:                 'slack': path.slack,
 260:                 'path_type': path.path_type,
 261:                 'clock': path.clock,
 262:                 'endpoint': path.endpoint,
 263:             })
 264:             metadata.set(
 265:                 'timing_slacks',
 266:                 existing_slacks,
 267:                 FieldSource.ANALYZED,
 268:                 ParsingStage.TIMING,
 269:             )
 270:         
 271:         # Arrival time - 여러 값 중 최악(최대)만 저장
 272:         if path.arrival_time is not None:
 273:             if node.arrival_time is None or path.arrival_time > node.arrival_time:
 274:                 node.arrival_time = path.arrival_time
 275:         
 276:         # Required time - 여러 값 중 최선(최소)만 저장
 277:         if path.required_time is not None:
 278:             if node.required_time is None or path.required_time < node.required_time:
 279:                 node.required_time = path.required_time
 280:         
 281:         # Clock domain - 여러 클럭이 있을 수 있으므로 대표값만 저장
 282:         # (첫 번째로 발견된 클럭 or 가장 빈번한 클럭)
 283:         if path.clock and not node.clock_domain:
 284:             node.clock_domain = path.clock
 285:     
 286:     def _update_edge_timing(
 287:         self,
 288:         src_stage: TimingStage,
 289:         dst_stage: TimingStage,
 290:         path: TimingPath,
 291:         edges: Dict[str, DKGEdge],
 292:         updater: GraphUpdater,
 293:     ) -> None:
 294:         """엣지의 타이밍 정보 업데이트
 295:         
 296:         주의: 한 엣지도 여러 경로에 나타날 수 있으므로:
 297:         - delay는 최악값만 저장 (일반적으로 동일해야 함)
 298:         - 상세 정보는 메타데이터에 누적
 299:         """
 300:         from ..stages import FieldSource, ParsingStage
 301:         
 302:         # 엣지 찾기 (휴리스틱: src/dst 이름 기반)
 303:         edge = self._find_edge_by_pins(src_stage.point, dst_stage.point, edges)
 304:         if not edge:
 305:             return
 306:         
 307:         edge_id = edge.edge_id
 308:         
 309:         # Delay 업데이트 - 최대값 저장 (보수적)
 310:         if edge.delay is None or dst_stage.incr_delay > edge.delay:
 311:             edge.delay = dst_stage.incr_delay
 312:         
 313:         # 메타데이터에 경로별 delay 누적
 314:         metadata = updater.edge_metadata[edge_id]
 315:         existing_delays = metadata.get('timing_delays', [])
 316:         existing_delays.append({
 317:             'delay': dst_stage.incr_delay,
 318:             'path_type': path.path_type,
 319:             'clock': path.clock,
 320:         })
 321:         metadata.set(
 322:             'timing_delays',
 323:             existing_delays,
 324:             FieldSource.ANALYZED,
 325:             ParsingStage.TIMING,
 326:         )
 327:         
 328:         # Arrival time - 최대값만 저장
 329:         if edge.arrival_time is None or dst_stage.cumulative_delay > edge.arrival_time:
 330:             edge.arrival_time = dst_stage.cumulative_delay
 331:         
 332:         # Clock domain - 첫 번째 클럭 저장 (또는 가장 빈번한 클럭)
 333:         if path.clock and not edge.clock_domain_id:
 334:             edge.clock_domain_id = path.clock
 335:     
 336:     def _find_node_by_name(
 337:         self, name: str, nodes: Dict[str, DKGNode]
 338:     ) -> Optional[DKGNode]:
 339:         """이름으로 노드 찾기 (휴리스틱)"""
 340:         # 1. 직접 매칭
 341:         if name in nodes:
 342:             return nodes[name]
 343:         
 344:         # 2. hier_path로 매칭
 345:         for node in nodes.values():
 346:             if node.hier_path == name:
 347:                 return node
 348:         
 349:         # 3. canonical_name으로 매칭
 350:         for node in nodes.values():
 351:             if node.canonical_name == name:
 352:                 return node
 353:         
 354:         # 4. 부분 매칭 (마지막 시도)
 355:         for node in nodes.values():
 356:             if name in node.hier_path or node.hier_path in name:
 357:                 return node
 358:         
 359:         return None
 360:     
 361:     def _find_edge_by_pins(
 362:         self, src_pin: str, dst_pin: str, edges: Dict[str, DKGEdge]
 363:     ) -> Optional[DKGEdge]:
 364:         """src/dst 핀 이름으로 엣지 찾기 (휴리스틱)"""
 365:         # 타이밍 리포트의 핀 이름과 DKG 엣지의 노드 이름이 다를 수 있음
 366:         # 예: "cpu/pc_reg[0]/Q" vs "cpu.pc_reg[0]"
 367:         
 368:         for edge in edges.values():
 369:             src_node_name = edge.src_node
 370:             dst_node_name = edge.dst_node
 371:             
 372:             # 부분 매칭 시도
 373:             if (src_pin in src_node_name or src_node_name in src_pin) and \
 374:                (dst_pin in dst_node_name or dst_node_name in dst_pin):
 375:                 return edge
 376:         
 377:         return None
 378:     
 379:     def get_summary(self) -> Dict:
 380:         """파싱 결과 요약"""
 381:         if not self.paths:
 382:             return {'total_paths': 0}
 383:         
 384:         slacks = [p.slack for p in self.paths if p.slack is not None]
 385:         worst_slack = min(slacks) if slacks else None
 386:         met_count = sum(1 for s in slacks if s >= 0)
 387:         
 388:         return {
 389:             'total_paths': len(self.paths),
 390:             'worst_slack': worst_slack,
 391:             'met_timing': met_count,
 392:             'failed_timing': len(slacks) - met_count,
 393:             'clocks': list(set(p.clock for p in self.paths if p.clock)),
 394:         }



FILE: dkg\parsers\xdc_parser.py

   1: from __future__ import annotations
   2: 
   3: import re
   4: from typing import Dict
   5: 
   6: from ..graph import DKGEdge, DKGNode
   7: from ..graph_updater import GraphUpdater
   8: from ..stages import FieldSource, ParsingStage
   9: from . import ConstraintParser
  10: from .parser_utils import extract_bracket_targets, match_any
  11: 
  12: 
  13: class XdcParser(ConstraintParser):
  14:     """
  15:     XDC (Xilinx Design Constraints) 파서.
  16:     
  17:     SDC와 유사하지만 Xilinx 특화 명령 포함:
  18:     - set_property LOC / IOSTANDARD: 핀 배치
  19:     - create_pblock: 물리적 블록 정의
  20:     """
  21:     
  22:     def get_stage(self) -> ParsingStage:
  23:         return ParsingStage.CONSTRAINTS
  24:     
  25:     def parse_and_update(
  26:         self,
  27:         filepath: str,
  28:         updater: GraphUpdater,
  29:         nodes: Dict[str, DKGNode],
  30:         edges: Dict[str, DKGEdge],
  31:     ) -> None:
  32:         with open(filepath, "r", encoding="utf-8") as f:
  33:             lines = f.readlines()
  34: 
  35:         for line_num, line in enumerate(lines, start=1):
  36:             raw = line.strip()
  37:             if not raw or raw.startswith("#"):
  38:                 continue
  39: 
  40:             if raw.startswith("set_property"):
  41:                 self._parse_set_property(raw, line_num, filepath, updater, nodes)
  42:             elif raw.startswith("create_pblock"):
  43:                 self._parse_create_pblock(raw)
  44:             elif raw.startswith("add_cells_to_pblock"):
  45:                 self._parse_add_cells_to_pblock(raw, line_num, filepath, updater, nodes)
  46: 
  47:     def _parse_set_property(
  48:         self,
  49:         line: str,
  50:         line_num: int,
  51:         filepath: str,
  52:         updater: GraphUpdater,
  53:         nodes: Dict[str, DKGNode],
  54:     ) -> None:
  55:         match = re.search(r"set_property\s+(LOC|IOSTANDARD)\s+(\S+)", line)
  56:         if not match:
  57:             return
  58: 
  59:         prop = match.group(1)
  60:         value = match.group(2)
  61:         targets = extract_bracket_targets(line, ("ports", "pins", "cells"))
  62:         if not targets:
  63:             return
  64: 
  65:         for node_id, node in nodes.items():
  66:             candidates = [node.local_name, node.hier_path, node.canonical_name]
  67:             if not match_any(targets, candidates):
  68:                 continue
  69: 
  70:             new_attrs = dict(node.attributes)
  71:             new_attrs[prop] = value
  72:             updater.update_node_field(
  73:                 node_id,
  74:                 "attributes",
  75:                 new_attrs,
  76:                 FieldSource.DECLARED,
  77:                 ParsingStage.CONSTRAINTS,
  78:                 filepath,
  79:                 line_num,
  80:             )
  81: 
  82:     def _parse_create_pblock(self, line: str) -> None:
  83:         match = re.search(r"create_pblock\s+(\S+)", line)
  84:         if not match:
  85:             return
  86: 
  87:     def _parse_add_cells_to_pblock(
  88:         self,
  89:         line: str,
  90:         line_num: int,
  91:         filepath: str,
  92:         updater: GraphUpdater,
  93:         nodes: Dict[str, DKGNode],
  94:     ) -> None:
  95:         pblock_name = None
  96:         pblock_match = re.search(r"add_cells_to_pblock\s+\[get_pblocks\s+([^\]]+)\]", line)
  97:         if pblock_match:
  98:             pblock_name = pblock_match.group(1).strip()
  99:         else:
 100:             direct = re.search(r"add_cells_to_pblock\s+(\S+)", line)
 101:             if direct:
 102:                 pblock_name = direct.group(1)
 103: 
 104:         if not pblock_name:
 105:             return
 106: 
 107:         targets = extract_bracket_targets(line, ("cells",))
 108:         if not targets:
 109:             return
 110: 
 111:         for node_id, node in nodes.items():
 112:             candidates = [node.local_name, node.hier_path, node.canonical_name]
 113:             if not match_any(targets, candidates):
 114:                 continue
 115: 
 116:             new_attrs = dict(node.attributes)
 117:             new_attrs["pblock"] = pblock_name
 118:             new_attrs["pblock_seed"] = pblock_name
 119:             updater.update_node_field(
 120:                 node_id,
 121:                 "attributes",
 122:                 new_attrs,
 123:                 FieldSource.DECLARED,
 124:                 ParsingStage.FLOORPLAN,
 125:                 filepath,
 126:                 line_num,
 127:             )
 128: 



FILE: dkg\pipeline.py

   1: from __future__ import annotations
   2: 
   3: from pathlib import Path
   4: from typing import Dict, List, Optional
   5: 
   6: from .config import YosysConfig
   7: from .graph import DKGEdge, DKGNode
   8: from .graph_build import build_nodes_and_edges, build_wires_and_cells
   9: from .graph_updater import GraphUpdater
  10: from .graphcache import GraphSnapshot, GraphVersion, load_snapshot, save_snapshot
  11: from .parsers import ConstraintParser
  12: from .parsers.sdc_parser import SdcParser
  13: from .parsers.tcl_parser import TclParser
  14: from .parsers.timing_report_parser import TimingReportParser
  15: from .parsers.xdc_parser import XdcParser
  16: from .parsers.bd_parser import BdParser
  17: from .stages import FieldSource, ParsingStage
  18: from .supergraph import SuperGraph
  19: from .utils import compute_file_hash
  20: from .yosys_parser import parse_yosys
  21: 
  22: 
  23: class DKGPipeline:
  24:     """
  25:     전체 DKG 구축 파이프라인.
  26:     
  27:     Usage:
  28:         pipeline = DKGPipeline(yosys_config)
  29:         
  30:         # Stage 1: RTL 파싱
  31:         pipeline.run_rtl_stage()
  32:         
  33:         # Stage 2: Constraint 추가
  34:         pipeline.add_constraints("design.sdc")
  35:         pipeline.add_constraints("design.xdc")
  36:         
  37:         # Stage 3: 타이밍 리포트 추가
  38:         pipeline.add_timing_report("timing.rpt")
  39:         
  40:         # 최종 그래프 반환
  41:         nodes, edges = pipeline.get_graph()
  42:     """
  43:     
  44:     def __init__(self, yosys_config: YosysConfig):
  45:         self.yosys_config = yosys_config
  46:         
  47:         self.nodes: Optional[Dict[str, DKGNode]] = None
  48:         self.edges: Optional[Dict[str, DKGEdge]] = None
  49:         self.updater: Optional[GraphUpdater] = None
  50:         self.supergraph: Optional[SuperGraph] = None
  51:         
  52:         self.current_stage = None
  53:         self.completed_stages: List[ParsingStage] = []
  54:         
  55:         # 입력 파일 추적 (버전 계산용)
  56:         self.rtl_files: List[str] = []
  57:         self.constraint_files: List[str] = []
  58:         self.timing_files: List[str] = []
  59:         
  60:         # 파서 레지스트리
  61:         self.parsers: Dict[str, ConstraintParser] = {
  62:             "sdc": SdcParser(),
  63:             "xdc": XdcParser(),
  64:             # TODO: 추가 파서 등록
  65:         }
  66:     
  67:     def run_rtl_stage(self) -> None:
  68:         """Stage 1: RTL 파싱 (Yosys)"""
  69:         yosys = parse_yosys(self.yosys_config)
  70:         wires, cells = build_wires_and_cells(yosys)
  71:         self.nodes, self.edges = build_nodes_and_edges(wires, cells)
  72:         
  73:         # RTL 파일 추적
  74:         if self.yosys_config.out_json_win:
  75:             self.rtl_files.append(self.yosys_config.out_json_win)
  76:         
  77:         self.updater = GraphUpdater(self.nodes, self.edges)
  78:         self.current_stage = ParsingStage.RTL
  79:         self.completed_stages.append(ParsingStage.RTL)
  80:         
  81:         # 초기 메타데이터 설정 (모두 INFERRED)
  82:         self._mark_initial_fields_as_inferred()
  83:     
  84:     def add_constraints(self, filepath: str) -> None:
  85:         """Stage 2: Constraint 파일 추가"""
  86:         if self.updater is None or self.nodes is None or self.edges is None:
  87:             raise RuntimeError("RTL stage must be run first")
  88:         
  89:         # 파일 확장자로 파서 선택
  90:         ext = Path(filepath).suffix.lower().lstrip(".")
  91:         
  92:         if ext not in self.parsers:
  93:             raise ValueError(f"Unsupported constraint format: {ext}")
  94:         
  95:         parser = self.parsers[ext]
  96:         parser.parse_and_update(filepath, self.updater, self.nodes, self.edges)
  97:         
  98:         # 제약 파일 추적
  99:         self.constraint_files.append(filepath)
 100:         
 101:         if ParsingStage.CONSTRAINTS not in self.completed_stages:
 102:             self.completed_stages.append(ParsingStage.CONSTRAINTS)
 103:     
 104:     def add_timing_report(self, filepath: str) -> None:
 105:         """Stage 3: 타이밍 리포트 추가"""
 106:         if self.updater is None or self.nodes is None or self.edges is None:
 107:             raise RuntimeError("RTL stage must be run first")
 108:         
 109:         # 타이밍 리포트 파싱
 110:         parser = TimingReportParser()
 111:         paths = parser.parse_file(filepath)
 112:         
 113:         # 그래프에 반영
 114:         parser.apply_to_graph(self.nodes, self.edges, self.updater)
 115:         
 116:         # 파일 추적
 117:         self.timing_files.append(filepath)
 118:         
 119:         if ParsingStage.TIMING not in self.completed_stages:
 120:             self.completed_stages.append(ParsingStage.TIMING)
 121:         
 122:         # 요약 출력
 123:         summary = parser.get_summary()
 124:         print(f"✅ 타이밍 리포트 파싱 완료: {filepath}")
 125:         print(f"   - 경로 수: {summary['total_paths']}")
 126:         if summary.get('worst_slack') is not None:
 127:             print(f"   - 최악 slack: {summary['worst_slack']:.2f} ns")
 128:     
 129:     def add_floorplan(self, filepath: str) -> None:
 130:         """Stage 4: Floorplan TCL 추가"""
 131:         if self.updater is None or self.nodes is None or self.edges is None:
 132:             raise RuntimeError("RTL stage must be run first")
 133: 
 134:         parser = TclParser()
 135:         parser.parse_and_update(filepath, self.updater, self.nodes, self.edges)
 136: 
 137:         if ParsingStage.FLOORPLAN not in self.completed_stages:
 138:             self.completed_stages.append(ParsingStage.FLOORPLAN)
 139: 
 140:     def add_board(self, filepath: str) -> None:
 141:         """Stage 5: BD/board constraints 추가"""
 142:         if self.updater is None or self.nodes is None or self.edges is None:
 143:             raise RuntimeError("RTL stage must be run first")
 144: 
 145:         parser = BdParser()
 146:         parser.parse_and_update(filepath, self.updater, self.nodes, self.edges)
 147: 
 148:         if ParsingStage.BOARD not in self.completed_stages:
 149:             self.completed_stages.append(ParsingStage.BOARD)
 150:     
 151:     def get_graph(self) -> tuple[Dict[str, DKGNode], Dict[str, DKGEdge]]:
 152:         """최종 그래프 반환"""
 153:         if self.nodes is None or self.edges is None:
 154:             raise RuntimeError("No graph available. Run RTL stage first.")
 155:         return self.nodes, self.edges
 156:     
 157:     def get_updater(self) -> GraphUpdater:
 158:         """GraphUpdater 반환 (고급 사용자용)"""
 159:         if self.updater is None:
 160:             raise RuntimeError("No updater available. Run RTL stage first.")
 161:         return self.updater
 162:     
 163:     def export_metadata(self) -> dict:
 164:         """메타데이터 요약 반환 (캐싱/디버깅용)"""
 165:         if self.updater is None:
 166:             return {}
 167:         return self.updater.export_metadata_summary()
 168:     
 169:     def _mark_initial_fields_as_inferred(self) -> None:
 170:         """RTL stage에서 추론한 필드들을 INFERRED로 마킹"""
 171:         if self.nodes is None or self.edges is None or self.updater is None:
 172:             return
 173:         
 174:         # clock_domain, flow_type 등 휴리스틱으로 채운 필드들
 175:         for node_id, node in self.nodes.items():
 176:             if node.clock_domain:
 177:                 self.updater.node_metadata[node_id].set(
 178:                     "clock_domain",
 179:                     node.clock_domain,
 180:                     FieldSource.INFERRED,
 181:                     ParsingStage.RTL,
 182:                 )
 183:         
 184:         for edge_id, edge in self.edges.items():
 185:             if edge.flow_type:
 186:                 self.updater.edge_metadata[edge_id].set(
 187:                     "flow_type",
 188:                     edge.flow_type.value,
 189:                     FieldSource.INFERRED,
 190:                     ParsingStage.RTL,
 191:                 )
 192:     
 193:     def compute_version(self) -> GraphVersion:
 194:         """현재 상태의 GraphVersion 계산"""
 195:         import hashlib
 196:         
 197:         # RTL 해시 (모든 RTL 파일의 조합)
 198:         rtl_hash = ""
 199:         if self.rtl_files:
 200:             combined = "".join(compute_file_hash(f) for f in self.rtl_files)
 201:             rtl_hash = hashlib.sha256(combined.encode()).hexdigest()[:16]
 202:         
 203:         # Constraint 해시
 204:         constraint_hash = None
 205:         if self.constraint_files:
 206:             combined = "".join(compute_file_hash(f) for f in self.constraint_files)
 207:             constraint_hash = hashlib.sha256(combined.encode()).hexdigest()[:16]
 208:         
 209:         # Timing 해시
 210:         timing_hash = None
 211:         if self.timing_files:
 212:             combined = "".join(compute_file_hash(f) for f in self.timing_files)
 213:             timing_hash = hashlib.sha256(combined.encode()).hexdigest()[:16]
 214:         
 215:         # 정책 버전 (향후 확장)
 216:         policy_versions = {}
 217:         
 218:         return GraphVersion(
 219:             rtl_hash=rtl_hash,
 220:             constraint_hash=constraint_hash,
 221:             timing_hash=timing_hash,
 222:             policy_versions=policy_versions,
 223:         )
 224:     
 225:     def save_cache(self, filepath: str | Path, indent: Optional[int] = None) -> None:
 226:         """현재 그래프를 캐시 파일로 저장"""
 227:         if self.nodes is None or self.edges is None:
 228:             raise RuntimeError("No graph available. Run RTL stage first.")
 229:         
 230:         version = self.compute_version()
 231:         snapshot = GraphSnapshot(
 232:             version=version,
 233:             dkg_nodes=self.nodes,
 234:             dkg_edges=self.edges,
 235:             supergraph=self.supergraph,
 236:         )
 237:         save_snapshot(snapshot, filepath, indent=indent)
 238:     
 239:     @classmethod
 240:     def load_from_cache(cls, filepath: str | Path, yosys_config: Optional[YosysConfig] = None) -> "DKGPipeline":
 241:         """캐시 파일에서 파이프라인 복원"""
 242:         snapshot = load_snapshot(filepath)
 243:         
 244:         # 더미 config (캐시 로딩 시에는 필요 없음)
 245:         if yosys_config is None:
 246:             yosys_config = YosysConfig(src_dir_win="", out_json_win="", top_module="")
 247:         
 248:         pipeline = cls(yosys_config)
 249:         pipeline.nodes = snapshot.dkg_nodes
 250:         pipeline.edges = snapshot.dkg_edges
 251:         pipeline.supergraph = snapshot.supergraph
 252:         
 253:         # 메타데이터는 재생성하지 않음 (읽기 전용 모드)
 254:         pipeline.updater = None
 255:         
 256:         return pipeline
 257: 



FILE: dkg\provenance.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass
   4: from typing import Iterable, Optional, Tuple, List
   5: 
   6: 
   7: @dataclass
   8: class Provenance:
   9:     origin_file: Optional[str] = None
  10:     origin_line: Optional[int] = None
  11:     tool_stage: str = "rtl"  # rtl / synth / timing / constraint
  12:     confidence: str = "exact"  # exact / inferred
  13: 
  14: 
  15: def add_provenance(obj, prov: Provenance, make_primary: bool = False) -> None:
  16:     obj.provenances.append(prov)
  17:     if make_primary or obj.primary_provenance is None:
  18:         obj.primary_provenance = prov
  19: 
  20: 
  21: def merge_provenances_nodes(nodes: Iterable) -> Tuple[Provenance, List[Provenance]]:
  22:     new_provs: List[Provenance] = []
  23:     for n in nodes:
  24:         new_provs.extend(n.provenances)
  25: 
  26:     files = [p.origin_file for p in new_provs if p.origin_file]
  27:     lines = [p.origin_line for p in new_provs if p.origin_line]
  28: 
  29:     primary = Provenance(
  30:         origin_file=files[0] if files else None,
  31:         origin_line=min(lines) if lines else None,
  32:         tool_stage="rtl",
  33:         confidence="inferred",
  34:     )
  35: 
  36:     return primary, new_provs
  37: 
  38: 
  39: def merge_provenances_edges(edges: Iterable) -> Tuple[Provenance, List[Provenance]]:
  40:     new_provs: List[Provenance] = []
  41:     for e in edges:
  42:         new_provs.extend(e.provenances)
  43: 
  44:     files = [p.origin_file for p in new_provs if p.origin_file]
  45:     lines = [p.origin_line for p in new_provs if p.origin_line]
  46: 
  47:     primary = Provenance(
  48:         origin_file=files[0] if files else None,
  49:         origin_line=min(lines) if lines else None,
  50:         tool_stage="rtl",
  51:         confidence="inferred",
  52:     )
  53: 
  54:     return primary, new_provs



FILE: dkg\stages.py

   1: from __future__ import annotations
   2: 
   3: from enum import Enum
   4: from typing import FrozenSet
   5: 
   6: 
   7: class ParsingStage(str, Enum):
   8:     """파싱 단계 정의. 순서대로 실행됨."""
   9:     
  10:     RTL = "rtl"                    # Yosys JSON (구조 정보)
  11:     SYNTHESIS = "synthesis"        # 합성 후 netlist
  12:     CONSTRAINTS = "constraints"    # SDC/XDC (타이밍/클럭 제약)
  13:     FLOORPLAN = "floorplan"        # TCL/Pblock (물리적 배치)
  14:     TIMING = "timing"              # 타이밍 리포트
  15:     BOARD = "board"                # BD/board constraints
  16: 
  17: 
  18: class FieldSource(str, Enum):
  19:     """필드 값의 출처"""
  20:     
  21:     INFERRED = "inferred"          # 휴리스틱으로 추론
  22:     DECLARED = "declared"          # 명시적으로 선언됨
  23:     ANALYZED = "analyzed"          # 도구 분석 결과
  24:     USER_OVERRIDE = "user_override"  # 사용자 직접 설정
  25: 
  26: 
  27: # 각 필드가 어느 stage에서 확정되는지 정의
  28: FIELD_STAGES: dict[str, FrozenSet[ParsingStage]] = {
  29:     # Node fields
  30:     "entity_class": frozenset([ParsingStage.RTL, ParsingStage.SYNTHESIS]),
  31:     "hier_path": frozenset([ParsingStage.RTL, ParsingStage.SYNTHESIS]),
  32:     "clock_domain": frozenset([ParsingStage.RTL, ParsingStage.CONSTRAINTS]),
  33:     "arrival_time": frozenset([ParsingStage.TIMING]),
  34:     "required_time": frozenset([ParsingStage.TIMING]),
  35:     "slack": frozenset([ParsingStage.TIMING]),
  36:     
  37:     # Edge fields
  38:     "signal_name": frozenset([ParsingStage.RTL, ParsingStage.SYNTHESIS]),
  39:     "flow_type": frozenset([ParsingStage.RTL, ParsingStage.CONSTRAINTS]),
  40:     "clock_signal": frozenset([ParsingStage.CONSTRAINTS]),
  41:     "reset_signal": frozenset([ParsingStage.CONSTRAINTS]),
  42:     "delay": frozenset([ParsingStage.TIMING]),
  43:     "timing_exception": frozenset([ParsingStage.CONSTRAINTS]),
  44: }
  45: 
  46: 
  47: def get_priority(source: FieldSource) -> int:
  48:     """값 우선순위. 높을수록 신뢰도 높음."""
  49:     priorities = {
  50:         FieldSource.INFERRED: 1,
  51:         FieldSource.DECLARED: 3,
  52:         FieldSource.ANALYZED: 2,
  53:         FieldSource.USER_OVERRIDE: 4,
  54:     }
  55:     return priorities[source]
  56: 
  57: 
  58: def should_update_field(
  59:     current_source: FieldSource | None,
  60:     new_source: FieldSource,
  61: ) -> bool:
  62:     """기존 값을 새 값으로 업데이트할지 결정"""
  63:     if current_source is None:
  64:         return True
  65:     return get_priority(new_source) >= get_priority(current_source)



FILE: dkg\supergraph.py

   1: from __future__ import annotations
   2: 
   3: from dataclasses import dataclass, field
   4: from enum import Enum
   5: from typing import Any, Dict, List, Optional, Set, Tuple
   6: 
   7: from .graph import DKGEdge, DKGNode, EdgeFlowType, EntityClass, RelationType
   8: from .provenance import Provenance
   9: from .utils import stable_hash
  10: 
  11: 
  12: # ============================================================================
  13: # Analysis Attachment Model
  14: # ============================================================================
  15: # Analysis는 그래프 구조를 변경하지 않는 immutable snapshot이며
  16: # SuperNode/SuperEdge에 keyed bundle로 부착됩니다.
  17: # ============================================================================
  18: class AnalysisKind(str, Enum):
  19:     TIMING = "timing"
  20:     AREA = "area"
  21:     POWER = "power"
  22: 
  23: @dataclass(frozen=True)
  24: class TimingNodeMetrics:
  25:     """
  26:     SuperNode에 부착되는 Timing 분석 결과.
  27:     
  28:     원칙:
  29:     - 집계 가능한 통계 정보만 포함
  30:     - critical path 여부, slack region 등의 단언 금지
  31:     - path membership 정보 포함 금지
  32:     """
  33:     # 필수 Metrics
  34:     min_slack: float              # 절대 최악값
  35:     p5_slack: float               # tail risk 지표 (5th percentile)
  36:     max_arrival_time: float       # 가장 늦은 도착 시간
  37:     min_required_time: float      # 가장 타이트한 요구 시간
  38:     critical_node_ratio: float    # slack < threshold 비율
  39:     near_critical_ratio: float    # slack < α·clock 비율
  40:     
  41:     # 선택적 Metric
  42:     timing_risk_score: Optional[float] = None  # UI/Alert용 단일 스칼라
  43: 
  44: 
  45: @dataclass(frozen=True)
  46: class TimingEdgeMetrics:
  47:     """
  48:     SuperEdge에 부착되는 Timing 분석 결과.
  49:     
  50:     원칙:
  51:     - 지연 특성의 통계만 제공
  52:     - "이 edge가 slack을 결정한다" 등의 단언 금지
  53:     - critical edge 여부 표현 금지
  54:     """
  55:     # 필수 Metrics
  56:     max_delay: float
  57:     p95_delay: float              # 95th percentile delay
  58:     flow_type_histogram: Dict[EdgeFlowType, int]  # comb / seq 비율
  59:     
  60:     # 선택적 Metrics
  61:     fanout_max: Optional[int] = None
  62:     fanout_p95: Optional[float] = None
  63: 
  64: 
  65: # ============================================================================
  66: # 그래프 외부 Timing 정보 (필수 분리 대상)
  67: # ============================================================================
  68: # Timing 분석 결과 중 alert, finding, summary, path digest는
  69: # 그래프 외부 객체로 관리하며 그래프와 재결합하지 않습니다.
  70: # ============================================================================
  71: 
  72: 
  73: class TimingAlertSeverity(str, Enum):
  74:     INFO = "info"
  75:     WARN = "warn"
  76:     ERROR = "error"
  77: 
  78: 
  79: @dataclass
  80: class TimingAlert:
  81:     """
  82:     Timing 분석에서 발견된 Alert/Finding.
  83:     
  84:     그래프 외부 객체로 관리되며, entity_ref를 통해 참조만 수행.
  85:     """
  86:     entity_ref: str               # node_id / supernode_id / edge_id
  87:     entity_type: str              # "node" / "supernode" / "edge"
  88:     severity: TimingAlertSeverity
  89:     reason: str
  90:     metrics_snapshot: Dict[str, Any]  # 발견 시점의 metric 사본
  91: 
  92: 
  93: @dataclass
  94: class TimingSummary:
  95:     """
  96:     전체 Timing 분석의 요약 정보.
  97:     
  98:     그래프와 독립적으로 관리되는 분석 결과 요약.
  99:     """
 100:     worst_slack: float
 101:     violation_count: int
 102:     near_critical_count: int
 103:     clock_period: float
 104:     analysis_mode: str            # "setup" / "hold" / "both"
 105:     timestamp: Optional[str] = None
 106: 
 107: 
 108: @dataclass
 109: class CriticalPathDigest:
 110:     """
 111:     Critical Path의 참조용 Digest (선택적).
 112:     
 113:     Path digest는 참조용이며 그래프와 재결합하지 않습니다.
 114:     UI/Query 계층에서만 사용됩니다.
 115:     """
 116:     path_id: str
 117:     startpoint: str
 118:     endpoint: str
 119:     total_delay: float
 120:     slack: float
 121:     node_sequence: Optional[List[str]] = None  # 참조용
 122: 
 123: 
 124: class GraphViewType(str, Enum):
 125:     Structural = "Structural"
 126:     Connectivity = "Connectivity"
 127:     Physical = "Physical"
 128: 
 129: 
 130: class SuperClass(str, Enum):
 131:     ATOMIC = "Atomic"
 132:     MODULE_CLUSTER = "ModuleCluster"
 133:     SEQ_CHAIN = "SequentialChain"
 134:     COMB_CLOUD = "CombinationalCloud"
 135:     IO_CLUSTER = "IOCluster"
 136:     CONSTRAINT_GROUP = "ConstraintGroup"
 137:     ELIMINATED = "EliminatedNode" # superedge로 환원되지 않는 노드의 보존적 표현
 138: 
 139: 
 140: class NodeAction(Enum):
 141:     PROMOTE = "promote"
 142:     MERGE = "merge"
 143:     ELIMINATE = "eliminate"
 144: 
 145: 
 146: @dataclass
 147: class SuperNode:
 148:     node_id: str
 149:     super_class: SuperClass
 150:     member_nodes: Set[str]
 151:     member_edges: Set[str]
 152:     aggregated_attrs: Dict[str, Any] = field(default_factory=dict)
 153:     provenances: List[Provenance] = field(default_factory=list)
 154:     canonical_name: Optional[str] = None
 155:     display_name: Optional[str] = None
 156:     # Analysis Attachment: keyed bundle for extensibility
 157:     analysis: Dict[AnalysisKind, Any] = field(default_factory=dict, repr=False)
 158:     # analysis dict itself is mutable,
 159:     # but each analysis bundle MUST be immutable snapshot
 160: 
 161: # superedge는 DKGEdge와 달리 고유한 의미를 가지지 않고 그래프적 연결성을 나타내는 용도. 의미는 멤버 엣지들이 담당
 162: @dataclass
 163: class SuperEdge:
 164:     edge_id: str
 165:     src_node: str
 166:     dst_node: str
 167:     member_edges: Set[str]
 168:     member_nodes: Set[str]
 169:     relation_types: Set[RelationType]
 170:     flow_types: Set[EdgeFlowType]
 171:     provenances: List[Provenance] = field(default_factory=list)
 172:     canonical_name: Optional[str] = None
 173:     display_name: Optional[str] = None
 174:     # Analysis Attachment: keyed bundle for extensibility
 175:     analysis: Dict[AnalysisKind, Any] = field(default_factory=dict, repr=False)
 176:     # analysis dict itself is mutable,
 177:     # but each analysis bundle MUST be immutable snapshot
 178: 
 179: 
 180: 
 181: @dataclass
 182: class SuperGraph:
 183:     super_nodes: Dict[str, SuperNode]
 184:     super_edges: Dict[Tuple[str, str], SuperEdge]
 185:     node_to_super: Dict[str, str]
 186: 
 187: 
 188: def make_supernode_canonical_name(sn: SuperNode, nodes: Dict[str, DKGNode]) -> str:
 189:     any_node = nodes[next(iter(sn.member_nodes))]
 190:     base = any_node.hier_path
 191:     return f"{base} : {sn.super_class.value}"
 192: 
 193: 
 194: def make_supernode_display_name(sn: SuperNode) -> str:
 195:     if sn.super_class == SuperClass.COMB_CLOUD:
 196:         return "Combinational Logic"
 197:     if sn.super_class == SuperClass.SEQ_CHAIN:
 198:         return "Sequential Chain"
 199:     if sn.super_class == SuperClass.ATOMIC:
 200:         return "Block"
 201:     if sn.super_class == SuperClass.ELIMINATED:
 202:         return "Collapsed"
 203:     return sn.super_class.value
 204: 
 205: 
 206: def make_superedge_canonical_name(se: SuperEdge, super_nodes: Dict[str, SuperNode]) -> str:
 207:     src = super_nodes[se.src_node].canonical_name
 208:     dst = super_nodes[se.dst_node].canonical_name
 209:     return f"{src} -> {dst}"
 210: 
 211: 
 212: def make_superedge_display_name(se: SuperEdge) -> str:
 213:     if len(se.relation_types) == 1:
 214:         return next(iter(se.relation_types)).value.replace("Relation", "")
 215:     return "Multiple Signals"
 216: 
 217: 
 218: def supernode_signature(
 219:     view: GraphViewType,
 220:     super_class: SuperClass,
 221:     member_node_ids: set[str],
 222:     policy_version: str = "v1",
 223: ) -> str:
 224:     nodes_part = ",".join(sorted(member_node_ids))
 225:     return "|".join([view.value, super_class.value, policy_version, nodes_part])
 226: 
 227: 
 228: def make_supernode_id(
 229:     view: GraphViewType,
 230:     super_class: SuperClass,
 231:     member_node_ids: set[str],
 232:     policy_version: str = "v1",
 233: ) -> str:
 234:     sig = supernode_signature(view, super_class, member_node_ids, policy_version)
 235:     h = stable_hash(sig)
 236:     return f"SN_{view.value}_{super_class.value}_{h}"
 237: 
 238: 
 239: def superedge_signature(
 240:     src_sn: str,
 241:     dst_sn: str,
 242:     member_edge_ids: set[str],
 243:     policy_version: str = "v1",
 244: ) -> str:
 245:     edges_part = ",".join(sorted(member_edge_ids))
 246:     return "|".join([src_sn, dst_sn, policy_version, edges_part])
 247: 
 248: 
 249: def make_superedge_id(
 250:     src_sn: str,
 251:     dst_sn: str,
 252:     member_edge_ids: set[str],
 253:     policy_version: str = "v1",
 254: ) -> str:
 255:     sig = superedge_signature(src_sn, dst_sn, member_edge_ids, policy_version)
 256:     h = stable_hash(sig)
 257:     return f"SE_{h}"
 258: 
 259: @dataclass(frozen=True)
 260: class NodePolicy:
 261:     action: NodeAction
 262:     super_class: Optional[SuperClass]  # PROMOTE면 None 가능
 263: 
 264: POLICY_MAP: dict[GraphViewType, dict[EntityClass, NodePolicy]] = {
 265: 
 266:     GraphViewType.Structural: {
 267:         EntityClass.MODULE_INSTANCE: NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 268:         EntityClass.IO_PORT:        NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 269: 
 270:         EntityClass.RTL_BLOCK:      NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 271:         EntityClass.FSM:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 272:         EntityClass.FLIP_FLOP:      NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 273:         EntityClass.LUT:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 274:         EntityClass.MUX:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 275:         EntityClass.DSP:            NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 276:         EntityClass.BRAM:           NodePolicy(NodeAction.MERGE, SuperClass.MODULE_CLUSTER),
 277: 
 278:         EntityClass.PACKAGE_PIN:    NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 279:         EntityClass.PBLOCK:         NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 280:         EntityClass.BOARD_CONNECTOR:NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 281:     },
 282: 
 283:     GraphViewType.Connectivity: {
 284:         EntityClass.FLIP_FLOP:      NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 285:         EntityClass.DSP:            NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 286:         EntityClass.BRAM:           NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 287:         EntityClass.IO_PORT:        NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 288: 
 289:         EntityClass.RTL_BLOCK:      NodePolicy(NodeAction.MERGE, SuperClass.COMB_CLOUD),
 290:         EntityClass.FSM:            NodePolicy(NodeAction.MERGE, SuperClass.COMB_CLOUD),
 291:         EntityClass.LUT:            NodePolicy(NodeAction.MERGE, SuperClass.COMB_CLOUD),
 292:         EntityClass.MUX:            NodePolicy(NodeAction.MERGE, SuperClass.COMB_CLOUD),
 293: 
 294:         EntityClass.MODULE_INSTANCE:NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 295:         EntityClass.PACKAGE_PIN:    NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 296:         EntityClass.PBLOCK:         NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 297:         EntityClass.BOARD_CONNECTOR:NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED),
 298:     },
 299: 
 300:     GraphViewType.Physical: {
 301:         EntityClass.IO_PORT:        NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 302:         EntityClass.PACKAGE_PIN:    NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 303:         EntityClass.PBLOCK:         NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 304:         EntityClass.BOARD_CONNECTOR:NodePolicy(NodeAction.PROMOTE, SuperClass.ATOMIC),
 305: 
 306:         EntityClass.DSP:            NodePolicy(NodeAction.MERGE, SuperClass.CONSTRAINT_GROUP),
 307:         EntityClass.BRAM:           NodePolicy(NodeAction.MERGE, SuperClass.CONSTRAINT_GROUP),
 308: 
 309:         # 나머지는 전부 제거
 310:     },
 311: }
 312: 
 313: 
 314: 
 315: def get_node_policy(node: DKGNode, view: GraphViewType) -> NodePolicy:
 316:     return POLICY_MAP[view].get(
 317:         node.entity_class,
 318:         NodePolicy(NodeAction.ELIMINATE, SuperClass.ELIMINATED)
 319:     )
 320: 
 321: 
 322: class ViewBuilder:
 323:     def __init__(self, nodes: Dict[str, DKGNode], edges: Dict[str, DKGEdge], view: GraphViewType):
 324:         self.nodes = nodes
 325:         self.edges = edges
 326:         self.view = view
 327: 
 328:         self.node_to_super: Dict[str, str] = {}
 329:         self.super_nodes: Dict[str, SuperNode] = {}
 330:         self.super_edges: Dict[Tuple[str, str], SuperEdge] = {}
 331: 
 332:     def _neighbors_1hop(self, nid: str) -> Set[str]:
 333:         n = self.nodes[nid]
 334:         nbrs = set()
 335:         for eid in n.in_edges + n.out_edges:
 336:             e = self.edges[eid]
 337:             nbrs.add(e.src_node)
 338:             nbrs.add(e.dst_node)
 339:         return nbrs
 340: 
 341:     def cycle1_promote(self) -> None:
 342:         for n in self.nodes.values():
 343:             node_policy = get_node_policy(n, self.view)
 344:             if node_policy.action != NodeAction.PROMOTE:
 345:                 continue
 346:             if node_policy.super_class is None:
 347:                 continue
 348: 
 349:             sn = SuperNode(
 350:                 node_id=f"SN_{n.node_id}",
 351:                 super_class=node_policy.super_class,
 352:                 member_nodes={n.node_id},
 353:                 member_edges=set(),
 354:                 provenances=list(n.provenances),
 355:             )
 356:             sn.canonical_name = make_supernode_canonical_name(sn, self.nodes)
 357:             sn.display_name = make_supernode_display_name(sn)
 358:             self.super_nodes[sn.node_id] = sn
 359:             self.node_to_super[n.node_id] = sn.node_id
 360: 
 361:     def cycle2_merge(self) -> None:
 362:         # 각 노드가 머지될 super_class 결정
 363:         node_merge_class: Dict[str, SuperClass] = {}
 364:         for nid, n in self.nodes.items():
 365:             node_policy = get_node_policy(n, self.view)
 366:             if node_policy.action == NodeAction.MERGE and node_policy.super_class is not None:
 367:                 node_merge_class[nid] = node_policy.super_class
 368:         
 369:         merge_candidates = set(node_merge_class.keys())
 370:         visited: Set[str] = set()
 371: 
 372:         for nid in merge_candidates:
 373:             if nid in visited:
 374:                 continue
 375: 
 376:             target_class = node_merge_class[nid]
 377:             stack = [nid]
 378:             component: Set[str] = set()
 379: 
 380:             while stack:
 381:                 cur = stack.pop()
 382:                 if cur in visited or cur not in merge_candidates:
 383:                     continue
 384:                 # 같은 super_class를 가진 노드만 처리
 385:                 if node_merge_class[cur] != target_class:
 386:                     continue
 387: 
 388:                 visited.add(cur)
 389:                 component.add(cur)
 390: 
 391:                 for nb in self._neighbors_1hop(cur):
 392:                     if nb in merge_candidates and node_merge_class.get(nb) == target_class:
 393:                         stack.append(nb)
 394: 
 395:             if not component:
 396:                 continue
 397: 
 398:             sn_id = make_supernode_id(
 399:                 view=self.view,
 400:                 super_class=target_class,
 401:                 member_node_ids=component,
 402:                 policy_version="v2",
 403:             )
 404: 
 405:             sn = SuperNode(
 406:                 node_id=sn_id,
 407:                 super_class=target_class,
 408:                 member_nodes=component,
 409:                 member_edges=set(),
 410:             )
 411:             sn.canonical_name = make_supernode_canonical_name(sn, self.nodes)
 412:             sn.display_name = make_supernode_display_name(sn)
 413: 
 414:             self.super_nodes[sn.node_id] = sn
 415:             for n in component:
 416:                 self.node_to_super[n] = sn.node_id
 417: 
 418:     def cycle2_5_eliminate(self) -> None:
 419:         for nid, n in self.nodes.items():
 420:             if nid in self.node_to_super:
 421:                 continue
 422: 
 423:             node_policy = get_node_policy(n, self.view)
 424:             if node_policy.action != NodeAction.ELIMINATE:
 425:                 raise RuntimeError(f"Unassigned node in view {self.view}: {nid}")
 426: 
 427:             eliminate_class = node_policy.super_class if node_policy.super_class is not None else SuperClass.ELIMINATED
 428: 
 429:             sn = SuperNode(
 430:                 node_id=make_supernode_id(
 431:                     view=self.view,
 432:                     super_class=eliminate_class,
 433:                     member_node_ids={nid},
 434:                     policy_version="v1",
 435:                 ),
 436:                 super_class=eliminate_class,
 437:                 member_nodes={nid},
 438:                 member_edges=set(),
 439:             )
 440:             sn.canonical_name = make_supernode_canonical_name(sn, self.nodes)
 441:             sn.display_name = make_supernode_display_name(sn)
 442: 
 443:             self.super_nodes[sn.node_id] = sn
 444:             self.node_to_super[nid] = sn.node_id
 445: 
 446:     def cycle3_rewrite_edges(self) -> None:
 447:         for e in self.edges.values():
 448:             src_sn = self.node_to_super[e.src_node]
 449:             dst_sn = self.node_to_super[e.dst_node]
 450: 
 451:             if src_sn == dst_sn:
 452:                 self.super_nodes[src_sn].member_edges.add(e.edge_id)
 453:                 continue
 454: 
 455:             key = (src_sn, dst_sn)
 456:             if key not in self.super_edges:
 457:                 self.super_edges[key] = SuperEdge(
 458:                     edge_id=make_superedge_id(src_sn, dst_sn, set()),
 459:                     src_node=src_sn,
 460:                     dst_node=dst_sn,
 461:                     member_edges=set(),
 462:                     member_nodes=set(),
 463:                     relation_types=set(),
 464:                     flow_types=set(),
 465:                     provenances=[],
 466:                 )
 467:                 self.super_edges[key].canonical_name = make_superedge_canonical_name(
 468:                     self.super_edges[key],
 469:                     self.super_nodes,
 470:                 )
 471:                 self.super_edges[key].display_name = make_superedge_display_name(
 472:                     self.super_edges[key],
 473:                 )
 474: 
 475:             se = self.super_edges[key]
 476:             se.member_edges.add(e.edge_id)
 477:             se.member_nodes.update({e.src_node, e.dst_node})
 478:             se.relation_types.add(e.relation_type)
 479:             se.flow_types.add(e.flow_type)
 480:             se.provenances.extend(e.provenances)
 481: 
 482:     def build(self) -> SuperGraph:
 483:         self.cycle1_promote()
 484:         self.cycle2_merge()
 485:         self.cycle2_5_eliminate()
 486:         self.cycle3_rewrite_edges()
 487: 
 488:         return SuperGraph(
 489:             super_nodes=self.super_nodes,
 490:             super_edges=self.super_edges,
 491:             node_to_super=self.node_to_super,
 492:         )
 493: 
 494: 
 495: # ============================================================================
 496: # Analysis Attachment Helper Functions
 497: # ============================================================================
 498: # Analysis는 구조 로직에서 직접 참조하지 않으며, 결과의 귀속 대상으로만 사용됩니다.
 499: # ============================================================================
 500: 
 501: def attach_timing_analysis_to_supernode(
 502:     sn: SuperNode,
 503:     metrics: TimingNodeMetrics
 504: ) -> None:
 505:     """
 506:     SuperNode에 Timing Analysis 결과를 부착합니다.
 507:     
 508:     원칙:
 509:     - 구조 변경 없음
 510:     - 기존 analysis["timing"]은 전체 교체됨 (immutable snapshot)
 511:     - 구조 로직은 이 함수를 호출하지 않음
 512:     """
 513:     sn.analysis[AnalysisKind.TIMING] = metrics
 514: 
 515: 
 516: def attach_timing_analysis_to_superedge(
 517:     se: SuperEdge,
 518:     metrics: TimingEdgeMetrics
 519: ) -> None:
 520:     """
 521:     SuperEdge에 Timing Analysis 결과를 부착합니다.
 522:     
 523:     원칙:
 524:     - 구조 변경 없음
 525:     - 기존 analysis["timing"]은 전체 교체됨 (immutable snapshot)
 526:     - 구조 로직은 이 함수를 호출하지 않음
 527:     """
 528:     se.analysis[AnalysisKind.TIMING] = metrics
 529: 
 530: 
 531: def get_timing_analysis_from_supernode(
 532:     sn: SuperNode
 533: ) -> Optional[TimingNodeMetrics]:
 534:     """
 535:     SuperNode에서 Timing Analysis 결과를 조회합니다.
 536:     
 537:     Returns:
 538:         TimingNodeMetrics or None if not attached
 539:     """
 540:     return sn.analysis.get(AnalysisKind.TIMING)
 541: 
 542: 
 543: def get_timing_analysis_from_superedge(
 544:     se: SuperEdge
 545: ) -> Optional[TimingEdgeMetrics]:
 546:     """
 547:     SuperEdge에서 Timing Analysis 결과를 조회합니다.
 548:     
 549:     Returns:
 550:         TimingEdgeMetrics or None if not attached
 551:     """
 552:     return se.analysis.get(AnalysisKind.TIMING)
 553: 
 554: 
 555: # ============================================================================
 556: # 향후 확장 예시 (Area / Power Analysis)
 557: # ============================================================================
 558: # 동일한 패턴으로 확장 가능:
 559: #
 560: # @dataclass(frozen=True)
 561: # class AreaMetrics:
 562: #     area_density: float
 563: #     area_utilization: float
 564: #     area_total: float
 565: #
 566: # @dataclass(frozen=True)
 567: # class PowerMetrics:
 568: #     power_peak: float
 569: #     power_average: float
 570: #     power_leakage: float
 571: #
 572: # def attach_area_analysis_to_supernode(sn: SuperNode, metrics: AreaMetrics) -> None:
 573: #     sn.analysis[AnalysisKind.AREA] = metrics
 574: #
 575: # def attach_power_analysis_to_supernode(sn: SuperNode, metrics: PowerMetrics) -> None:
 576: #     sn.analysis[AnalysisKind.POWER] = metrics
 577: #
 578: # 사용 예시:
 579: #     supernode.analysis[AnalysisKind.TIMING]  -> TimingNodeMetrics
 580: #     supernode.analysis[AnalysisKind.AREA]    -> AreaMetrics
 581: #     supernode.analysis[AnalysisKind.POWER]   -> PowerMetrics
 582: # ============================================================================



FILE: dkg\utils.py

   1: from __future__ import annotations
   2: 
   3: import hashlib
   4: import re
   5: from pathlib import Path
   6: from typing import Optional, Tuple
   7: 
   8: 
   9: def is_clock_name(name: str) -> bool:
  10:     n = name.lower()
  11:     return n == "clk" or n.startswith("clk") or n.endswith("_clk") or "clock" in n
  12: 
  13: 
  14: def is_reset_name(name: str) -> bool:
  15:     n = name.lower()
  16:     return n == "rst" or n.startswith("rst") or n.startswith("reset")
  17: 
  18: 
  19: def is_active_low(name: str) -> bool:
  20:     return name.lower().endswith("_n")
  21: 
  22: 
  23: def is_ff_cell(cell_type: str) -> bool:
  24:     return cell_type in {"$dff", "$adff", "$sdff", "$dffe", "$sdffe"}
  25: 
  26: 
  27: def is_async_reset_ff(cell_type: str) -> bool:
  28:     return cell_type == "$adff"
  29: 
  30: 
  31: def is_sync_reset_ff(cell_type: str) -> bool:
  32:     return cell_type == "$sdff"
  33: 
  34: 
  35: def win_to_wsl_path(win_path: str) -> str:
  36:     p = Path(win_path).resolve()
  37:     drive = p.drive[0].lower()
  38:     path_no_drive = p.as_posix()[2:]
  39:     return f"/mnt/{drive}{path_no_drive}"
  40: 
  41: 
  42: def parse_src(src_str: Optional[str]) -> Tuple[Optional[str], Optional[int]]:
  43:     if not src_str:
  44:         return None, None
  45:     try:
  46:         file_part, line_part = src_str.split(":")
  47:         line = int(line_part.split(".")[0])
  48:         return file_part, line
  49:     except Exception:
  50:         return None, None
  51: 
  52: 
  53: def split_signal_bit(sig: str) -> Tuple[str, Optional[int]]:
  54:     m = re.match(r"(.+)\[(\d+)\]$", sig)
  55:     if m:
  56:         return m.group(1), int(m.group(2))
  57:     return sig, None
  58: 
  59: 
  60: def stable_hash(s: str, length: int = 12) -> str:
  61:     return hashlib.sha1(s.encode()).hexdigest()[:length]
  62: 
  63: 
  64: def compute_file_hash(filepath: str | Path) -> str:
  65:     """파일 내용의 SHA-256 해시 계산"""
  66:     filepath = Path(filepath)
  67:     if not filepath.exists():
  68:         raise FileNotFoundError(f"File not found: {filepath}")
  69:     
  70:     sha256 = hashlib.sha256()
  71:     with open(filepath, "rb") as f:
  72:         for chunk in iter(lambda: f.read(8192), b""):
  73:             sha256.update(chunk)
  74:     return sha256.hexdigest()[:16]  # 처음 16자만 사용



FILE: dkg\yosys_parser.py

   1: from __future__ import annotations
   2: 
   3: import glob
   4: import json
   5: import os
   6: import subprocess
   7: from pathlib import Path
   8: from typing import List
   9: 
  10: from .config import YosysConfig
  11: from .utils import win_to_wsl_path
  12: 
  13: 
  14: def collect_hdl_files(src_dir_win: str) -> List[str]:
  15:     verilog_files = glob.glob(os.path.join(src_dir_win, "*.v"))
  16:     sv_files = glob.glob(os.path.join(src_dir_win, "*.sv"))
  17:     return verilog_files + sv_files
  18: 
  19: 
  20: def build_yosys_script(files_wsl: List[str], top_module: str, out_json_wsl: str) -> str:
  21:     return "\n".join(
  22:         [
  23:             f"read_verilog -sv {' '.join(files_wsl)};",
  24:             f"hierarchy -check -top {top_module};",
  25:             "proc;",
  26:             "opt;",
  27:             f"write_json {out_json_wsl}",
  28:         ]
  29:     )
  30: 
  31: 
  32: def run_yosys(files_win: List[str], config: YosysConfig) -> None:
  33:     if not files_win:
  34:         raise RuntimeError("No HDL files found.")
  35: 
  36:     files_wsl = [win_to_wsl_path(f) for f in files_win]
  37:     out_json_wsl = win_to_wsl_path(config.out_json_win)
  38:     yosys_script = build_yosys_script(files_wsl, config.top_module, out_json_wsl)
  39: 
  40:     subprocess.run(["wsl", "yosys", "-p", yosys_script], check=True)
  41: 
  42: 
  43: def load_yosys_json(out_json_win: str) -> dict:
  44:     with open(out_json_win, "r", encoding="utf-8") as f:
  45:         return json.load(f)
  46: 
  47: 
  48: def parse_yosys(config: YosysConfig) -> dict:
  49:     files_win = collect_hdl_files(config.src_dir_win)
  50:     run_yosys(files_win, config)
  51:     return load_yosys_json(config.out_json_win)


